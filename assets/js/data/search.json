[ { "title": "실시간 채팅방 구현(19) - (현재까지의 일정관리 Notion)", "url": "/posts/chatting(19)/", "categories": "채팅서버 프로젝트, 일정관리", "tags": "", "date": "2023-01-27 01:00:25 +0000", "snippet": "프로젝트 전체 일정현재 진행중인 스프린트" }, { "title": "실시간 채팅방 구현(18) - (성능개선 사안들 및 적용된 값)", "url": "/posts/chatting(18)/", "categories": "채팅서버 프로젝트, 성능문제 개선", "tags": "", "date": "2023-01-27 01:00:25 +0000", "snippet": "성능개선사안들 및 적용된 값들 batch &lt;– 성능을 가장 많이 개선시킨 부분(네트워크 로드 감소) before : 1 after : 100 chatting_id 내부 자동 생성(네트워크 로드 감소) before : from db sequence after : random.UUID db parallel processor 확장(db cpu 사용률 증가) before : 1개 after : 8개 쿼리 빈도 축소( 최적화 + lazy fetch ) before : 6번 after : 4번 서버 수평 확장 before : 1 after : 2 테스트 환경   사양 MODEL MacBook Pro (13-inch, 2020, Four Thunderbolt 3 ports) CPU 2 GHz 쿼드 코어 Intel Core i5 MEM 32GB 3733 MHz LPDDR4X Disk Macintosh HD Docker Cpu 8 Docker Disk Storage 150 GB Docker Memory 16 GB 결과10K request 80초 -&gt; 29초로 개선되었다.Beforetest-multiple-http-request | Request url: http://127.0.0.1:8060/chat/chattest-multiple-http-request | The number of HTTP Requests: 10000test-multiple-http-request | The number of threads: 100test-multiple-http-request | Proceeding! Please wait until getting all the responsestest-multiple-http-request | Elapsed Time: 80.02316758test-multiple-http-request | Response status code: 200 , How many?: 10000After...test-multiple-http-request | Proceeding! Please wait until getting all the responsestest-multiple-http-request | Elapsed Time: 29.554215625" }, { "title": "실시간 채팅방 구현(17) - (프론트 서버 개발 완료 및 성능이슈 발생)", "url": "/posts/chatting(17)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-24 01:00:25 +0000", "snippet": "INDEX Batch JPA batch JDBC batch 일반적인 성능비교 그래서 Chatting을 insert할 때는 어떤 방식이 좋을까? 이슈비교 그 외 종합 성능비교 그래프 결론기능 설명 및 시연영상 https://www.youtube.com/watch?v=3VqwZ17XyEQ&amp;t=237s현재는 4.2.0 버전이며 프론트 서버까지 모두 완료되었다!이후, 실제 트래픽양이 많은 chat을 이전에 만들어둔 Go 프로그램으로 테스트 해보았더니, 아래와 같이 성능결과가 암울했다. 약 10K개의 채팅을 수용하는데 80초가 소요된 것이다.test-multiple-http-request | Request url: http://127.0.0.1:8060/chat/chattest-multiple-http-request | The number of HTTP Requests: 10000test-multiple-http-request | The number of threads: 100test-multiple-http-request | Proceeding! Please wait until getting all the responsestest-multiple-http-request | Elapsed Time: 80.02316758test-multiple-http-request | Response status code: 200 , How many?: 10000그래서 우리는 Batch를 사용해서 이를 개선해볼 것이다.Batch배치는 특정 사이즈만큼 리스트에 저장하고 송출하는 방식이다. 여기는 JPA, JDBC 두 가지 방식이 있는데, 아래는 각각을 설명한다.JPA batchJPA의 batch는 여러개의 TX를 모아서 전송함으로써 네트워크 딜레이 로드를 줄이는 방식이다.-- JPA Batch=4 설정INSERT INTO PRODUCT(TITLE, CREATED_TS, PRICE)VALUES ('test1', LOCALTIMESTAMP, 100.10);INSERT INTO PRODUCT(TITLE, CREATED_TS, PRICE)VALUES ('test2', LOCALTIMESTAMP, 101.10);INSERT INTO PRODUCT(TITLE, CREATED_TS, PRICE)VALUES ('test3', LOCALTIMESTAMP, 102.10);INSERT INTO PRODUCT(TITLE, CREATED_TS, PRICE)VALUES ('test4', LOCALTIMESTAMP, 103.10);JDBC batchJPA의 batch는 그냥 여러개의 TX를 모아서 전송하는 느낌인 반면에, JDBC Batch는 실제 쿼리를 튜닝하여 좀더 로우 레벨에서 insert한다.-- JDBC Batch=4 사용INSERT INTO PRODUCT(TITLE, CREATED_TS, PRICE)VALUES ('test1', LOCALTIMESTAMP, 100.10), ('test2', LOCALTIMESTAMP, 101.10), ('test3', LOCALTIMESTAMP, 102.10), ('test4', LOCALTIMESTAMP, 104.10);일반적인 성능비교--------------------------------------------------Regular inserts | 1 | 15msBatch inserts | 1 | 10msTotal gain: 33 %--------------------------------------------------Regular inserts | 10 | 3msBatch inserts | 10 | 2msTotal gain: 33 %--------------------------------------------------Regular inserts | 100 | 42msBatch inserts | 100 | 10msTotal gain: 76 %--------------------------------------------------Regular inserts | 1000 | 141msBatch inserts | 1000 | 19msTotal gain: 86 %--------------------------------------------------Regular inserts | 10000 | 827msBatch inserts | 10000 | 104msTotal gain: 87 %--------------------------------------------------Regular inserts | 100000 | 5093msBatch inserts | 100000 | 981msTotal gain: 80 %--------------------------------------------------Regular inserts | 1000000 | 50482msBatch inserts | 1000000 | 9821msTotal gain: 80 %--------------------------------------------------JPA batch는 1초에 약 2만개의 insert 쿼리를 처리할 수 있는 반면에, JDBC batch는 약 1초에 10만개의 insert 쿼리를 처리할 수 있다.그래서 Chatting을 insert할 때는 어떤 방식이 좋을까?이슈비교 캐싱 성능 JPA 승리. 현재 Chatting이 많을 경우 이를 불러오는 부분에서 많은 네트워크 로드가 걸린다. JPA는 영속성을 활용해서 캐싱을 지원하기에 성능이득을 볼 수 있다. 네트워크 로드 JPA, JDBC 둘 다 쿼리를 모아두었다가 보내기에 비슷하다. 다만 JDBC의 패킷 크기가 조금 작다. DB 병렬지원 시 성능 만약 DB가 병렬지원한다면, JPA 승리. 개별쿼리를 가지기 떄문에 DB가 병렬로 실행시킬 수 있기 때문이다. postgres는 max_parallel_workers 로 병렬쿼리실행을 지원한다. DB 컨트롤 능력 JDBC 승리. JPA도 물론 네이티브 쿼리 및 여러가지 설정을 할 순 있지만, 기본적으로 JDBC 위에서 동작하기 떄문에 로우 레벨인 JDBC가 승리할 수 밖에 없다. 쿼리 실행 속도 JDBC 승리. DB 병렬을 극한까지 사용하지 않으면 JDBC 승리. 그 외 fine-grain JPA 승리. JDBC 보다 사용하기가 훨씬 간편하다. 동시성 문제발생? JDBC batch 사용하기 위해서는 요청을 차곡차곡 쌓을 리스트가 필요하다. 이 때, 리스트는 thread safe해야한다. 그 이유는 아래와 같다. 그리고 여기서 병목현상이 발생할 수 있다. 우리는 @Service 어노테이션을 사용하여 싱글톤으로 생성했다. 얘네는 자원을 힙에다 두고 공유하는 방식이다. 그런데 우리는 멀티 스레드로 서비스의 공유자원을 사용하기때문에 동시접근 문제가 발생한다. 따라서 batch size만큼 저장하는 리스트는 mutex가 지원되는 리스트를 사용해야만 한다. 종합 성능비교 그래프 Regular Batch 실험환경 Batch=100 사용 레코드는 synchronously 하게 배치에 추가된다 다 따지고 봤을 때, JDBC가 약 4%정도 성능이 뛰어나다.reference https://medium.com/walmartglobaltech/a-cruncher-choice-jpa-or-jdbc-85c589f584a9결론필자는 JPA를 사용할 것이다. 아무래도 채팅기록을 받아올 때, 캐싱된 값을 받아올 수 있어 성능이점을 가지기 때문이다. 그리고 무엇보다도 쓰기가 정말정말 편하기 때문에 JPA를 선택하였다. 다음 포스팅에서는 JPA batch를 통한 성능개선 방법을 소개한다." }, { "title": "실시간 채팅방 구현(16) - (프론트 서버의 예상 UI + 시퀀스 다이어그램 + HTTP 송/수신 방법 정의)", "url": "/posts/chatting(16)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-20 01:00:25 +0000", "snippet": "INDEX(프론트 서버 설계) 예상 UI 시퀀스 다이어그램 프론트 HTTP request/response 비동기 or Non-blocking?예상 UI앞서 우리는 API Gateway까지 백엔드 전체를 생성했다. 이제 실제 body와 uri를 게이트웨이에 전송하는 프론트 서버를 만들어 볼 것이다.먼저 Protopie 툴을 통해서 간단히 예상되는 UI를 만들고 시뮬레이션 해보았다. 화면목록 1 : 로그인 2 : 회원등록 3 : 메인 4 : 상태메세지 변경 5 : 친구 추가 6 : 채팅방 목록 7 : 채팅방 추가 8 : 채팅방 시퀀스 다이어그램간단하게 1~3, 8화면의 플로우를 시퀀스 다이어그램으로 나타내보았다. 3~7 화면의 시퀀스는 반복이라 따로 그리지는 않았다.로그인(1) Front Server 는 클라이언트 로부터 유저ID 및 PW가 담긴 POST 요청을 수신한다 Front Server 는 세션을 확인한다 세션에 유저 정보가 없다면, 백엔드 게이트웨이(NGINX:http://localhost:8080/auth/login)에 GET 요청을 전송한다 백엔드 게이트웨이 는 ip_hash로 Auth Server 중 하나에 전달한다 Auth Server 는 비밀번호와 매칭된 유저ID를 DB에서 검색 후 매칭시킨다(+loginDate 업데이트) 비밀번호 매칭 x : INVALID_PASSWORD(401, “잘못된 비밀번호입니다”)를 반환 유저ID 미발견 : CANNOT_FIND_USER(404, “해당 유저 정보를 찾을 수 없습니다”)를 반환 매칭된 User정보 및 StatusCode=200 반환 매칭된 User정보 및 StatusCode=200 반환 Front Server 는 200 도착하면 세션에 유저를 저장한다 Front Server 는 클라이언트 에게 반환 로그아웃(1-1) Front Server 는 클라이언트 로부터 유저ID가 담긴 POST 요청을 수신한다 Front Server 는 세션을 확인한다 세션에 저장된 유저정보가 없음 : CANNOT_FIND_USER_IN_SESSION(NOT_FOUND, “세션에서 해당 유저 정보를 찾을 수 없습니다”)를 반환 Front Server 는 http://localhost:8080/auth/logout 에 유저ID와 함께 POST 요청 백엔드 게이트웨이 는 http://auth-server-1:8082/auth/logout 로 프록시 패스 Auth Server 은 해당 유저의 logoutDate 업데이트 유저ID 미발견 : CANNOT_FIND_USER(NOT_FOUND, “해당 유저 정보를 찾을 수 없습니다”)를 반환 statusCode만 반환 statusCode만 반환 statusCode만 반환 회원등록(2) Front Server 는 클라이언트 로부터 유저ID/유저PW/유저이름/이메일 이 담긴 POST 요청을 수신한다 백엔드 게이트웨이 에 전달 백엔드 게이트웨이 는 http://auth-server-1:8082/auth/user 로 프록시 패스 Auth Server 은 유저ID가 기존에 존재하는지 확인하고 저장 (Async-1) kafka의 user-change 토픽(partition:2,replica:3)에 같은 파티션(key=userId)으로 메세지 전송 중복되는 유저 ID 발견 : DUPLICATE_USER_RESOURCE(409, “유저 데이터가 이미 존재합니다”)를 반환 등록된 유저정보를 반환 (Async-2) Chat Server 은 3대의 kafka Broker로부터 user-change 토픽을 읽음 등록된 유저정보를 반환 (Async-3) Chat Server 은 유저의 INSERT/DELETE를 감지하여 자신의 DB에 SYNC 등록된 유저정보를 반환 메인화면 입장(3) Front Server 는 클라이언트 로부터 GET 요청을 수신한다 Front Server 는 세션을 확인한다 세션에 저장된 유저정보가 없음 : CANNOT_FIND_USER_IN_SESSION(NOT_FOUND, “세션에서 해당 유저 정보를 찾을 수 없습니다”)를 반환 Front Server 는 http://localhost:8080/chat/user 에 userId를 포함한 GET 요청 Front Server 는 http://localhost:8080/chat/friend 에 userId를 포함한 GET 요청 백엔드 게이트웨이 는 http://chat-server-1:8084/chat/user 로 프록시 패스 백엔드 게이트웨이 는 http://chat-server-1:8084/chat/friend 로 프록시 패스 Chat Server 은 userId와 매칭되는 유저 레코드 확인 유저 레코드 반환 Chat Server 은 userId와 매칭되는 친구 레코드 확인 유저 레코드 반환 친구 레코드 반환 친구 레코드 반환 View 반환 채팅방 입장 및 채팅입력(8) Front Server 는 클라이언트 로부터 roomId가 포함된 GET 요청을 수신한다 Front Server 는 세션 유저정보 로드 후, http://localhost:8080/chat/chats 로 userId/roomId가 포함된 이전 채팅기록 조회를 요청한다 세션에 저장된 유저정보가 없음 : CANNOT_FIND_USER_IN_SESSION(NOT_FOUND, “세션에서 해당 유저 정보를 찾을 수 없습니다”)를 반환 백엔드 게이트웨이 는 http://chat-server-1:8084/chat/chats 로 프록시 패스 Chat Server 은 실제 유저로직 수행 유저가 해당 채팅방에 참여하지 않음 : INVALID_PARTICIPANT(400, “해당 채팅방에 참여하는 유저가 아닙니다”)를 반환 유저가 없음 : CANNOT_FIND_USER(NOT_FOUND, “해당 유저 정보를 찾을 수 없습니다”)를 반환 채팅방이 없음 : CANNOT_FIND_ROOM(NOT_FOUND, “해당 채팅방 정보를 찾을 수 없습니다”) 반환 이전 채팅기록 반환 이전 채팅기록 반환 STOMP 토픽(sub/chat/{roomId})에 연결 연결성공 반환 Front Server 는 클라이언트 로부터 roomId/writer/writerId/message 가 포함된 POST 요청을 수신한다 Front Server 는 http://localhost:8080/chat/chat 으로 POST 전송 백엔드 게이트웨이 는 http://chat-server-1:8084/chat/chat 로 프록시 패스 Chat Server 는 채팅저장 + Kafka의 log-add-chat 토픽에 메세지 전달(이 후 ELK로 모니터링) status code를 반환 status code를 반환 Front Server 는 정상코드 반환 시, STOMP 토픽(sub/chat/{roomId})에 브로드캐스트 STOMP 토픽을 구독하는 클라이언트 들은 메세지 수신 프론트 HTTP request/response 비동기 or Non-blocking?위와 같이 플로우를 미리 생성해놓으면 중간에 빠질 우려가 있는 에러코드 및 기능들을 한눈에 볼 수 있다.이제 우리가 고려할 것은 프론트에서 API Gateway 로 요청 송신/수신하는 방법이다. 우리는 해당 송/수신에 WebClient를 사용할 것이다. WebClient는 Spring-WebFlux 라이브러리에서 지원하는 비동기-Non blocking 클라이언트이다. 물론 sync 및 blocking 은 기본지원된다. 따라서 어떤 기능을 sync/Async, blocking/non-blocking 처리할 것인지 정하면 된다. 기능 송신/수신방법 로그인 전송 sync + blocking 로그아웃 전송 Async + non-blocking (response까지 대기할 필요가 없기 때문) 유저 추가 Async + non-blocking (마찬가지로 response 대기 미필요) 유저 상태메세지 업데이트 sync + blocking 메인화면 수신 sync + blocking 친구 수신 sync + blocking 친구 추가 sync + blocking 채팅방 추가 sync + blocking 채팅방 리스트 수신 sync + blocking 이전 채팅 수신 sync + blocking(카카오 같은 경우에는 local에 미리 저장하는듯 하다) 채팅 입력 Async + non-blocking(서버에 저장되기까지 임시 개인화면에 송출하고, 에러 시 fault 화면 표시) " }, { "title": "실시간 채팅방 구현(15) - (대량 Rest api test 시 속도문제 해결과정-2)", "url": "/posts/chatting(15)/", "categories": "채팅서버 프로젝트, 성능문제 개선", "tags": "", "date": "2023-01-17 01:00:25 +0000", "snippet": "이전 테스트에서 발생한 성능이슈를 (1)Server 수평확장 및 (2)Docker 할당 리소스 추가로 10K HTTP request 테스트의 response까지 걸리는 시간을 110초에서 49초로 감소시켰다.이번에는 Spring Jpa Batch를 통해 49초에서 23초로 감소시킨다. NOTICE 쿼리튜닝이 아직 생소하기때문에 내용이 정확하지 않을 수 있습니다(특히 JDBC batch!). 혹시 잘못된 내용을 보신다면, ghkdqhrbals@gmail.com 로 메일주시면 정말정말 감사하겠습니다 :)INDEX(의심가는 부분) AuthServer –&gt; AuthDB 네트워크 오버로드 1-1. 개선 결과(49초 -&gt; 23초) 1-2. 추가 고려점1. AuthServer –&gt; AuthDB 네트워크 오버로드현재의 AuthServer은 JPA를 통해 AuthDB에 쿼리를 전송한다. 이 때, 하나의 쿼리를 반복전송 하게된다. 이는 다음의 가설을 정립시킨다. 가설-1 : 도커 내부 네트워크를 너무 많이 사용하는 이유때문이 아닐까?따라서 여러개의 쿼리를 모았다가 한번에 DB에 전송할 수 있는 JPA의 batch기능을 사용하고자 한다.1-1. 개선결과(49초 -&gt; 23초)먼저 application.properties에 다음을 설정한다.배치 사이즈를 너무 크게 잡아버리면, (1) 서버 다운시 전부 날라가버린다. (2) 잡아먹는 메모리가 크다. (3) DB종류에 따른 패킷 용량제한에 걸린다. 그래서 적절한 배치 사이즈를 설정해주어야 한다. ex) MySQL는 최대 64MB 패킷까지만 듣는다. Postgres(9.5 버전 이후)는 1GB 까지 허용한다. Proper Batch Size (5 ~ 50) : https://stackoverflow.com/questions/14789052/hibernate-batch-size-best-value Proper Batch Size (5 ~ 30) : in Hibernate documentation 위 내용으로 미루어보면, (1) postgres를 쓰며, (2) insert user 쿼리길이가 짧은 필자는 50개가 적당하다고 판단!spring.jpa.properties.hibernate.jdbc.batch_size=50spring.jpa.properties.hibernate.order_inserts=true이후 테스트해본 결과 49-&gt;23초의 개선결과를 확인할 수 있다. 즉, 가설-1이 타당하다라고 판단할 수 있다. 물론 JPA batch 기능은 multi-row insert가 아닌, 개별 쿼리를 모아서 전송하는 기능이다.test-multiple-http-request | Request url: http://127.0.0.1:8080/auth/usertest-multiple-http-request | The number of HTTP Requests: 10000test-multiple-http-request | The number of threads: 100test-multiple-http-request | Proceeding! Please wait until getting all the responsestest-multiple-http-request | Elapsed Time: 23.889751999test-multiple-http-request | Response status code: 200 , How many?: 10000 현재 user의 ID는 @GenerateID(Sequence) 어노테이션으로 생성하지 않는다. 따라서 Sequence를 계속해서 받아오는 성능이슈는 해당되지 않는다고 생각한다.1-2. 추가 고려점앞서 적용한 Batch는 단순히 개별 쿼리를 모아서 전송하는 기능이다. 그런데 이는 네트워크 통신에 소모되는 시간을 아껴준다 뿐이지, 실제 쿼리시간을 소모시킨다는 개념은 아니다. 따라서 실제 쿼리비용을 아껴줄 방법을 찾던중 다음의 표를 찾았다. 기능 10건 100건 1000건 10000건 100000건 JPA 0.03s 0.19s 2.06s 18.47s 175.58s jdbc 0.01s 0.01s 0.09s 0.33s 4.31s JPA/jdbc 비율 3 19 22 55.96 40.73 reference : https://sabarada.tistory.com/220JDBC로 직접 배치하게 되면 50배 이상의 속도를 낼 수 있다라고 볼 수 있다. 즉, JPA의 batch가 개별 insert를 모아서 전송하는 기능이라면, jdbc의 batch로 multi-row insert를 세부적으로 설정해줄 수 있다. 하지만 배치 중간에 에러가 발생할 경우를 심각하게 고려해야한다. 예로 배치된 여러 정상 쿼리 사이에 동일한 PK 값을 가지는 쿼리가 삽입되었다고 하자. 그러면, 트랜젝션 처리에 있어서 이 부분만 빼고 진행할 수가 없다… 그래서 이를 사전에 미리 파악해주는 로직이 들어가있어야지만 안정적으로 서비스할 수 있다." }, { "title": "실시간 채팅방 구현(14) - (대량 Rest api test 자동화)", "url": "/posts/chatting(14)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-17 00:00:25 +0000", "snippet": "실시간 채팅방 backend에 직접 대량의 Rest api test를 진행했었다. 그런데 이 rest api test가 매번 직접 설정해주기 힘들어서 이를 Docker-compose 와 Viper 로 자동화 할 것이다. 아래는 자동화 시키고 실행한 결과를 먼저 보인다. 사용법과 코드는 https://github.com/ghkdqhrbals/multiple-restapi-request-test 에 업로드하였다. 주의점! 프록시 서버가 컨테이너로 실행되고, 호스트한테 프록시 포트가 expose되어있다면, 그대로 docker-compose up을 실행하면 된다. 하지만 내부 포트로 expose되어 있다면, 같은 네트워크로 묶어줘야한다!!실행 결과test-multiple-http-request | Request url: http://127.0.0.1:8080/auth/usertest-multiple-http-request | The number of HTTP Requests: 10000test-multiple-http-request | The number of threads: 100test-multiple-http-request | Proceeding! Please wait until getting all the responsestest-multiple-http-request | Elapsed Time: 30.533003028test-multiple-http-request | Response status code: 200 , How many?: 10000총 10K개의 rest api call 을 진행하였고, 100개의 go-routine으로 진행한다. 결과로 약 30.5초가 소요되었으며, 전부 200 status code를 반환 받은 것을 확인할 수 있다.흐름도는 아래와 같다.app.env 설정--(Viper)--&gt; go build--&gt; build docker images--&gt; run docker container(network:host)--&gt; nginx--&gt; auth-server아래는 이미지 생성에 필요한 Dockerfile# Build stageFROM golang:1.18.3-alpine3.16 AS builderWORKDIR /appCOPY . .RUN go build -o main main.go# Run stageFROM alpine:3.16WORKDIR /appCOPY --from=builder /app/main .COPY app.env .CMD [\"/app/main\"]아래는 docker-compose.yamlversion: '3'services: multi-test: container_name: test-multiple-http-request build: context: . dockerfile: Dockerfile ports: - \"8002:8002\" command: [\"/app/main\"] network_mode: \"host\"" }, { "title": "실시간 채팅방 구현(13) - (대량 Rest api test 시 속도문제 해결과정-1)", "url": "/posts/chatting(13)/", "categories": "채팅서버 프로젝트, 성능문제 개선", "tags": "", "date": "2023-01-16 00:00:25 +0000", "snippet": "이전 포스팅에서 10000개의 rest api를 직접 test해보았다. 로직에는 문제가 없지만 성능이슈가 너무 나버렸다.10000개 테스트 하는데 110초가 걸린것이다. 의심가는 부분이 한두군데가 아니다. 오늘의 포스팅은 의심가는 부분을 한번 찾아보고 정리하려한다. 본 과정의 결과로 10K개의 request test 시, 10K개의 response까지 걸리는 시간: 110초 -&gt; 49초로 성능개선 및 이유를 확인할 수 있다.INDEX(의심가는 부분) Golang Code의 http request 속도 Auth Server -&gt; Kafka -&gt; Chat Server -&gt; Chat DB 과정 Chat DB(tx log) -&gt; Debezium -&gt; Kafka -&gt; JDBC-Sink-Connector -&gt; Backup DB 과정 약 10개의 토픽과 각각의 토픽이 1~25개의 파티션을 보유함에 따라 3대의 Kafka broker끼리 과한 replication 생성 도커 리소스 및 nginx 프로세서 제한 5-1. 실험환경 5-2. CPU 사용률 확인 5-3. 개선결과(110초 -&gt; 86초) 단일 Auth 서버로 인한 병목현상 6-1. auth-server 수평 확장 6-2. nginx loadbalancing 6-3. 개선결과-1(86초 -&gt; 83초) 6-4. 개선결과-2(83초 -&gt; 49초)1. Golang Code의 http request시 속도전송속도는 매우 빠르다. 사실 처리할 로직이 별로 필요없기때문에 빠른것은 예상된 결과이다. 문제는 response가 오는 시점이 느리다는 것!2. Auth Server -&gt; Kafka -&gt; Chat Server -&gt; Chat DB 과정 위의 캡처에서 보는바와 같이, 지연되는 데이터가 거의 7600개의 메세지 중 1000개 정도 되는것을 확인할 수 있다. 즉, 여기서 어느정도의 지연이 발생하는 것을 확인할 수 있다. 얘는 일단 파티션을 2개만 사용하는데, 파티션을 늘려 분산처리할 수 있도록 설정하는 것이 좋을것 같다! 하지만 이 부분은 Auth Server 테스트에 크게 영향이 없다. 오히려 Chat Server의 성능테스트에 영향을 준다.3. Chat DB(tx log) -&gt; Debezium -&gt; Kafka -&gt; JDBC-Sink-Connector -&gt; Backup DB 과정이 부분은 마찬가지로 Chat Server 테스트시 고려해야할 부분이다.4. 약 10개의 토픽과 각각의 토픽이 1~25개의 파티션을 보유함에 따라 3대의 Kafka broker끼리 과한 replication 생성이 부분은 마찬가지로 (1)Chat Server 테스트 + (2)전체 서비스 리소스 소모량 관찰 시 고려해야할 부분이다.아무래도 3번의 의심부분처럼 직접 캡처하면서 확인하기는 힘들것 같다(토픽이 여러개라서ㅜㅜ). 그래서 Kafka의 lag 데이터를 따로 통계내는 Grafana를 사용하여 한눈에 봐야겠다.5. 도커 리소스 및 nginx 프로세서 제한5-1. 실험환경 MacBook Pro (13-inch, 2020, Four Thunderbolt 3 ports) Processor : 2 GHz 쿼드 코어 Intel Core i5 Memory : 32GB 3733 MHz LPDDR4X 5-2. CPU 사용률 확인필자의 맥북이 논리 cpu 코어가 8개 물리 코어가 4개인 것을 감안한다면 약 50%의 자원을 사용하고 있는 것을 확인할 수 있다.gyuminhwangbo@Gyuminui-MacBookPro spring-chatting-server % sysctl -a | grep cpu...hw.activecpu: 8hw.perflevel0.physicalcpu: 4hw.perflevel0.physicalcpu_max: 4hw.perflevel0.logicalcpu: 8...그리고 현재 도커는 4개의 CPU 논리코어만 활용가능하도록 아래와 같이 설정되어있다.즉, Docker에 할당가능한 모든 리소스를 풀로 다 쓰고있다고 볼 수 있다!! 따라서 대량 http request 테스트의 지연 문제점은 Docker 할당 리소스 부족 이라는 1차 결론을 내릴것이다.5-3. 개선결과(1)cpu 용량을 더 늘림과 동시에 (2)nginx의 work 프로세스 제한걸린 부분을 확장하였더니 아래의 결과를 확인하였다.# nginx.conf 프로세서 확장worker_processes 100;# shell 확인gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://127.0.0.1:8080/auth/user 10000실행시간 86.281953431status: 200 count: 10000비교했을 떄, 110초 -&gt; 86초 로 약 24초가량 단축됨을 관찰할 수 있다!6. 단일 Auth 서버로 인한 병목현상Auth 서버가 하나이기때문에 서버 로직의 병목현상이 존재할 수 있다. 따라서 Auth 서버를 확장(DB는 단일)시킬것이다. 우리는 쉽게 이를 확장시킬 수 있다! Kafka의 확장성 장점이 여기서 드러나고, Docker의 이미지 재활용 장점또한 여기서 드러난다!우리가 수정해야 할 곳은 nginx.conf와 docker-compose.yaml 이 두 가지이다.6-1. 먼저 docker-compose.yaml에 auth-server-2를 아래와 같이 추가version : '2'services: # -------- KAFKA -------- ... # -------- Chatting Server -------- ... # -------- kafdrop for visualization -------- ... # -------- postgres -&gt; kafka source connector -------- ... # -------- API GATEWAY -------- ... # -------- Authentication Server -------- ... auth-server-2: container_name: auth-server-2 build: ./spring-auth-backend-server ports: - \"8072:8072\" environment: - SERVER_PORT=8072 - SPRING_DATASOURCE_URL=jdbc:postgresql://auth-db:5435/auth - SPRING_DATASOURCE_USERNAME=postgres - SPRING_DATASOURCE_PASSWORD=password - SPRING_JPA_HIBERNATE_DDL_AUTO=update - KAFKA_BOOTSTRAP=kafka1:9092,kafka2:9092,kafka3:9092 # 내부포트 depends_on: - auth-db restart: always6-2. 이후, nginx.conf의 upstream에 해당 컨테이너를 추가upstream auth-server { server auth-server:8085; server auth-server-2:8072; }위와 같이 아무것도 설정하지 않으면, nginx가 라운드 로빈으로 번갈아가면서 메세지를 날려준다.6-3. 개선결과-1gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://127.0.0.1:8080/auth/user 10000실행시간 83.900060333status: 200 count: 1000086초 -&gt; 83초로 아주 조금 감소한 것을 확인할 수 있다. 참고로 cpu사용률은 평균 520%로 440%에서 약 80%가량 상승했다.6-4. 개선결과-2그런데 이상한 점을 발견했다. 혹시 몰라서 바로 한번더 테스트해보았더니,gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://127.0.0.1:8080/auth/user 10000실행시간 83.900060333status: 200 count: 10000gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://127.0.0.1:8080/auth/user 10000실행시간 49.853405095status: 200 count: 10000 &lt;--- 약 34초 소모시간 감소?!이게 무슨일일까? 일단 TCP handshake에 소모되는 시간 감소는 아닐것이다. Golang 코드에서 Timeout 발생하면 연결을 유지하지 않기때문에, 다시 handshake를 해줘야하기 떄문이다. 혹시 nginx가 이전 connection에 대한 정보를 캐시로 가지고 있기때문에 초기 연결이 빠르게 설정된게 아닐까? 이또한 아닐 것이다. nginx는 default로 GET 메소드에 대해서만 캐시 구성을 설정해주기 때문이다. 우리는 POST 메소드를 전송한다. 증명하기 위해 nginx container을 초기화시키고 재실행 해보았다. 역시 49초 언저리로 나왔다.그렇다면 원인은 nginx는 아니고, test code 아니고, auth-server에 있다. Spring Server warming-up process스프링서버는 대부분이 LAZY로 되어있어서 초기 데이터들로 워밍업되기 전까지는 아직 완벽히 initialize되지 않는다. 따라서, 스프링 서버가 워밍업(JPA, driver, etc.)되기 시간이 초기 테스트에 포함되기때문에 시간이 더 소요된다고 볼 수 있다.reference : https://stackoverflow.com/questions/63019528/why-are-spring-rest-services-slow-on-the-first-request" }, { "title": "실시간 채팅방 구현(12) - (대량 Rest api test 한계 돌파)", "url": "/posts/chatting(12)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-15 00:00:25 +0000", "snippet": "INDEX 랜덤값으로 test하는 code 10000개 rest api 트래픽 실행(110초 소요)이제 동시 연결가능한 TCP 소켓을 필자의 맥북이 버틸 수 있는만큼 열어서 다량의 http request를 서버로 전송해볼려고 한다.\tt := http.DefaultTransport.(*http.Transport).Clone()\tt.MaxIdleConns = 10000\tt.MaxConnsPerHost = 10000\tt.MaxIdleConnsPerHost = 1\t// 클라이언트 설정 및 timeout\tclient := &amp;http.Client{\t\tTimeout: 1 * time.Second,\t\tTransport: t,\t}Transport 설정을 잠깐 설명하자면 아래와 같다. MaxIdleConns : 전체 커넥션 풀 MaxConnsPerHost : 호스트 별(서버) 할당 가능한 커넥션 개수 MaxIdleConnsPerHost : 호스트 별 활성화 되지 않아도 유지하는 최대 connection 개수.(Server-Sent Event같이 클라이언트가 따로 뭘 안보내도 서버가 계속 보내야 될 때 필요하다. 본 테스트에서는 필요업으니 pass) Dial.Timeout = tcp 소켓 유지 타임아웃(Keep-alive 시간) - http.Client.Timeout랑 같다. http.Client.Timeout : 클라이언트(테스트 클라이언트)가 송신/수신이 끝나면 connection 유지하는 시간현재 필자의 맥북은 약 tcp 소켓을 동시에 10K까지는 무난히 열 수 있다. 따라서 10K로 커넥션 풀을 설정한다. 또한 status code 200을 수신받기 위한 수정된 코드를 실행하며 스레드는 100개로 실행한다.랜덤값으로 test하는 code...type User struct {\tUserId string `json:\"userId\"`\tUserName string `json:\"userName\"`\tEmail string `json:\"email\"`\tUserPw string `json:\"userPw\"`}type Usera struct {\tName string}func init() {\trand.Seed(time.Now().UnixNano())}var letterRunes = []rune(\"김이박최정강조윤장임한오서신권황안송류전홍고문양손배조백허유남심노정하곽성차주우구신임나전민유진지엄채원천방공강현함변염양변여추노도소신석선설마길주연방위표명기반왕금옥육인맹제모장남탁국여진어은편구용\")var letterRunes_name = []rune(\"가강건경고관광구규근기길나남노누다단달담대덕도동두라래로루리마만명무문미민바박백범별병보빛사산상새서석선설섭성세소솔수숙순숭슬승시신아안애엄여연영예오옥완요용우원월위유윤율으은의이익인일잎자잔장재전정제조종주준중지진찬창채천철초춘충치탐태택판하한해혁현형혜호홍화환회효훈휘희운모배부림봉혼황량린을비솜공면탁온디항후려균묵송욱휴언령섬들견추걸삼열웅분변양출타흥겸곤번식란더손술훔반빈실직흠흔악람뜸권복심헌엽학개롱평늘늬랑얀향울련\")const letterBytes_id = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"func RandStringBytes(n int, letterBytes string) string {\tb := make([]byte, n)\tfor i := range b {\t\tb[i] = letterBytes[rand.Intn(len(letterBytes))]\t}\treturn string(b)}func RandStringRunes_firstname(n int) string {\tb := make([]rune, n)\tfor i := range b {\t\tb[i] = letterRunes[rand.Intn(len(letterRunes))]\t}\treturn string(b)}func RandStringRunes_lastname(n int) string {\tb := make([]rune, n)\tfor i := range b {\t\tb[i] = letterRunes_name[rand.Intn(len(letterRunes_name))]\t}\treturn string(b)}func worker(contexts *sync.Map, wg *sync.WaitGroup, requestURL string, jsonBody []byte, client *http.Client, transferRatePerSecond int, number_worker int) {\tdefer wg.Done()\tfmt.Println(\"Threads start:\", number_worker)\t// 로컬 맵 생성\tm := make(map[int]int)\tfor i := 0; i &lt; transferRatePerSecond; i++ {\t\ts := &amp;User{\t\t\tUserId: RandStringBytes(8, letterBytes_id),\t\t\tUserName: RandStringRunes_lastname(1) + RandStringRunes_firstname(2),\t\t\tEmail: RandStringBytes(5, letterBytes_id) + \"@gmail.com\",\t\t\tUserPw: RandStringBytes(10, letterBytes_id),\t\t}\t\tbuf, err := json.Marshal(s)\t\tif err != nil {\t\t\tlog.Fatal(err)\t\t\treturn\t\t}\t\tbodyReader := bytes.NewReader(buf)\t\t// 요청 생성\t\treq, err := http.NewRequest(http.MethodPost, requestURL, bodyReader)\t\tif err != nil {\t\t\tos.Exit(1)\t\t}\t\t// json 헤더 설정\t\treq.Header.Set(\"Content-Type\", \"application/json\")\t\t// 실제 요청 전송 및 반환\t\tres, _ := client.Do(req)\t\t// 로컬 맵에 삽입\t\tm[res.StatusCode] += 1\t}\tfor k, v := range m {\t\tresult, ok := contexts.Load(k)\t\tif ok {\t\t\tcontexts.Store(k, result.(int)+v)\t\t} else {\t\t\tcontexts.Store(k, v)\t\t}\t}}func main() {\targsWithoutProg := os.Args[1:]\tvar wg sync.WaitGroup\tnumber_worker := 100\t// http 전송 url\trequestURL := argsWithoutProg[0]\t// json 파일 경로 ex) user\t// .json 확장자 명은 제외\tjsonReq := argsWithoutProg[1]\t// 실행횟수 설정\ttransferRatePerSecond, _ := strconv.Atoi(argsWithoutProg[2])\t// JSON 파일 읽기\tjsonFile, err := os.Open(jsonReq + \".json\")\tif err != nil {\t\tfmt.Println(err)\t}\tdefer jsonFile.Close()\tt := http.DefaultTransport.(*http.Transport).Clone()\tt.MaxIdleConns = 10000 // connection pool 크기\tt.MaxConnsPerHost = 10000 // 호스트 별 최대 할당 connection\tt.MaxIdleConnsPerHost = 1\t// 클라이언트 설정 및 timeout\tclient := &amp;http.Client{\t\tTimeout: 5 * time.Second,\t\tTransport: t,\t}\t// json파일을 바이트로 변환\tjsonBody, _ := ioutil.ReadAll(jsonFile)\t// 스레드 싱크 맵\tvar contexts = &amp;sync.Map{}\t// 멀티 스레드 http request\tfor i := 0; i &lt; number_worker; i++ {\t\twg.Add(1)\t\tgo worker(contexts, &amp;wg, requestURL, jsonBody, client, transferRatePerSecond/number_worker, i)\t}\twg.Wait()\t// 성공한 http request 개수 확인\tcontexts.Range(func(k, v interface{}) bool {\t\tfmt.Println(\"status: \", k, \" count: \", v)\t\treturn true\t})}10000개 쿼리 실행 결과(110초 소요)결과로 빠짐없이 정상 200 code 를 받는것을 확인할 수 있으며, db에도 값에 정상저장되는 것을 확인할 수 있다. terminal gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go 10000실행시간 110.251228122status: 200 count: 10000 auth DB chat DB backup DB - chat DB" }, { "title": "실시간 채팅방 구현(11) - (대량 Rest api test 코드)", "url": "/posts/chatting(11)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-15 00:00:25 +0000", "snippet": "현재까지 auth/chat Server 및 여러 장애대응과 모니터링 서비스를 구축했다.서비스관련 테스트 코드는 이미 생성하였으며, 이제는 다량의 Rest api 트래픽을 테스트를 위한 클라이언트를 제작해 볼 차례이다. 아래의 코드는 Golang으로 제작하였다. 예상되는 아래의 질문에 대한 답변을 준비해봤다. Question : 왜 Java가 아닌 Golang으로 제작했나요? Answer 1 : 간단한 테스트를 위해 JVM까지 메모리에 올릴필요는 없다고 생각하기때문이다. Golang은 컴파일 언어로써 오브젝트 파일만 있으면 실행 가능하다. Answer 2 : Golang은 경량 스레드인 go-routine을 사용(M:N), Java는 네이티브 스레드를 사용한다. 크기를 비교하면 Golang은 스레드 크기가 2KB인데 비해, Java는 스레드 크기가 1MB이상이다. 이는 즉, 스케쥴링에 있어 Golang이 Context Switch에서 이점을 가져가며 동시에 더 많은 스레드를 실행시킬 수 있다는 이점을 가지고 있다는 것이다. 관련된 자료는 필자의 Goroutine structure and behavior 포스팅에 있다. package main...func worker(finished chan map[int]int, requestURL string, bodyReader *bytes.Reader, client *http.Client, transferRatePerSecond int) {\t// 로컬 맵 생성\tm := make(map[int]int)\tfor i := 0; i &lt; transferRatePerSecond; i++ {\t\t// 요청 생성\t\treq, err := http.NewRequest(http.MethodPost, requestURL, bodyReader)\t\tif err != nil {\t\t\tfmt.Printf(\"client: could not create request: %s\\n\", err)\t\t\tos.Exit(1)\t\t}\t\t// json 헤더 설정\t\treq.Header.Set(\"Content-Type\", \"application/json\")\t\t// 실제 요청 전송 및 반환\t\tres, _ := client.Do(req)\t\t// 로컬 맵에 삽입\t\tm[res.StatusCode] += 1\t}\t// 완료 후 채널로 로컬 맵 전송\tfinished &lt;- m}func main() {\targsWithProg := os.Args\targsWithoutProg := os.Args[1:]\t// http 전송 url\trequestURL := argsWithoutProg[0]\t// json 파일 경로 ex) user\t// .json 확장자 명은 제외\tjsonReq := argsWithoutProg[1]\t// 실행횟수 설정\ttransferRatePerSecond, _ := strconv.Atoi(argsWithoutProg[2])\t// JSON 파일 읽기\tjsonFile, err := os.Open(jsonReq + \".json\")\tif err != nil {\t\tfmt.Println(err)\t}\tdefer jsonFile.Close()\t// 클라이언트 설정 및 timeout\tclient := &amp;http.Client{\t\tTimeout: 30 * time.Second,\t}\t// json파일을 바이트로 변환\tjsonBody, _ := ioutil.ReadAll(jsonFile)\tbodyReader := bytes.NewReader(jsonBody)\t// 스레드 싱크 맵\tvar contexts = sync.Map{}\tfinished := make(chan map[int]int)\t// 멀티 스레드 http request\tgo worker(finished, requestURL, bodyReader, client, transferRatePerSecond)\t// 스레드 별 채널을 통한 완료 확인 및 스레드의 맵 가져오기\tm := &lt;-finished\t// 스레드 별 완료값 꺼내서 mutex lock해주는 맵 저장\tfor k, v := range m {\t\tresult, ok := contexts.Load(k)\t\tif ok {\t\t\tcontexts.Store(k, result.(int)+v)\t\t} else {\t\t\tcontexts.Store(k, v)\t\t}\t}\t// 성공한 http request 개수 확인\tcontexts.Range(func(k, v interface{}) bool {\t\tfmt.Println(\"status: \", k, \" count: \", v)\t\treturn true\t})}[문제점1] 400 status code error그런데 문제점이 발생했다. 위의 테스트 코드를 실행시키면 아래와 같이 결과를 받는다. 실행gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://localhost:8080/auth/user user 50[http://localhost:8080/auth/user user 50]status: 409 count: 1 &lt;- 예상된 결과status: 400 count: 49 &lt;- 왜?;; 서버로그auth-server | 2023-01-16 05:55:17.348 INFO 1 --- [nio-8085-exec-9] chatting.chat.web.UserController : request URI=/auth/userauth-server | 2023-01-16 05:55:17.350 DEBUG 1 --- [nio-8085-exec-9] org.hibernate.SQL :auth-server | selectauth-server | user0_.user_id as user_id1_0_0_,auth-server | user0_.email as email2_0_0_,auth-server | user0_.join_date as join_dat3_0_0_,auth-server | user0_.login_date as login_da4_0_0_,auth-server | user0_.logout_date as logout_d5_0_0_,auth-server | user0_.user_name as user_nam6_0_0_,auth-server | user0_.user_pw as user_pw7_0_0_auth-server | fromauth-server | user_table user0_auth-server | whereauth-server | user0_.user_id=?auth-server | 2023-01-16 05:55:17.351 TRACE 1 --- [nio-8085-exec-9] o.h.type.descriptor.sql.BasicBinder : binding parameter [1] as [VARCHAR] - [a]auth-server | 2023-01-16 05:55:17.355 ERROR 1 --- [nio-8085-exec-9] c.chat.web.error.GlobalExceptionHandler : handleCustomException throw CustomException : DUPLICATE_RESOURCE# ------------------------------ 정상 에러 ------------------------------nginx | 192.168.192.1 - - [16/Jan/2023:05:55:17 +0000] \"POST /auth/user HTTP/1.1\" 409 162 \"-\" \"Go-http-client/1.1\" \"-\"# ------------------------------ 비정상 에러 ------------------------------nginx | 192.168.192.1 - - [16/Jan/2023:05:55:17 +0000] \"POST /auth/user HTTP/1.1\" 400 0 \"-\" \"Go-http-client/1.1\" \"-\"nginx | 192.168.192.1 - - [16/Jan/2023:05:55:17 +0000] \"POST /auth/user HTTP/1.1\" 400 0 \"-\" \"Go-http-client/1.1\" \"-\"nginx | 192.168.192.1 - - [16/Jan/2023:05:55:17 +0000] \"POST...HTTP client in Go by default has DefaultMaxIdleConnsPerHost value of 2같은 ID로 auth server에 http reqeust하기 때문에, status code 409은 분명 예상된 결과이다. 하지만! 400 에러는 원하는 결과가 아니다. 애초에 nginx를 통해 auth-server로 첫 번째 request만 들어가고 나머지는 팅겨나가는 현상을 관찰할 수 있다. 그러면 test code가 문제이거나 nginx configuration이 잘못되었다라고 볼 수 있다.[문제점1 해결] byte코드 오류...func worker(finished chan map[int]int, requestURL string, jsonBody []byte, client *http.Client, transferRatePerSecond int) { ...\tfor i := 0; i &lt; transferRatePerSecond; i++ {\t # bodyReader은 포인터이기 때문에, 사용 이전과 이후가 다르다\t\tbodyReader := bytes.NewReader(jsonBody)\t\t...\t} ...}이전 bodyReader = &amp;{[123 10 ...] 0 -1}전송 이후의 bodyReader &amp;{[123 10 ...] 89 -1}https://go.dev/src/compress/gzip/example_test.go를 보면 아래의 레퍼런스가 있다. Note that passing req to http.Client.Do promises that itwill close the body, in this case bodyReader.즉, 89는 이미 소비된 bodyReader을 표시하는 바이트이다. 따라서 이를 다시 resend하게 된다면 net/http 라이브러리에서 이를 읽지 않기때문에, 해당 바이트를 다시 0으로 재설정하는 과정을 가져야한다.따라서 이를 아래와 같이 수정하여 정상에러 결과를 받아볼 수 있다.수정된 test code...func worker(contexts *sync.Map, wg *sync.WaitGroup, requestURL string, jsonBody []byte, client *http.Client, transferRatePerSecond int, number_worker int) {\tdefer wg.Done()\tfmt.Println(\"Threads start:\", number_worker)\t// 로컬 맵 생성\tm := make(map[int]int)\tfor i := 0; i &lt; transferRatePerSecond; i++ {\t\tbodyReader := bytes.NewReader(jsonBody)\t\t// 요청 생성\t\treq, err := http.NewRequest(http.MethodPost, requestURL, bodyReader)\t\tif err != nil {\t\t\tfmt.Printf(\"client: could not create request: %s\\n\", err)\t\t\tos.Exit(1)\t\t}\t\t// json 헤더 설정\t\treq.Header.Set(\"Content-Type\", \"application/json\")\t\t// 실제 요청 전송 및 반환\t\tres, _ := client.Do(req)\t\t// 로컬 맵에 삽입\t\tm[res.StatusCode] += 1\t}\tfor k, v := range m {\t\tresult, ok := contexts.Load(k)\t\tif ok {\t\t\tcontexts.Store(k, result.(int)+v)\t\t} else {\t\t\tcontexts.Store(k, v)\t\t}\t}}func main() {\targsWithoutProg := os.Args[1:]\tvar wg sync.WaitGroup\tnumber_worker := 10\t// http 전송 url\trequestURL := argsWithoutProg[0]\t// json 파일 경로 ex) user\t// .json 확장자 명은 제외\tjsonReq := argsWithoutProg[1]\t// 실행횟수 설정\ttransferRatePerSecond, _ := strconv.Atoi(argsWithoutProg[2])\t// JSON 파일 읽기\tjsonFile, err := os.Open(jsonReq + \".json\")\tif err != nil {\t\tfmt.Println(err)\t}\tdefer jsonFile.Close()\tt := http.DefaultTransport.(*http.Transport).Clone()\tt.MaxIdleConns = 1000 // connection pool 크기\tt.MaxConnsPerHost = 1000 // 호스트 별 최대 할당 connection\tt.MaxIdleConnsPerHost = 1000\tt.Dial = (&amp;net.Dialer{\t\tTimeout: 1 * time.Second,\t}).Dial\t// 클라이언트 설정 및 timeout\tclient := &amp;http.Client{\t\tTimeout: 1 * time.Second,\t\tTransport: t,\t}\t// json파일을 바이트로 변환\tjsonBody, _ := ioutil.ReadAll(jsonFile)\t// 스레드 싱크 맵\tvar contexts = &amp;sync.Map{}\t// 멀티 스레드 http request\tfor i := 0; i &lt; number_worker; i++ {\t\twg.Add(1)\t\tgo worker(contexts, &amp;wg, requestURL, jsonBody, client, transferRatePerSecond/number_worker, i)\t}\twg.Wait()\t// 성공한 http request 개수 확인\tcontexts.Range(func(k, v interface{}) bool {\t\tfmt.Println(\"status: \", k, \" count: \", v)\t\treturn true\t})}여기서 추가된 중요한 코드는 아래와 같다.\tt := http.DefaultTransport.(*http.Transport).Clone()\tt.MaxIdleConns = 100\tt.MaxConnsPerHost = 100\tt.MaxIdleConnsPerHost = 100\tt.Dial = (&amp;net.Dialer{\t\tTimeout: 5 * time.Second,\t}).Dial\tt.TLSHandshakeTimeout = 5 * time.Second\t// 클라이언트 설정 및 timeout\tclient := &amp;http.Client{\t\tTimeout: 10 * time.Second,\t\tTransport: t,\t}기본적으로 동시 연결 가능 tcp 소켓 개수는 net/http에서는 2개로 설정되어있다. 따라서 우리는 100개의 동시 tcp 소켓으로 맞춰주고, 각각의 소켓이 5초 이내로 재 dial되지 않으면 종료하도록 설정해준다.이 밖에 number_worker 스레드 개수는 10개로 설정하여 시뮬레이션한다.결과gyuminhwangbo@Gyuminui-MacBookPro testing % go run main.go http://localhost:8080/auth/user user 200Threads start: 8Threads start: 1Threads start: 7Threads start: 6Threads start: 3Threads start: 9Threads start: 0Threads start: 2Threads start: 4Threads start: 5status: 409 count: 200 &lt;-- 원하는 결과 도출" }, { "title": "실시간 채팅방 구현(10) - (양방향 DB Sync 구현의 어려움 + 비용)", "url": "/posts/chatting(10)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-08 00:00:25 +0000", "snippet": "서론단방향까지는 구현이 가능했었는데, 양방향(BDR)은 아래의 이유로 굉장히 까다로웠다. 컨셉만 존재할 뿐, 코드 레퍼런스가 없다. 현업자분들께 방향성을 여쭙고싶다. 두 개의 마스터 노드일 때는 그래도 할 순 있겠지만, 그 이상일 때는 각각을 전부다 설정해줘야하기때문에 상당히 까다롭다. 즉, 클러스터 구현을 직접 해야한다. 동시성 문제가 대두된다. 예로 만약 master-db-1과 master-db-2에서 동시에 같은 키의 데이터가 삽입되었을 때 crash error가 발생한다. 일차적 방어방법으로 같은 key가 각각의 DB에 동시적으로 들어오지 못하게 ip 해시값 기반 라우팅 로드 밸런싱을 적용했지만, 누군가 악의적으로 동시에 다른 ip로 같은 key를 삽입요청할 수도 있다. 네트워크 장애로 replication data 유실 시, 장애대응이 미비하다. 예로 데이터 유실 시, 다시 replication 데이터를 보내줘야되는데 이런 설정또한 까다롭다. 위의 4가지 이유 이외 진짜 여러가지 문제점이 발생가능하다. 그래서 DBA가 아닌 취준 개발자 입장에서는, 이미 안정적이며 체계적인 cloud의 솔루션을 사용할 수 밖에 없는것 같다. AWS : Additional steps to set up bi-directional logical replication그런데 너무 비싸다!!!! 필자는 현재 포트폴리오 프로젝트(EC2, route-53), 뱅킹 백엔드 프로젝트(secret, ECR, EKS, etc.)을 사용하고 있는데 한달에 12만원 가량 고정 지출이 발생한다. 여기에 예상되는 postgres DB 비용을 계산해보자면, 다음과 같다.제일 싼 db.t4g.micro의 시간당 요금 =\t0.032 USD * 24시간 * 30일 * 2대 = 46 USD = 57,258 WON. 약 고정지출로 20만원이 사라지게 될것이다. 취준생에게는 매우 큰 돈이다정말 분산 DB는 MSA에서 빠질 수 없는 주제이며 정말 중요한 기능이다. 하지만 지금은 잠시 위의 이유로 양방향 DB sync는 미뤄두도록 하겠다. 그리고 이제는 Front에 집중을 해볼것이다." }, { "title": "실시간 채팅방 구현(9) - (단방향 DB Sync with Kafka connector)", "url": "/posts/chatting(9)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-04 00:00:25 +0000", "snippet": "INDEX DB sync 시 고려할점 어떻게 두 개의 master DB를 sync 해야할까? Kafka connector란? Kafka Source connector setting postgres wal_level 설정 debezium connector 설정(source) connector kafka 등록(schema configuration) DB에 값 추가했을 때 실제로 Kafka로 흘러가는지 확인 Kafka Sink connector setting postgres-kafka-source-connector 컨테이너 실행 jdbc-connector 설치 및 삽입 sink-connector configuration uni-directional DB sink 결과 현재 단방향까지 구현된 버전은 v3.1.0이며, 해당 코드의 다양한 버전은 아래의 깃허브에 존재한다. https://github.com/ghkdqhrbals/spring-chatting-server앞서 연동에 추가할 부분이 있다. 채팅서비스를 두개로 실행하는데, 문제는 DB가 서로 독립이라는 점이다. 따라서 두 개중, 어느 DB가 INSERT/ALTER 등이 된다면 다른 DB도 같은 트랜젝션을 수행해야한다. 즉, 분산 DB이면서 서로 sync되도록 해야된다. 이렇게 DB를 따로 뗀 이유는 수평확장시키기 좋기 때문이다. 이러한 두 개의 DB 모두 master DB로 수행된다.1. DB sync 시 고려할점이 부분에서 고려할 점은 다음과 같다. 백업 DB처럼 단반향 sync가 아닌 양방향 sync 를 해야하기 때문에 서로 맞물리는 무한루프를 조심해야한다. 보통 Source/Target DB를 정하고 CDC(Change Data Capture)후 Sync를 하는경우가 대부분이다.양방향 sync는 좀더 까다로운것 같다. 아래는 양방향에 있어 발생가능한 이슈 및 해결방법이다. Second, you’ll need to make sure to not propagate the same data change forth and back in an infinite loop. One way of doing so could for instance be an SMT(단일 메시지 변환) which you apply to both sources and which adds a Kafka Connect header property representing the “origin” of a given change. In your sink connector, you’d then add that origin as an additional column to your data as you update it. The source connector on that side would then have to be set up (e.g. again using an SMT) to ignore all the changes which originate from replication, as opposed to actual data changes e.g. done by a business application. Issue from https://groups.google.com/g/debezium/c/YS22DAgFXSc 1-1. 여기서 SMT란? Single Message Transforms (SMTs) is a Kafka API that provides a simple interface for manipulating records as they flow through both the source and sink side of your data pipeline. reference https://camel.apache.org/camel-kafka-connector/3.18.x/reference/transformers/index.html위의 레퍼런스를 번역하자면 SMTs는 source connector or sink connector에서 레코드의 칼럼명이나 value 등을 변경시켜서 전달해주는 kafka 편의기능 rest api 이다. 아래는 다양한 변환방법이다. Some common uses for transforms are: Renaming fields( 칼럼명 재정의 ) Masking values( 특정 칼럼의 value를 valid null로 만듬 ex) {value} to 0 or “” or false ) Routing records to topics based on a value( cloud 에서는 안됨 ) Converting or inserting timestamps into the record Manipulating keys, like setting a key from a field’s value( 카프카는 key를 통해 원하는 메세지를 가져올 수 있다. 이 방법은 특정 칼럼의 value를 key로 변환해주는 방법이다 ) reference https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples/?_ga=2.130915337.76772118.1672804235-1001218784.1670749352&amp;_gac=1.191662808.1671423652.CjwKCAiAkfucBhBBEiwAFjbkr7Bq_5Npm8yLue-N4DKIv4hpPc44IdpcBYN3ITQzeAAdIkGX2Y5wJRoCBYIQAvD_BwE2. 어떻게 두 개의 master DB를 sync 해야할까?필자는 Kafka connector을 이용하여 싱크를 맞추려한다. 그러기 위해서는 Kafka connector에 대한 이해가 바탕이 되어야한다.2-1. Kafka connector란?Kafka connector의 기본적인 플로우는 다음과 같다. RDB에 INSERT/UPDATE/ALTER 등 변경되는 트랜젝션이 실행되고 TXlog에 기록된다 Kafka Connector은 이를 읽고(CDC) Kafka의 Topic에 삽입한다이 때 이 connector을 우리는 source conenctor이라고 부른다. 그리고 Kafka ---&gt; DB를 연결시켜주는 connector은 sink connector이라고 부른다.일단 먼저 Kafka source connector을 설정해보고, kafdrop으로 실제 CDC되는지 관찰해보자2-2. Kafka Source connector settingKafka soruce connector은 아래의 4가지 과정을 거쳐 설정 및 확인할 수 있다. postgres wal_level 설정 debezium connector 설정 connector kafka 등록 DB에 값 추가했을 떄 실제로 Kafka로 흘러가는지 확인2-2-1. postgres wal_level 설정 먼저 wal-level이 무엇일까?WAL은 Write-Ahead Logging의 약자로 트랜젝션로그에 어떤식으로 변경된 사항을 저장할 지 정하는 설정이다.WAL은 크게 Logical과 Replica가 존재한다. Logical level : 레코드 값이 변경되면, 변경된 레코드 전체가 저장된다. 예시 wal_level = logical lsn | xid | data-----------+-----+------------------------------------------------------------------------0/703F738 | 583 | BEGIN 5830/703F738 | 583 | table public.mytab: INSERT: id[integer]:3 name[character varying]:'t3'0/703F838 | 583 | table public.mytab: INSERT: id[integer]:4 name[character varying]:'t4'0/703F8A8 | 583 | COMMIT 5830/703F8E0 | 584 | BEGIN 5840/703F8E0 | 584 | table public.mytab: DELETE: (no-tuple-data)0/703F948 | 584 | COMMIT 584 statement + row 기반으로 저장된다고 볼 수 있다. Replica level : 레코드 값이 변경되면, 레코드 내 변경된 값 부분만 트랜젝션 로그에 저장한다.그리고 이러한 WAL level은 postgres에서는 기본적으로 replica로 설정되어있다(9.4버전 이후부터 logcal을 지원한다). 이 replica level은 debezium kafka connector에서는 지원하지 않는다. 즉, 레코드 전체값이 적혀있는 트랜젝션 로그(logical level)를 보고 CDC하도록 설정되어있다. 따라서 우리는 이 default replica level을 logical로 아래와같이 바꿔줘야한다. chatting-db-1: container_name: chatting-db-1 image: postgres:12-alpine environment: - POSTGRES_PASSWORD=password - POSTGRES_USER=postgres - POSTGRES_DB=chat1 expose: - \"5433\" # Publishes 5433 to other containers but NOT to host machine ports: - \"5433:5433\" volumes: - ./backups:/home/backups command: -c wal_level=logical -p 5433 # logical로 변경 restart: always chatting-db-2: container_name: chatting-db-2 image: postgres:12-alpine environment: - POSTGRES_PASSWORD=password - POSTGRES_USER=postgres - POSTGRES_DB=chat2 expose: - \"5434\" # Publishes 5433 to other containers but NOT to host machine ports: - \"5434:5434\" volumes: - ./backups:/home/backups command: -c wal_level=logical -p 5434 # logical로 변경 restart: always2-2-2. debezium connector 설정이제 DB설정은 끝났고, DB의 트랜젝션 로그의 변경사항을 관찰(CDC)하고 Kafka 토픽에 삽입해주는 connector을 컨테이너로 아래와 같이 띄울 것이다. # -------- postgres -&gt; kafka source connector -------- kafka-source-connector: image: debezium/connect:1.9 container_name: postgres-kafka-source-connector ports: - 8083:8083 environment: CONFIG_STORAGE_TOPIC: my_connect_configs OFFSET_STORAGE_TOPIC: my_connect_offsets STATUS_STORAGE_TOPIC: my_connect_statuses BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092 depends_on: - kafka1 - kafka2 - kafka3 - zookeeper - chatting-db-22-2-3. connector kafka 등록이렇게 DB, Connector-Kafka 을 띄웠다면 이제는 서로 연결해주어야할 차례이다. 우리는 Kafka connector가 지원하는 restapi를 통해 연결시켜줄 수 있다.POST http://localhost:8083/connectors{ \"name\": \"source-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"plugin.name\": \"pgoutput\", \"database.hostname\": \"chatting-db-2\", \"database.port\": \"5434\", \"database.user\": \"postgres\", \"database.password\": \"password\", \"database.dbname\" : \"chat2\", \"database.server.name\": \"dbserver5434\", \"transforms\": \"unwrap,addTopicPrefix\", # message의 schema를 after 필드로만 설정 # 이 부분을 설정하지 않는다면 source와 sink connector의 schema가 일치하지 않는다. # 즉, kafka로 메세지가 흘러갈 순 있지만 kafka에서 db로 sink가 진행되지 않을것이다. \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\", \"transforms.addTopicPrefix.type\":\"org.apache.kafka.connect.transforms.RegexRouter\", \"transforms.addTopicPrefix.regex\":\"(.*)\", \"transforms.addTopicPrefix.replacement\":\"$1\" }}2-2-4. DB에 값 추가했을 때 실제로 Kafka로 흘러가는지 확인이제는 실제로 확인해 볼 차례이다. 우리는 다음과 같이 확인해볼것이다.This is final process. POST to server POST http://localhost:8080/chat/user { \"userId\":\"aa\", \"userName\":\"황보규민\" } In docker container log chatting-server-2 | 2023-01-05 05:55:07.829 INFO 1 --- [ad | producer-1] chatting.chat.web.ChatController : 메세지 전송 성공 topic=log-user-add, offset=0, partition=2 See kafka with Kafdrop 아래는 발생한 에러와 해결한 방법에 대해 정리했다. 이슈 Connector configuration is invalid and contains the following 1 error(s) Error while validating connector config: Connection to localhost:5434 refused 이슈 해결방법 정리 : https://github.com/ghkdqhrbals/spring-chatting-server/issues/1 2-3. Kafka Sink connector setting자! 이제 DB-&gt;Kafka는 완료되었으니, Kafka-&gt;DB로 Sink connector을 구축해야한다. 다음의 동영상을 참고하자.https://youtu.be/2bPx3hfKX04위의 동영상은 confluent의 cloud로 connector을 설정하는 방법이다. 하지만 필자는 굳이 cloud로 설정할 필요없다고 생각했다. 따라서 source connector에서 사용하던 debezium connector 컨테이너에 jdbc-connector만 추가해서 사용하는 방법으로 가기로 하였다.2-3-1. postgres-kafka-source-connector 컨테이너 실행 kafka-source-connector: image: debezium/connect:1.9 container_name: postgres-kafka-source-connector ports: - 8083:8083 environment: CONFIG_STORAGE_TOPIC: __pg.source.config.storage OFFSET_STORAGE_TOPIC: __pg.source.offset.storage STATUS_STORAGE_TOPIC: __pg.source.status.storage PLUGIN_PATH: /kafka/connect # connector 플러그인 저장소 위치 BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092 depends_on: - kafka1 - kafka2 - kafka3 - zookeeper - chatting-db-1 - chatting-db-22-3-2. jdbc-connector 설치 및 삽입아래와 같이 shell script를 작성해서 실행한다. 해당 shell script를 실행하기 전에 먼저 postgres-kafka-source-connector 컨테이너가 필요하다.#!/bin/bashecho \"(step-1) confluent-hub cli 다운로드 및 압축해제\"curl -LO http://client.hub.confluent.io/confluent-hub-client-latest.tar.gzmkdir confluent-etcs | tar -xvzf confluent-hub-client-latest.tar.gz -C confluent-etcs#echo \"(step-2) confluent-hub 환경변수 설정\"echo $(pwd)export CONFLUENT_HOME=$(pwd)/confluent-etcsexport PATH=$PATH:$CONFLUENT_HOME/bin#echo \"(step-3) cli를 통한 jdbc connector 다운로드\"$CONFLUENT_HOME/bin/confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 --component-dir $(pwd)/confluent-etcs#echo \"(step-4) debezium connector 컨테이너의 connector리스트에 삽입\"docker cp $(pwd)/confluent-etcs/confluentinc-kafka-connect-jdbc postgres-kafka-source-connector:/kafka/connectecho \"(step-5) debezium connector 컨테이너 재시작으로 loading new connector\"docker restart postgres-kafka-source-connector이렇게 설치를 끝내고 GET http://localhost:8083/connector-plugins 을 전송하면 아래와 같이 정상적으로 JdbcSink/SourceConnector와 연동된 것을 확인할 수 있다.[ { \"class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\", \"type\": \"sink\", \"version\": \"10.6.0\" }, { \"class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\", \"type\": \"source\", \"version\": \"10.6.0\" }, ... { \"class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"type\": \"source\", \"version\": \"1.9.7.Final\" }, ...]2-3-3. sink-connector configurationPOST http://localhost:8083/connectors{ \"name\": \"sink-connector\", \"config\": { \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\", \"task.max\" : 1, \"topics\": \"dbserver5434.public.user_table\", \"connection.url\": \"jdbc:postgresql://chatting-db-1:5433/chat1\", \"connection.user\":\"postgres\", \"connection.password\":\"password\", # table/column 자동생성 방지 # 두개의 테이블이 이미 동일함 \"auto.create\": \"false\", \"auto.evolve\": \"false\", \"delete.enabled\": \"true\", \"insert.mode\": \"upsert\", \"pk.mode\": \"record_key\", \"tombstones.on.delete\": \"true\", # schema일치 확인 및 payload 추출 과정 \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"key.converter.schemas.enable\": \"true\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\": \"true\", \"transforms\": \"unwrap,addTopicPrefix\", \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\", \"transforms.addTopicPrefix.type\":\"org.apache.kafka.connect.transforms.RegexRouter\", \"transforms.addTopicPrefix.regex\":\"(.*)\", \"transforms.addTopicPrefix.replacement\":\"$1\", # 해당 테이블에 new row 삽입 \"table.name.format\":\"user_table\", # 몇 개의 메세지를 읽고 sink할 것인지 \"batch.size\": \"1\" }}최종적으로 Kafka에서 dbserver5434.public.user_table 토픽에 대한 schema는 아래와 같이 설정된다.{ \"schema\": { \"type\": \"struct\", \"fields\": [ { \"type\": \"string\", \"optional\": false, \"field\": \"user_id\" }, { \"type\": \"string\", \"optional\": true, \"field\": \"user_name\" }, { \"type\": \"string\", \"optional\": true, \"field\": \"user_status\" } ], \"optional\": false, \"name\": \"dbserver5434.public.user_table.Value\" }, \"payload\": { \"user_id\": \"a\", \"user_name\": \"a\", \"user_status\": \"a\" }}2-4. uni-directional DB sink 결과일단 단방향 sink 설정은 이걸로 끝이 났다. 한번 확인해보자.먼저 kafka-connector 컨테이너 실행 이후 install-jdbc-connector.sh 실행gyuminhwangbo@Gyuminui-MacBookPro spring-chatting-server % sh ./install-jdbc-connector.sh(step-1) confluent-hub cli 다운로드 및 압축해제 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 7584k 100 7584k 0 0 1131k 0 0:00:06 0:00:06 --:--:-- 1587kmkdir: confluent-etcs: File existsx share/doc/confluent-hub-client/notices/x share/doc/confluent-hub-client/licenses/x share/java/confluent-hub-client/jakarta.ws.rs-api-2.1.6.jar...(step-2) confluent-hub 환경변수 설정/Users/gyuminhwangbo/study/spring-chatting-server(step-3) cli를 통한 jdbc connector 다운로드Running in a \"--no-prompt\" modeImplicit acceptance of the license below:Confluent Community Licensehttps://www.confluent.io/confluent-community-licenseDownloading component Kafka Connect JDBC 10.6.0, provided by Confluent, Inc. from Confluent Hub and installing into /Users/gyuminhwangbo/study/spring-chatting-server/confluent-etcsImplicit confirmation of the question: Do you want to uninstall existing version 10.6.0?...(step-4) debezium connector 컨테이너의 connector리스트에 삽입(step-5) debezium connector 컨테이너 재시작으로 loading new connectorpostgres-kafka-source-connector그리고 connector configuration upload를 하였다.아래는 DB:5434과 연결된 chatServer의 api gateway(nginx)에 user_table의 insert api를 날렸을 때 터미널 상황이다.# (1) kafka-connector가 postgresDB:5434의 Txlog에서 변경사항 감지(CDC)postgres-kafka-source-connector | 2023-01-08 07:19:07,589 INFO Postgres|dbserver5434|streaming First LSN 'LSN{0/168BF58}' received [io.debezium.connector.postgresql.connection.WalPositionLocator]postgres-kafka-source-connector | 2023-01-08 07:19:07,589 INFO Postgres|dbserver5434|streaming WAL resume position 'LSN{0/168BF58}' discovered [io.debezium.connector.postgresql.PostgresStreamingChangeEventSource]postgres-kafka-source-connector | 2023-01-08 07:19:07,593 INFO Postgres|dbserver5434|streaming Connection gracefully closed [io.debezium.jdbc.JdbcConnection]postgres-kafka-source-connector | 2023-01-08 07:19:07,600 INFO Postgres|dbserver5434|streaming Connection gracefully closed [io.debezium.jdbc.JdbcConnection]# (Additional) POST 반환값nginx | 192.168.240.1 - - [08/Jan/2023:07:19:07 +0000] \"POST /chat/user HTTP/1.1\" 200 86 \"-\" \"PostmanRuntime/7.29.2\" \"-\"# (2) PgOutput - postgres가 기본적으로 제공하는 replica설정을 앞서 우리가# logical로 바꿧었다. 그리고 logical로 저장된 TXlog들을 디코딩해서 kafka에 밀어넣기 위해# PgOutput이라는 모듈을 통해 logicalTX----(decoding-PgOutput)----&gt;kafka를 수행한다.postgres-kafka-source-connector | 2023-01-08 07:19:07,614 INFO Postgres|dbserver5434|streaming Initializing PgOutput logical decoder publication [io.debezium.connector.postgresql.connection.PostgresReplicationConnection]chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] LOG: starting logical decoding for slot \"debezium\"chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] DETAIL: Streaming transactions committing after 0/168BDD8, reading WAL from 0/168BDD8.chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] STATEMENT: START_REPLICATION SLOT \"debezium\" LOGICAL 0/168BDD8 (\"proto_version\" '1', \"publication_names\" 'dbz_publication')chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] LOG: logical decoding found consistent point at 0/168BDD8chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] DETAIL: There are no running transactions.chatting-db-2 | 2023-01-08 07:19:07.620 UTC [82] STATEMENT: START_REPLICATION SLOT \"debezium\" LOGICAL 0/168BDD8 (\"proto_version\" '1', \"publication_names\" 'dbz_publication')postgres-kafka-source-connector | 2023-01-08 07:19:07,636 INFO Postgres|dbserver5434|streaming Requested thread factory for connector PostgresConnector, id = dbserver5434 named = keep-alive [io.debezium.util.Threads]postgres-kafka-source-connector | 2023-01-08 07:19:07,637 INFO Postgres|dbserver5434|streaming Creating thread debezium-postgresconnector-dbserver5434-keep-alive [io.debezium.util.Threads]postgres-kafka-source-connector | 2023-01-08 07:19:07,638 INFO Postgres|dbserver5434|streaming Processing messages [io.debezium.connector.postgresql.PostgresStreamingChangeEventSource]postgres-kafka-source-connector | 2023-01-08 07:19:07,654 INFO Postgres|dbserver5434|streaming Message with LSN 'LSN{0/168BF58}' arrived, switching off the filtering [io.debezium.connector.postgresql.connection.WalPositionLocator]postgres-kafka-source-connector | 2023-01-08 07:19:08,264 INFO || 1 records sent during previous 00:01:18.579, last recorded offset: {transaction_id=null, lsn_proc=23641944, lsn=23641944, txId=501, ts_usec=1673162347279067} [io.debezium.connector.common.BaseSourceTask]# (Additional) 이건 그냥 별개로 connector 안거치고 바로 kafka에 삽입하는 별도의 pipeline.chatting-server-2 | 2023-01-08 07:19:07.806 INFO 1 --- [ad | producer-1] chatting.chat.web.ChatController : 메세지 전송 성공 topic=log-user-add, offset=0, partition=1# (3) JDBC-Sink connector가 kafka-topic의 소비된 message의 last offset을 확인하고,# 신규 데이터 발견, postgresql에 대한 dialect를 만들어서 쿼리를 실행시키는 과정postgres-kafka-source-connector | 2023-01-08 07:19:08,287 INFO || [Producer clientId=connector-producer-source-connector-0] Resetting the last seen epoch of partition dbserver5434.public.user_table-0 to 0 since the associated topicId changed from null to oCJqYdENQ1C2cPZeNEhtsw [org.apache.kafka.clients.Metadata]postgres-kafka-source-connector | 2023-01-08 07:19:08,313 INFO || Attempting to open connection #1 to PostgreSql [io.confluent.connect.jdbc.util.CachedConnectionProvider]postgres-kafka-source-connector | 2023-01-08 07:19:08,412 INFO || Maximum table name length for database is 63 bytes [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]postgres-kafka-source-connector | 2023-01-08 07:19:08,412 INFO || JdbcDbWriter Connected [io.confluent.connect.jdbc.sink.JdbcDbWriter]postgres-kafka-source-connector | 2023-01-08 07:19:08,430 INFO || Checking PostgreSql dialect for existence of TABLE \"user_table\" [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]postgres-kafka-source-connector | 2023-01-08 07:19:08,440 INFO || Using PostgreSql dialect TABLE \"user_table\" present [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]postgres-kafka-source-connector | 2023-01-08 07:19:08,456 INFO || Checking PostgreSql dialect for type of TABLE \"user_table\" [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]postgres-kafka-source-connector | 2023-01-08 07:19:08,460 INFO || Setting metadata for table \"user_table\" to Table{name='\"user_table\"', type=TABLE columns=[Column{'user_name', isPrimaryKey=false, allowsNull=true, sqlType=varchar}, Column{'user_id', isPrimaryKey=true, allowsNull=false, sqlType=varchar}, Column{'user_status', isPrimaryKey=false, allowsNull=true, sqlType=varchar}]} [io.confluent.connect.jdbc.util.TableDefinitions]드디어! 단방향 설정이 끝났다. 이제는 양방향이 남았다. 포스팅이 너무 길어져서 양방향 설계는 다음 포스팅에서 설계하겠다. 아래는 고려하는 양방향 DB sync의 아키텍처이다.Reference https://www.confluent.io/blog/sync-databases-and-remove-silos-with-kafka-cdc/ One way DB sync Bi-directional DB sync https://medium.com/event-driven-utopia/configuring-debezium-to-capture-postgresql-changes-with-docker-compose-224742ca5372 https://stackoverflow.com/questions/59978213/debezium-could-not-access-file-decoderbufs-using-postgres-11-with-default-plug source connector configuration 문법 with debezium Postgres connector Debezium을 이용 source/sink connector 설정" }, { "title": "실시간 채팅방 구현(8) - (Kafka + Spring + ELK Stack 연동완료)", "url": "/posts/chatting(8)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2023-01-03 00:00:25 +0000", "snippet": "서론드디어 길고 긴 실시간 채팅방 구현 중 backend가 완료되었다! 경험많고 설계를 잘하시는 분들이 보면 간단해보이겠지만, 경험이 전무한 필자는 모든 구현과 기술 하나하나가 벅찼다. 진짜 응애 개발자로써 기술 문법부터 배워나가는 입장인데, 레퍼런스가 조금이라도 미비한 설명을 한다면 구글링만 3시간 넘게 할 수 밖에 없었다. 그리고 간단한 에러들도 해결하기 위해서 몇시간을 쏟아 부어야만했다. 그래서 이번 Backend 마무리 포스팅은 필자에게 상당히 뜻깊은 포스팅이다.Backend ArchitectureVisualized Kafka Traffics and othersRunning ContainersCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc8844bebca0f docker-elk_logstash \"/usr/local/bin/dock…\" 32 seconds ago Up 30 seconds 0.0.0.0:5044-&gt;5044/tcp, 0.0.0.0:9600-&gt;9600/tcp, 0.0.0.0:50000-&gt;50000/tcp, 0.0.0.0:50000-&gt;50000/udp docker-elk_logstash_110932c3ca0cf docker-elk_kibana \"/bin/tini -- /usr/l…\" 32 seconds ago Up 31 seconds 0.0.0.0:5601-&gt;5601/tcp docker-elk_kibana_1ecc046260f13 docker-elk_elasticsearch \"/bin/tini -- /usr/l…\" 33 seconds ago Up 32 seconds 0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp docker-elk_elasticsearch_14f85aff682ba spring-chatting-server_nginx \"/docker-entrypoint.…\" 54 minutes ago Up 54 minutes 0.0.0.0:8080-&gt;80/tcp nginx374824d2b950 spring-chatting-server_chatting-server-1 \"java -jar app.jar\" 54 minutes ago Up 54 minutes 0.0.0.0:8083-&gt;8083/tcp chatting-server-104b35b27012f spring-chatting-server_chatting-server-2 \"java -jar app.jar\" 54 minutes ago Up 54 minutes 0.0.0.0:8084-&gt;8084/tcp chatting-server-2ba305f53a20e confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 54 minutes ago Up 54 minutes 0.0.0.0:8099-&gt;8099/tcp, 9092/tcp kafka3204f557e1588 confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 54 minutes ago Up 54 minutes 0.0.0.0:8098-&gt;8098/tcp, 9092/tcp kafka2bcc4230e019a confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 54 minutes ago Up 54 minutes 0.0.0.0:8097-&gt;8097/tcp, 9092/tcp kafka11e4311a3184e spring-chatting-server_auth-server \"java -jar app.jar\" 54 minutes ago Up 54 minutes 0.0.0.0:8085-&gt;8085/tcp auth-server3ae7be01ea42 postgres:12-alpine \"docker-entrypoint.s…\" 54 minutes ago Up 54 minutes 5432/tcp, 0.0.0.0:5434-&gt;5434/tcp chatting-db-2a8aec1c46475 confluentinc/cp-zookeeper:7.2.1 \"/etc/confluent/dock…\" 54 minutes ago Up 54 minutes 2181/tcp, 2888/tcp, 3888/tcp zookeeper35c58ec8ea78 postgres:12-alpine \"docker-entrypoint.s…\" 54 minutes ago Up 54 minutes 5432/tcp, 0.0.0.0:5435-&gt;5435/tcp auth-dbb6a5f60c3a39 postgres:12-alpine \"docker-entrypoint.s…\" 54 minutes ago Up 54 minutes 5432/tcp, 0.0.0.0:5433-&gt;5433/tcp chatting-db-1Code Index User Authentication Server Chat Server Api gateway Kafka Elastic Search configuration LogStash configuration Kibana configuration모든 코드는 https://github.com/ghkdqhrbals/spring-chatting-server/tree/v2에 첨부하였다." }, { "title": "Elastic Search 심화(2)-ELK", "url": "/posts/elastic-search(3)/", "categories": "Elastic Search", "tags": "", "date": "2023-01-02 00:00:25 +0000", "snippet": "ELK stackELK 스택은 Elastic Search + Logstash + Kibana를 지칭하는 단어다. ES만 사용하는것이 아닌, Logstash(or Beats/Agent)로 원하는 데이터를 정제하여 ES에 삽입하고, Kibana를 통해 시각화시켜주는 통합기술스택이라고 보면 되겠다.아래는 김종민 님의 ES Webinar 중 하나에서 발췌한 그림이다.reference https://www.elastic.co/kr/virtual-events/optimizing-the-ingest-pipeline-for-elasticsearch그림을 보면 크게 1. 데이터를 수집/정제하는 Ingest Pipeline, 2. 데이터가 저장되는 ElasticSearch, 3. 데이터를 시각화하는 Kibana 로 나뉜다. 이제부터 각각 어떤식으로 사용할 수 있는지 확인해보자.1. Ingest PipelineIngest pipeline은 Beats나 Agent, Logstash를 통해 구축할 수 있으며 별개로 따로 추가해줄 수도 있다. 주로 Beats와 Agent는 가벼운 필터링을 위해 사용되며 Logstash는 더욱 다양한 정제를 위해 사용된다.아래는 직접 Ingest api를 통해 pipeline을 추가한 예시이다.1-1. Injest API를 통한 pipeline 구축PUT _ingest/pipeline/my-pipeline{ \"description\": \"My optional pipeline description\", # 여러가지 데이터 정제를 위한 선언 \"processors\": [ { # 데이터가 들어오면 my-long-field 필드에 10을 넣어 정제 \"set\": { \"description\": \"My optional processor description\", \"field\": \"my-long-field\", \"value\": 10 } }, { # 데이터가 들어오면 my-boolean-field 필드를 true로 변환 \"set\": { \"description\": \"Set 'my-boolean-field' to true\", \"field\": \"my-boolean-field\", \"value\": true } }, { # my-keyword-field 필드의 값을 소문자로 변환시킨다 \"lowercase\": { \"field\": \"my-keyword-field\" } } ]}만약 파이프라인을 엮어서 추가하고 싶을 때, 아래와같이 추가한 뒤 kibana에서 파이프라인 순서를 변경시켜줄 수 있다.PUT _ingest/pipeline/my-pipeline-id{ \"version\": 1, \"processors\": [ ... ]}만약 위처럼 api로 Ingest pipeline 추가하지 않고, Logstash를 사용한다고 해보자.1-2. Logstash 를 통한 Ingest pipeline 구축input { kafka { bootstrap_servers =&gt; \"kafka1:9092,kafka2:9092,kafka3:9092\" group_id =&gt; \"logstash\" topics =&gt; [\"log-user-chat\"] consumer_threads =&gt; 1 decorate_events =&gt; true }}filter { json { source =&gt; \"message\" }}output { elasticsearch{ hosts =&gt; \"es01:9200\" index =&gt; \"log-user-chat\" }}보다시피 Logstash는 더 다양한 기능이 존재한다. 1. kafka로부터 직접 메세지를 소비하고, 2. 필터링하며, 3. ES의 원하는 index에 저장할 수 있다. 위의 예시는 정말 간단한 예시이며, filter뿐만 아니라 다른 여러 설정들이 무궁무진하다. 문법이나 이런것들은 다음을 참조하자. https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html2. ElasticSearch이부분은 이전 포스팅을 참조하자.3. KibanaKibana는 시각화 툴 + ElasticSearch의 여러가지 설정들을 쉽게 해주는 기술이다. 필자는 이를 이용해서 아래와 같이 신규가입자 수 + 가입자 이름 분포도 등을 확인할 수 있게 시각화해보았다.자세한 설정방법은 chatting 프로젝트에서 설명하겠다." }, { "title": "Elastic Search 심화(1)", "url": "/posts/elastic-search(2)/", "categories": "Elastic Search", "tags": "", "date": "2023-01-01 00:00:25 +0000", "snippet": "INDEX Shard란? Node란? 주의사항1. Shard란?이전 포스팅에서 ES는 Index단위로 데이터를 저장한다고 하였다. 이 Index는 Shard 단위로 분산저장된다.위의 그림은 하나의 Index를 총 5개의 샤드로 분산저장한 그림이다.그리고 이런 샤드는 분산저장이라는 수식어에 걸맞게 여러 노드로 분산시킬 수 있다. 또한 복제데이터를 생성할 수도 있는데, 이 경우 원본데이터인 프라이머리 샤드와 복제본인 래플리카 샤드로 구분된다. 노드가 1개만 있는 경우 프라이머리 샤드만 존재하고 복제본은 생성되지 않습니다. Elasticsearch 는 아무리 작은 클러스터라도 데이터 가용성과 무결성을 위해 최소 3개의 노드로 구성 할 것을 권장하고 있습니다.위의 레퍼런스에서 말하듯, 만약 노드가 하나만 존재하는 경우에는 복제본이 필요없을 뿐더러 생성되지 않는다. 또한 같은 샤드와 복제본은 동일한 데이터를 담고 있으며 반드시 서로 다른 노드에 저장된다고 한다.위 그림은 Node-3이 다운되었을 때 Elastic Search가 대체하는 과정을 나타낸다. Node-3가 다운되면, 0번 replica 샤드와 4번 primary 샤드도 다운된다. 이 경우 0번 replica 샤드는 새로 다른노드에 저장되며, 4번 primary 샤드는 기존 replica 샤드가 승계받게 된다.이런식으로 multi-node ES 클러스터를 운용하게 되면, 가용성과 무결성에서 이점을 가져올 수 있다. 예시(primary 샤드 5, 복제본 1 인 books 인덱스 생성) curl -XPUT \"http://localhost:9200/books\" -H 'Content-Type: application/json' -d'{\"settings\": {\"number_of_shards\": 5,\"number_of_replicas\": 1}}' 이렇게 되면 총 {number_of_shards} * ( {number_of_replicas} + 1 ) 개의 샤드가 생성된다. 추후, replica를 없애는 것 또한 가능하다고 한다.2. Node란?노드는 Index의 샤드들을 저장하는 서버이다. 이러한 노드는 두 가지로 구분할 수 있다. Master Node : 인덱스의 메타 데이터, 샤드의 위치와 같은 클러스터 상태 정보를 관리하는 노드 Data Node : 실제로 색인된 데이터를 저장하고 있는 노드마스터 노드는 동일 클러스터 내 단 하나만 존재하여야 하며, 마스터 후보노드(master eligible node)들 사이에서 선출된다. 마스터의 역할과 데이터 노드의 역할은 중복이 가능하다. 만약 샤드의 개수가 많다면, 각각 역할을 분리시키는 것이 성능에 도움이 된다.3. 주의사항3-1. 짝수개의 마스터 후보노드의 개수를 설정했을 때 문제점 - Split Brain Problem마스터 후보노드는 몇개가 적당할까? 이걸 정할 때는 Split-brain 문제를 고려해서 개수를 설정해야한다. 아래는 ES에서 Split-brain 문제가 발생할 수 있는 예시이다. The Split-brain problem At any given time, there is only one master node in the cluster among all the master eligible nodes. Split-brain is a situation when you have more than one master in the cluster. Let’s take for example a cluster that has two master eligible nodes, M1 and M2, with the quorum of minimum_master_node set to one. The split-brain situation can occur in the cluster if both M1 and M2 are alive and the communication network between M1 and M2 is interrupted. When that occurs, both M1 and M2 consider themselves to be alone in the cluster and both elect themselves as the master. At this point, your cluster will have two master nodes and you have a split-brain situation. reference https://opster.com/guides/elasticsearch/best-practices/elasticsearch-split-brain/위의 예시에서는 2개(짝수개) 마스터 후보노드 M1와 M2를 설정했다. 그리고 M1이 마스터 노드로 선정되었다고 하자. 그런데 M1과 M2 사이 네트워크 연결이 잠깐 유실되었다. 이렇게 되면 M1은 뭐 그대로 자기가 마스터 노드역할을 수행할 것이다. 하지만 M2는 M1이 죽었다고 판단, 새로운 클러스터를 생성함과 동시에 자기또한 마스터의 자격을 가져버리게 된다. M1과 M2 둘다 마스터 노드가 되어버린 상황. 두 개의 노드가 동시에 데이터를 인덱싱하고 수정하기때문에, 데이터 정합성 문제가 발생하게 된다.그렇다면 어떻게 Split-brain문제를 피할 수 있을까? 아래의 두 가지를 설정하게 된다면 피할 수 있다. 홀수개의 마스터 후보노드 설정 클러스터 동작에 필요한 최소 마스터 후보노드 개수 설정(minimum_master_nodes)자. 원문의 문제를 위의 두가지 원칙을 지키면서 해결해보자.3-2. Solution먼저 3개(홀수개)의 마스터 후보노드 M1, M2, M3를 설정한다. 이후 최소 마스터 후보노드 개수(minimum_master_nodes)는 2개로 설정한다. minimum_master_nodes = ( master_eligible_node / 2 ) + 1 = ( 3 / 2 + 1 ) 이 minimum_master_nodes 설정은 6.x 이전버전에서 직접 작성해야한다. 7.0 이후버전부터는 자동으로 계산해주기때문에, 초기 마스터노드 후보군 cluster.initial_master_nodes: [ ] 값만 설정하면 된다.그러면 M2와 네트워크가 끊어졌을 때, M2는 자신을 새로운 클러스터의 새로운 마스터노드로 승급하려할것이다(또 다른 brain 발생). 하지만 M2가 속한 클러스터에서 마스터후보노드 개수는 1개이며, 이는 minimum_master_nodes보다 작기때문에 해당 클러스터는 동작하지 않도록 설정된다. 따라서 같은 클러스터 내 분열해도 하나의 마스터노드만 선출될 수 있게된다.종합하면, 1. 마스터 후보 노드 개수는 항상 홀수로 하고 가동을 위한 2. 최소 마스터 후보 노드 설정은 (전체 마스터 후보 노드)/2+1 로 설정해야 한다.Reference Elastic Search Index &amp; Shards Elastic Search Master &amp; Data Nodes 멀티노드 클러스터 Docker 설정방법" }, { "title": "Elastic Search의 개념 및 RDB와의 차이점", "url": "/posts/elastic-search/", "categories": "Elastic Search", "tags": "", "date": "2022-12-31 00:00:25 +0000", "snippet": "INDEX 개념 RDB와의 차이점 Type을 독립적으로 가지지 않은 이유 부모자식관계를 설정하는 두 가지 방법 RDB와의 차이점(추가) 문법(Appendix)1. 개념ES(ElasticSearch)는 Java 오픈소스 분산 검색 엔진이다. ES는 역색인을 지원하기에 기존 RDB가 지원하지 않는 비정형 데이터를 인덱싱 + 검색하는 것에 특화되어있다. 비정형 데이터 : Boolean같이 true/false로 정형화 된 데이터가 아닌 규칙이 없는 데이터. ex) 음성, 텍스트, 영상 역색인 : 키워드를 통해 데이터를 찾는 방식 2. RDB와의 차이점먼저 ES의 Notation을 알고 문법을 정리하고자 한다. 표기법은 다음과 같이 RDB와 비슷하게 매칭된다. 엘라스틱서치 관계형 데이터베이스 Index DB Type(completely removed after v8.0 ) Table Document Row Field Column Mapping Schema 하지만, ES는 RDB와 엄연히 다르다! 위의 개념은 이해를 돕기위해 가져온 것이지 그 개념이 일치하지 않는다. 특히 ES의 Type과 RDB의 Table은 전혀 다른 개념을 띄고있다. 여러 블로그를 찾아봤었는데, 이에 대해 대부분 명시하지 않더라. In RDB : 테이블이 서로 독립이며, 같은 Column명을 가진다고해도 서로 영향을 주지 않는다. In ElasticSearch : Type은 서로 독립이 아니다(Index는 독립적임). 같은 Index 내 + 다른 Type + 같은 Field명을 가진다면 동일한 Lucene Engine 필드로 처리되기에 서로 영향을 받는다. 예로 만약 내가 User(Type)에서 user_name(Field)을 삭제한다고 하였을 때, Account(Type)의 user_name(Field)도 같이 삭제된다는 것이다.따라서 ES의 Type은 RDB의 Table과 같지않다.보통 ES에서의 Type은 RDB에서의 PK/FK 즉, 부모자식관계를 만들때 사용되었었다. 아래의 예시를 보자curl -XGET \"my_index/question,answer/_search\" -H 'Content-Type:application/json' -d`{ \"query\": { \"match\": { \"qid\": \"100\" } }}`위의 예시는 my_index의 question과 answer type에서 문제번호 100번을 가져온다. question의 qid는 PK, answer의 qid는 FK처럼 사용됨으로써 부모자식관계처럼 만든 것이다. ES의 Type은 주로 이런식으로 사용하곤 했는데, 사실 RDB의 PK/FK 플로우를 완벽하게 따라갈 수가 없다. 만약 qid(Field)가 삭제될때는 답이없어지기 때문이다.보통 JPA에서 RDB를 다룰 때, PK가 삭제되면 FK가 자동으로 cascade되도록 설정할 수 있다. 또한 RDB에서 PK 칼럼을 삭제하는것이 불가능하다. 하지만, ES에서는 PK역할을 하는 필드가 삭제가능하며, 이 때 FK로 쓰던 answer의 해당 필드의 document들은 그냥 평생 남아있게 되어버린다. 결론은, RDB의 플로우를 따라가기 위해 ES는 Type을 만들었지만, 사실상 따라갈 수 없다.이러한 이유로 ES는 Type을 삭제하기로 결심한다!그럼 애초에 ES가 Type을 독립시키도록 만들면 더 좋았지 않을까요? 라는 질문이 떠오를것이다. 이에 대한 답을 레퍼런스에서 해준다. On top of that, storing different entities that have few or no fields in common in the same index leads to sparse data and interferes with Lucene’s ability to compress documents efficiently. For these reasons, we have decided to remove the concept of mapping types from Elasticsearch. reference Why are Mapping Types being removed?3. Type을 독립적으로 가지도록 설정하지 않은 이유만약 동일한 인덱스 내부에서 Type별 독립 Field를 가지게 된다면, 비슷한 기능을 하는 Field가 분리되며 데이터가 분산되고 도큐먼트를 효율적으로 압축하는 Lucene의 기능을 방해할것이라고 한다.4. 그럼 어떻게 ES에서 부모자식관계를 설정해야할까? 두가지 선택지 존재 Field가 독립구분되는 Index을 RDB의 테이블처럼 생각하고 설정하는 방법 Type을 그대로 필드에 직접 customType으로 설정하는 방법. 4-1. Field가 독립구분되는 Index을 RDB의 테이블처럼 생각하고 설정하는 방법(물론 이 방법으로도 RDB의 플로우를 완벽히 따라갈 순 없다. 부모의 PK field를 바로 삭제할 수 있기 떄문이다.) 기존 Type으로 부모관계를 설정할 때curl -XGET \"my_index/question,answer/_search\" -H 'Content-Type:application/json' -d... 변경된 부모관계 설정curl -XGET \"question,answer/_search\" -H 'Content-Type:application/json' -d...4-2. Type을 그대로 필드에 직접 추가하는 방법(customType) 기존 Type으로 부모관계를 설정할 때PUT my_index{ \"mappings\": { \"question\": { \"properties\": { \"qid\": { \"type\": \"keyword\" }, \"문제제목\": { \"type\": \"text\" }, \"문제내용\": { \"type\": \"text\" } } }, \"answer\": { \"properties\": { \"qid\": { \"type\": \"keyword\" }, \"문제정답\": { \"type\": \"text\" } } } }}PUT my_index/question/100{ \"qid\": \"100\", \"문제제목\": \"깜짝퀴즈를 맞춰보세요\", \"문제내용\": \"1+1는?\"}PUT my_index/answer/100{ \"qid\": \"100\", \"문제정답\": \"귀요미\"}GET my_index/question,answer/_search{ \"query\": { \"match\": { \"qid\": \"100\" } }} 변경된 부모관계 설정PUT my_index{ \"mappings\": { \"_doc\": { \"properties\": { \"qid\": { \"type\": \"keyword\" }, # customType으로 직접 Type 설정 \"customType\": { \"type\": \"keyword\" }, \"문제제목\": { \"type\": \"text\" }, \"문제내용\": { \"type\": \"text\" }, \"문제정답\": { \"type\": \"text\" } } } }}PUT my_index/_doc/question-100{ \"customType\": \"question\", \"qid\": \"100\", \"문제제목\": \"깜짝퀴즈를 맞춰보세요\", \"문제내용\": \"1+1는?\"}PUT my_index/_doc/answer-100{ \"customType\": \"answer\", \"qid\": \"100\", \"문제정답\": \"귀요미\"}GET my_index/_search{ \"query\": { \"bool\": { \"must\": { \"match\": { \"qid\": \"100\" } }, \"filter\": { \"match\": { \"type\": \"answer question\" } } } }}5. 추가적인 RDB와의 차이점 RestApi를 통한 쿼리 RDB는 SQL문을 날려 테이블에 삽입하였다면, ES는 RestAPI를 통해 CRUD operation이 가능하다. curl -XPUT \"http://localhost:9200/my_index/_doc/1\" -H 'Content-Type:application/json' -d' {\"message\":\"안녕하세요 HB\"}' 높은 비정형 데이터 색인 효율성 예로 우리가 HB라는 단어를 포함하는 row를 뽑고싶다라고 했을 때, RDB는 모든 row를 다 뽑아서 특정단어포함여부를 따로 전부 확인해야한다. 즉, 긴 텍스트(비정형 데이터)를 색인하고 검색하는것이 힘들다. 하지만, ES는 다음과 같이 비정형 데이터를 검색할 수 있다. curl -XGET \"http://localhost:9200/my_index/_search\"{ \"query\":{ \"match\": { \"message\": \"HB\" } }} 트랜잭션과 롤백 기능이 없다 데이터의 업데이트를 제공하지 않는다 삭제 후 삽입 문법URI은 기본적으로 {ES-ip}:{ES-port} / {INDEX} / {method=[_doc, _search, ...]} / {id} 를 따라간다.이외에 상세한 문법은 자세히 설명된 자료가 있어 간단하게 첨부한다.https://github.com/kimjmin/elastic-demo/blob/master/demos/get-started/elasticsearch-7x.md" }, { "title": "실시간 채팅방 구현(7) - (수정된 아키텍처)", "url": "/posts/chatting(7)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-30 00:00:25 +0000", "snippet": "진행상황Architecture현재 구현된 서비스를 docker compose를 통해 실행시키면 다음과 같이 컨테이너들이 생긴다. Kafka와 Chat Server, Auth Server, DB들 모두 외부접속 가능하도록 노출 port를 도커 내부 포트랑 같게 설정했다. gyuminhwangbo@Gyuminui-MacBookPro spring-chatting-server % docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd54365955efa spring-chatting-server_nginx \"/docker-entrypoint.…\" 51 seconds ago Up 47 seconds 0.0.0.0:8080-&gt;80/tcp nginxaf5c00d411db spring-chatting-server_chatting-server-2 \"java -jar app.jar\" 51 seconds ago Up 48 seconds 0.0.0.0:8084-&gt;8084/tcp chatting-server-2becb35e0bc1f spring-chatting-server_chatting-server-1 \"java -jar app.jar\" 51 seconds ago Up 48 seconds 0.0.0.0:8083-&gt;8083/tcp chatting-server-19bfb2b8d53ca confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 51 seconds ago Up 49 seconds 0.0.0.0:8099-&gt;8099/tcp, 9092/tcp kafka3cc8d6c5944b3 confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 51 seconds ago Up 49 seconds 0.0.0.0:8097-&gt;8097/tcp, 9092/tcp kafka1995e41051907 confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 51 seconds ago Up 49 seconds 0.0.0.0:8098-&gt;8098/tcp, 9092/tcp kafka27d355211b1ce spring-chatting-server_auth-server \"java -jar app.jar\" 51 seconds ago Up 49 seconds 0.0.0.0:8085-&gt;8085/tcp auth-server29ab743b9847 confluentinc/cp-zookeeper:7.2.1 \"/etc/confluent/dock…\" 52 seconds ago Up 50 seconds 2181/tcp, 2888/tcp, 3888/tcp zookeeper722bf2b27df5 postgres:12-alpine \"docker-entrypoint.s…\" 52 seconds ago Up 50 seconds 5432/tcp, 0.0.0.0:5434-&gt;5434/tcp chatting-db-2ccc3e138004a postgres:12-alpine \"docker-entrypoint.s…\" 52 seconds ago Up 50 seconds 5432/tcp, 0.0.0.0:5433-&gt;5433/tcp chatting-db-13a52283f6f52 postgres:12-alpine \"docker-entrypoint.s…\" 52 seconds ago Up 50 seconds 5432/tcp, 0.0.0.0:5435-&gt;5435/tcp auth-db 구현 완료 API GATEWAY 채팅서버(/chat)에 접속 시, ip_hash로 특정 서버로만 들어가도록 구현 인증서버(/user)에 접속할 때 uri 맨 뒤 /가 붙어야만 접속되었었다. 그래서 rewrite {regex}로 맨뒤에 자동으로 /를 붙여주도록 설정 Auth Server 인증서버를 따로 구현함으로써 다른 곳에서도 편하게 이용할 수 있도록 구현 Chat Server 채팅서버를 수동으로 2대 구축 추후 K8S의 auto-scaling으로 구현할 예정 Kafka Broker 토픽은 두 개로 올렸으며, 각각 broker 3대 및 2개의 파티션에 걸쳐 백업되도록 운용 파티션은 200개가 적당하다고 한다. 지금 당장은 2개의 파티션으로 운용. 구현 미완료 Front Server API GATEWAY에 비동기+논 블로킹 webClient로 요청 전송 예정 Log Server DB선정에 있어 고민이 있다. 여러가지 비교해봤을 때, 1차적으로 Elastic Search을 선택하였다. 시계열 데이터에 맞는 DB는? Elastic Search = RESTful api기반으로 여러 곳에서 활용 가능하며, 역색인을 지원하므로 비정형 doc 색인/검색에 탁월하다. 필자는 채팅을 관찰하며 특정 단어의 빈도수를 모니터링하려 하는데 이때 쓰기 딱 좋을듯. 또한 데이터 저장 시점에 해당 데이터를 색인하기에 실시간이 아닌점이 마음에 든다. Kafka를 쓰기때문에 실시간이 아닐 뿐더러, Logging에 실시간성이 필요없기때문에 알맞은 엔진이라고 생각된다. HDFS = 분산저장 아직 하지 않을것임 File System = Transaction관리가 어려움 Timescale = 기존 Postgres를 시계열로 확장시킨 DB. 그래서 익숙한 SQL문을 그대로 사용할 수 있다는 장점이 있다. 하지만 단점은 직접 모든 SQL문을 작성해야한다는 점이다(이유는 JPA에서 Timescale을 지원하지 않기 때문에 모든 Dialect를 작성해야함) InfluxDB = 가장 많이 사용하는 시계열 DB이자, JPA가 지원하는 시계열 DB. 참고자료. 필자가 원하는 채팅개수 및 신규가입자 수라는 시계열 데이터들을 대시보드로 별도로 쉽게 확인할 수 있다. InfluxDB는 real-time querying에 좋다고 하는것 같은데, 우리는 카프카를 사용하기때문에 해당 장점을 활용하지 않는다. 만약 InfluxDB를 선택한다면, 시계열데이터는 다음과 같이 두 가지로 각각의 하이퍼테이블을 구성할 예정 symbol = chatAdd, data = 채팅 입력 수 symbol = userAdd, data = 신규가입자 수 Orchestration saga 필자는 Kafka를 MSA의 백본망으로 사용한다. 따라서 saga의 패턴 중 중앙집권형인 Orchestration 패턴으로 모니터링을 중앙에서 할 것이다. 사용할 모니터링 툴 리스트 메세지 모니터링 UI for Kafka 카프카 서버 연결 및 Consumer Group 확인/ 토픽 확인이 용이하다. 또한 실제 메세지 내용도 확인할 수 있어서 편리하다. Consumer 트래픽 모니터링 https://s262701-id.tistory.com/126 Linkedin의 burrow lag정보 Elasticsearch 수집 데이터파이프라인 ElasticSearch에 기록된 lag 정보 기반 Grafana lag 모니터링 대시보드각각의 Kafka 모니터링 툴 비교 완료된 설정상세 코드는 https://github.com/ghkdqhrbals/spring-chatting-server에서 확인하능하다. main branch에 존재완료된 API GATEWAY(nginx.config)user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events { worker_connections 1024;}http { include /etc/nginx/mime.types; default_type application/octet-stream; upstream chat-server { # IP 해쉬화 하여 특정 서버에 들어가도록 설정 # https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash ip_hash; server chatting-server-1:8083; server chatting-server-2:8084; } upstream auth-server { server auth-server:8085; } server { listen 80; # 추후 server_name 변경예정 server_name localhost; location / { # 채팅서버 backend location /chat { proxy_pass http://chat-server; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; # 클라이언트 요청 ip전송 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 클라이언트 요청 ip전송 } # 인증서버 backend location /user { rewrite ^([^.]*[^/])$ $1/ permanent; # tailing slash with every url proxy_pass http://auth-server; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; keepalive_timeout 65; include /etc/nginx/conf.d/*.conf;}완료된 Docker-compose.ymlKafka 설정에 있어 https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/ 문서가 많은 도움이 되었다.version : '2'services: # -------- API GATEWAY -------- nginx: restart: always container_name: nginx depends_on: - chatting-server-1 - chatting-server-2 build: context: ./nginx dockerfile: Dockerfile ports: - \"8080:80\" # -------- KAFKA -------- zookeeper: image: confluentinc/cp-zookeeper:7.2.1 container_name: zookeeper environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka1: image: confluentinc/cp-kafka:7.2.1 container_name: kafka1 ports: - \"8097:8097\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8097,INTERNAL://kafka1:9092 KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL kafka2: image: confluentinc/cp-kafka:7.2.1 container_name: kafka2 ports: - \"8098:8098\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8098,INTERNAL://kafka2:9092 KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL kafka3: image: confluentinc/cp-kafka:7.2.1 container_name: kafka3 ports: - \"8099:8099\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8099,INTERNAL://kafka3:9092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL # -------- Chatting Server -------- chatting-db-1: container_name: chatting-db-1 image: postgres:12-alpine environment: - POSTGRES_PASSWORD=password - POSTGRES_USER=postgres - POSTGRES_DB=chat expose: - \"5433\" # Publishes 5433 to other containers but NOT to host machine ports: - \"5433:5433\" volumes: - ./backups:/home/backups command: -p 5433 chatting-server-1: container_name: chatting-server-1 build: ./spring-chatting-backend-server ports: - \"8083:8083\" environment: - SPRING_DATASOURCE_URL=jdbc:postgresql://chatting-db-1:5433/chat - SPRING_DATASOURCE_USERNAME=postgres - SPRING_DATASOURCE_PASSWORD=password - SPRING_JPA_HIBERNATE_DDL_AUTO=update - SERVER_PORT=8083 - KAFKA_BOOTSTRAP=kafka1:9092,kafka2:9092,kafka3:9092 # 내부포트 depends_on: - kafka1 - kafka2 - kafka3 - chatting-db-1 restart: always chatting-db-2: container_name: chatting-db-2 image: postgres:12-alpine environment: - POSTGRES_PASSWORD=password - POSTGRES_USER=postgres - POSTGRES_DB=chat expose: - \"5434\" # Publishes 5433 to other containers but NOT to host machine ports: - \"5434:5434\" volumes: - ./backups:/home/backups command: -p 5434 chatting-server-2: container_name: chatting-server-2 build: ./spring-chatting-backend-server ports: - \"8084:8084\" environment: - SPRING_DATASOURCE_URL=jdbc:postgresql://chatting-db-2:5434/chat - SPRING_DATASOURCE_USERNAME=postgres - SPRING_DATASOURCE_PASSWORD=password - SPRING_JPA_HIBERNATE_DDL_AUTO=update - SERVER_PORT=8084 - KAFKA_BOOTSTRAP=kafka1:9092,kafka2:9092,kafka3:9092 # 내부포트 depends_on: - kafka1 - kafka2 - kafka3 - chatting-db-2 restart: always # -------- Authentication Server -------- auth-db: container_name: auth-db image: postgres:12-alpine environment: - POSTGRES_PASSWORD=password - POSTGRES_USER=postgres - POSTGRES_DB=auth expose: - \"5435\" # Publishes 5433 to other containers but NOT to host machine ports: - \"5435:5435\" volumes: - ./backups:/home/backups command: -p 5435 auth-server: container_name: auth-server build: ./spring-auth-backend-server ports: - \"8085:8085\" environment: - SERVER_PORT=8085 - SPRING_DATASOURCE_URL=jdbc:postgresql://auth-db:5435/auth - SPRING_DATASOURCE_USERNAME=postgres - SPRING_DATASOURCE_PASSWORD=password - SPRING_JPA_HIBERNATE_DDL_AUTO=update depends_on: - auth-db restart: always" }, { "title": "실시간 채팅방 구현(6) - (전체 아키텍처 수정)", "url": "/posts/chatting(6)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-24 00:00:25 +0000", "snippet": "전체 스키마 수정앞서 본 프로젝트에서 backend와 front가 kafka Request/Response 아키텍쳐로 통신하도록 설정하였다. 이 때, 발생하는 귀찮은 부분이 상당히 많았다.특히 로그인 부분에서 다시 생각해보게 되었다기존 구현아키텍처 (아직 로깅서버는 미구현) 통신 흐름 Client – POST https://{frontend}/login –&gt; Frontend Frontend Producer – { topic=login-request, data=ReqLoginDto, key=userId } –&gt; Kafka Backend Consumer – 로직처리 – Kafka { topic=login-response, data=RespLoginDto, key=userId } –&gt; Kafka Broker Frontend Consumer – STOMP { topic:/sub/userLogin/{userId}, data:”Accepted” } –&gt; Client 발생한 문제점고려할 점이 상당히 많다. 단순히 backend를 RESTAPI로 구현하면 webClient를 통해 발신/수신하면 된다. 이는 요청에 대한 반환의 시점이 명시되어야 하는 로그인서비스의 기능과 부합한다. 하지만 kafka로 REQ/RESP하면 메세지를 consume하는 시점자체가 명확하지 않다보니 언제 로그인 서비스가 해당 유저에게 제공될 지 모르는 문제점이 발생한다.수정된 구현아키텍처 Kafka를 여러 부가기능서버와 데이터를 주고받는 백본으로 활용한다(실시간 처리가 필요없는 부가기능들!). API Gateway로 같은 기능의 서버를 묶고 restapi형태로 제공하여 쉽게 사용할 수 있도록 한다. 통신 흐름 Client – POST https://{frontend}/login –&gt; Frontend Frontend –redirect to API Gateway–&gt; Api Gateway(“/chat”) API Gateway –redirect to Backend(Load Balancing) Backend –results–&gt; API GateWay –&gt; Frontend –STOMP { topic:/sub/userLogin/{userId}, data:”Accepted” }–&gt; Client Backend(ChatServer)에서 주기적으로 Logging토픽에 메세지 전송 LoggingServer에서 메세지 소비 곧 크리스마스인데 다들 따뜻하게 입으시고 즐거운 크리스마스 보내세요 :D 혹시라도 컨셉이 잘못되었거나 이상한점이 있다면 언제든! ghkdqhrbals@gmail.com 로 메일 보내주시면 감사하겠습니다." }, { "title": "실시간 채팅방 구현(5) - (Kafka 연동완료)", "url": "/posts/chatting(5)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-21 00:00:25 +0000", "snippet": "진행상황Backend Server 완료아래는 Kafka Backend Server 세팅 클래스 및 기능들이다. MessageListener : 실제 메세지 내부 로직 수행 KafkaTopicConst : 토픽을 application-properties에서 로드 KafkaTopicConfig : 토픽 등록 KafkaConsumerConfig : 메세지 수신 그룹 및 타입 설정 KafkaProducerConfig : 메세지 발신 설정 KafkaAdminConfig : 카프카 연결테스트(Login) 메세지 전송 메세지 수신 # 직접 메세지 소비 [appuser@1ec915a5c4a5 ~]$ kafka-console-consumer --bootstrap-server localhost:9092 --topic login-response {\"userId\":\"a\",\"isSuccess\":true,\"errorMessage\":\"\",\"user\":{\"userId\":\"a\",\"userPw\":\"1234\",\"email\":\"a@naver.com\",\"userName\":\"user_A\",\"userStatus\":\"바뀐 status message\",\"joinDate\":[2022,12,17],\"loginDate\":[2022,12,22],\"logoutDate\":[2022,12,17]}} Kafka 코드@Slf4j@Componentpublic class MessageListener extends KafkaTopicConst{ private final KafkaTemplate&lt;String, Object&gt; kafkaProducerTemplate; private final UserService userService; private final FriendService friendService; private final RoomService roomService; private final ChatService chatService; public MessageListener(KafkaTemplate&lt;String, Object&gt; kafkaProducerTemplate, UserService userService, FriendService friendService, RoomService roomService, ChatService chatService) { this.kafkaProducerTemplate = kafkaProducerTemplate; this.userService = userService; this.friendService = friendService; this.roomService = roomService; this.chatService = chatService; } // 로그인 요청 @KafkaListener(topics = \"${kafka.topic-login-request}\", containerFactory = \"loginKafkaListenerContainerFactory\") public void listenLogin(RequestLoginDTO req) { log.info(\"Receive [RequestLoginDTO] Message with userId={},userPw={}\",req.getUserId(),req.getUserPw()); ResponseLoginDTO resp = userService.login(req.getUserId(), req.getUserPw()); // Kafka 메세지 전송/비동기 처리를 위한 ListenableFuture 사용 ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_LOGIN_RESPONSE, req.getUserId(),resp); listenFuture(future); // 메시지 비동기 callback 처리 } // 로그아웃 요청 @KafkaListener(topics = \"${kafka.topic-logout-request}\", containerFactory = \"logoutKafkaListenerContainerFactory\") public void listenLogin(@Payload String userId) { ResponseLogoutDTO resp = userService.logout(userId); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_LOGOUT_RESPONSE, userId, resp); listenFuture(future); } // 유저 참여 채팅방 목록 조회 @KafkaListener(topics = \"${kafka.topic-user-search-room-request}\", containerFactory = \"userRoomKafkaListenerContainerFactory\") public void rooms(RequestUserRoomDTO req){ ResponseUserRoomDTO resp = userService.findAllMyRooms(req.getUserId()); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_SEARCH_ROOM_RESPONSE, req.getUserId(),resp); listenFuture(future); } // 유저의 친구목록 조회 @KafkaListener(topics = \"${kafka.topic-user-search-friend-request}\", containerFactory = \"userSearchFriendKafkaListenerContainerFactory\") public void findFriend(RequestUserFriendDTO req){ String userId = req.getUserId(); ResponseUserFriendDTO resp = new ResponseUserFriendDTO(userId); Optional&lt;User&gt; findUser = userService.findById(userId); if (!findUser.isPresent()){ resp.setStat(userId + \"유저가 존재하지 않습니다\"); }else{ List&lt;Friend&gt; friends = friendService.findAllByUserId(userId); resp.setIsSuccess(true); resp.setFriend(friends); } ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_SEARCH_FRIEND_RESPONSE, req.getUserId(),resp); listenFuture(future); } // 유저 저장 @KafkaListener(topics = \"${kafka.topic-user-add-request}\", containerFactory = \"userAddKafkaListenerContainerFactory\") public void addUser(RequestAddUserDTO req){ User user = new User( req.getUserId(), req.getUserPw(), req.getEmail(), req.getUserName(), \"\", LocalDate.now(), LocalDate.now(), LocalDate.now() ); ResponseAddUserDTO resp = userService.save(user); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_ADD_RESPONSE, req.getUserId(),resp); listenFuture(future); } // 유저 상태메세지 변경 @KafkaListener(topics = \"${kafka.topic-user-status-change-request}\", containerFactory = \"userChangeStatusKafkaListenerContainerFactory\") public void changeUserStatus(RequestChangeUserStatusDTO req){ ResponseChangeUserStatusDTO resp = userService.updateUserStatus(req); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_STATUS_CHANGE_RESPONSE, req.getUserId(),resp); listenFuture(future); } //채팅방 개설 @KafkaListener(topics = \"${kafka.topic-user-add-room-request}\", containerFactory = \"userAddRoomKafkaListenerContainerFactory\") public void createRoomForm(RequestAddUserRoomDTO req){ ResponseAddUserRoomDTO resp = userService.makeRoomWithFriends(req); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_ADD_ROOM_RESPONSE, req.getUserId(),resp); listenFuture(future); } // 채팅 저장 @KafkaListener(topics = \"${kafka.topic-user-add-chat-request}\", containerFactory = \"userAddChatKafkaListenerContainerFactory\") public void chattingRoom(RequestAddChatMessageDTO req) { Optional&lt;Room&gt; room = roomService.findByRoomId(req.getRoomId()); Optional&lt;User&gt; user = userService.findById(req.getWriterId()); ResponseAddChatMessageDTO resp = new ResponseAddChatMessageDTO(req.getRoomId(),req.getWriterId()); if (!room.isPresent()){ resp.setErrorMessage(\"채팅방이 존재하지 않습니다\"); } if(!user.isPresent()){ resp.setErrorMessage(\"유저가 존재하지 않습니다\"); } Chatting chat = createChatting(user.get(),room.get(),req.getMessage()); resp = chatService.save(chat); ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_ADD_CHAT_RESPONSE, req.getWriterId(),resp); listenFuture(future); } // 친구 저장 @KafkaListener(topics = \"${kafka.topic-user-add-friend-request}\", containerFactory = \"userAddFriendKafkaListenerContainerFactory\") public void addFriend(RequestAddFriendDTO req){ List&lt;String&gt; friendIds = req.getFriendId(); ResponseAddFriendDTO resp = new ResponseAddFriendDTO(req.getUserId()); for(String friendId : friendIds){ Optional&lt;User&gt; findId = userService.findById(friendId); if (!findId.isPresent()){ resp.setErrorMessage(\"유저가 존재하지 않습니다\"); break; } friendService.save(req.getUserId(), friendId); } ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_USER_ADD_FRIEND_RESPONSE, req.getUserId(),resp); listenFuture(future); } // utils private Chatting createChatting(User user,Room room, String message){ Chatting chat = new Chatting(); chat.setRoom(room); chat.setSendUser(user); chat.setCreatedDate(ZonedDateTime.now().toLocalDate()); chat.setCreatedTime(ZonedDateTime.now().toLocalTime()); chat.setMessage(message); return chat; } private void listenFuture(ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future) { future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() { @Override public void onFailure(Throwable ex) { log.error(\"메세지 전송 실패 : {}\", ex.getMessage()); } @Override public void onSuccess(SendResult&lt;String, Object&gt; result) { log.info(\"메세지 전송 성공 topic: {}, offset: {}, partition: {}\",result.getRecordMetadata().topic() ,result.getRecordMetadata().offset(), result.getRecordMetadata().partition()); } }); }}@Componentpublic class KafkaTopicConst { @Value(\"${kafka.topic-login-request}\") public String TOPIC_LOGIN_REQUEST; @Value(\"${kafka.topic-login-response}\") public String TOPIC_LOGIN_RESPONSE; @Value(\"${kafka.topic-logout-request}\") public String TOPIC_LOGOUT_REQUEST; @Value(\"${kafka.topic-logout-response}\") public String TOPIC_LOGOUT_RESPONSE; @Value(\"${kafka.topic-user-search-room-request}\") public String TOPIC_USER_ROOM_REQUEST; @Value(\"${kafka.topic-user-search-room-response}\") public String TOPIC_USER_ROOM_RESPONSE; @Value(\"${kafka.topic-user-search-room-request}\") public String TOPIC_USER_SEARCH_ROOM_REQUEST; @Value(\"${kafka.topic-user-search-room-response}\") public String TOPIC_USER_SEARCH_ROOM_RESPONSE; @Value(\"${kafka.topic-user-search-friend-request}\") public String TOPIC_USER_SEARCH_FRIEND_REQUEST; @Value(\"${kafka.topic-user-search-friend-response}\") public String TOPIC_USER_SEARCH_FRIEND_RESPONSE; @Value(\"${kafka.topic-user-add-request}\") public String TOPIC_USER_ADD_REQUEST; @Value(\"${kafka.topic-user-add-response}\") public String TOPIC_USER_ADD_RESPONSE; @Value(\"${kafka.topic-user-status-change-request}\") public String TOPIC_USER_STATUS_CHANGE_REQUEST; @Value(\"${kafka.topic-user-status-change-response}\") public String TOPIC_USER_STATUS_CHANGE_RESPONSE; @Value(\"${kafka.topic-user-add-room-request}\") public String TOPIC_USER_ADD_ROOM_REQUEST; @Value(\"${kafka.topic-user-add-room-response}\") public String TOPIC_USER_ADD_ROOM_RESPONSE; @Value(\"${kafka.topic-user-add-chat-request}\") public String TOPIC_USER_ADD_CHAT_REQUEST; @Value(\"${kafka.topic-user-add-chat-response}\") public String TOPIC_USER_ADD_CHAT_RESPONSE; @Value(\"${kafka.topic-user-add-friend-request}\") public String TOPIC_USER_ADD_FRIEND_REQUEST; @Value(\"${kafka.topic-user-add-friend-response}\") public String TOPIC_USER_ADD_FRIEND_RESPONSE;}@Configurationpublic class KafkaTopicConfig extends KafkaTopicConst { @Autowired private KafkaAdmin kafkaAdmin; // 계산 잘 해야한다. Partition 개수 &gt;= Group내 Conusmer 개수 // 생성하고자 하는 Conumser=2 Partition은 2이기에, 각각 conusmer에게 leader-partition 매칭가능 private NewTopic generateTopic(String topicName,int partitionNum, int brokerNum) { return TopicBuilder.name(topicName) .partitions(partitionNum) // 할당하고자 하는 파티션 개수 .replicas(brokerNum) // replica sync를 위한 broker 개수 .build(); // 토픽은 총 2개의 leader-partition, 4개의 follow-partition 로 설정할 것임 } @PostConstruct public void init() { kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGIN_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGIN_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGOUT_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGOUT_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ROOM_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ROOM_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_SEARCH_ROOM_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_SEARCH_ROOM_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_SEARCH_FRIEND_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_SEARCH_FRIEND_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_REQUEST,5,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_RESPONSE,5,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_STATUS_CHANGE_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_STATUS_CHANGE_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_ROOM_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_ROOM_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_CHAT_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_CHAT_RESPONSE,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_FRIEND_REQUEST,2,3)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_USER_ADD_FRIEND_RESPONSE,2,3)); }}@Configuration@EnableKafkapublic class KafkaProducerConfig { @Value(\"${kafka.bootstrap-servers}\") private String bootstrapServer; private ProducerFactory&lt;String, Object&gt; producerFactory() { Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;(); configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class); return new DefaultKafkaProducerFactory&lt;&gt;(configProps); } @Bean public KafkaTemplate&lt;String, Object&gt; kafkaProducerTemplate() { return new KafkaTemplate&lt;&gt;(producerFactory()); }}@Slf4j@EnableKafka@Configurationpublic class KafkaConsumerConfig { @Value(\"${kafka.bootstrap-servers}\") private String bootstrapServer; // 로그인 요청 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestLoginDTO&gt; loginKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestLoginDTO.class); } // 로그아웃 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; logoutKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",String.class); } // 유저 참여 채팅방 목록 조회 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestUserRoomDTO&gt; userRoomKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestUserRoomDTO.class); } // 유저 친구목록 조회 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestUserFriendDTO&gt; userSearchFriendKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestUserFriendDTO.class); } // 유저 저장 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestAddUserDTO&gt; userAddKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestAddUserDTO.class); } // 유저 상태메세지 변경 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestChangeUserStatusDTO&gt; userChangeStatusKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestChangeUserStatusDTO.class); } // 채팅방 개설 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestAddUserRoomDTO&gt; userAddRoomKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestAddUserRoomDTO.class); } // 채팅 저장 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestAddChatMessageDTO&gt; userAddChatKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestAddChatMessageDTO.class); } // 유저 친구 저장 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestAddFriendDTO&gt; userAddFriendKafkaListenerContainerFactory() { return getContainerFactory(\"defaultGroup\",RequestAddFriendDTO.class); } /** * 유틸 목록 - 제네릭 클래스로 여러메소드에서 중복사용가능하도록 유틸화하였다. * --Methods-- * getContainerFactory() * getKafkaConsumerFactory() * setConfig() * setDeserializer() */ private &lt;T&gt; ConcurrentKafkaListenerContainerFactory&lt;String, T&gt; getContainerFactory(String groupId, Class&lt;T&gt; classType) { ConcurrentKafkaListenerContainerFactory&lt;String, T&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(getKafkaConsumerFactory(groupId, classType)); return factory; } private &lt;T&gt; DefaultKafkaConsumerFactory&lt;String, T&gt; getKafkaConsumerFactory(String groupId,Class&lt;T&gt; classType) { JsonDeserializer&lt;T&gt; deserializer = setDeserializer(classType); return new DefaultKafkaConsumerFactory&lt;&gt;(setConfig(groupId, deserializer), new StringDeserializer(), deserializer); } private &lt;T&gt; ImmutableMap&lt;String, Object&gt; setConfig(String groupId, JsonDeserializer&lt;T&gt; deserializer) { ImmutableMap&lt;String, Object&gt; config = ImmutableMap.&lt;String, Object&gt;builder() .put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer) .put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class) .put(ConsumerConfig.GROUP_ID_CONFIG, groupId) .put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deserializer) .build(); return config; } private &lt;T&gt; JsonDeserializer&lt;T&gt; setDeserializer(Class&lt;T&gt; classType) { JsonDeserializer&lt;T&gt; deserializer = new JsonDeserializer&lt;&gt;(classType); deserializer.setRemoveTypeHeaders(false); deserializer.addTrustedPackages(\"*\"); deserializer.setUseTypeMapperForKey(true); return deserializer; }}@Configurationpublic class KafkaAdminConfig { @Value(\"${kafka.bootstrap-servers}\") private String bootstrapServer; @Bean public KafkaAdmin kafkaAdmin() { Map&lt;String, Object&gt; configs = new HashMap&lt;&gt;(); configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); return new KafkaAdmin(configs); }}이 밖에 많은수의 데이터 발신/수신 포멧들과 서비스들이 연계되어 사용된다.Frontend Server 진행중Backend 개발 이후 세션과 페이지를 관리하는 Frontend Server을 개발중에 있다. 이 과정에서 고려할 몇가지를 추가하려 한다. Server-Sent Event로 진행이전 계획은 Frontend Server에서 webClient로 HTTP 비동기-nonBlocking 수신하려하였지만, backend에서 HTTP를 통하지 않고 직접 토픽 내 메세지 수신으로 바꾸었기에 수정이 필요하다.수정 이후 예상되는 전체 flow는 다음과 같다. 수정된 Flow(로그인 예시) 클라이언트 브라우저에서 GET /login FrontServer에 전송. Front Server에서 SSE 모델 전송 및 유저인증 메시지 Kafka에 login-request 토픽으로 전달 클라이언트 브라우저에서 SSE 이벤트 수신 이전 로그인 로딩창 확인 Backend Server에서 Kafka메세지 수신 및 인증로직 수행 이후 login-response 토픽에 전달 Front Server는 이를 수신 및 SSE 이벤트 클라이언트 브라우저에 전달 클라이언트는 비로소 로그인 성공화면 redirect " }, { "title": "실시간 채팅방 구현(4) - (프로젝트 수행시 고려점3)", "url": "/posts/chatting(4)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-19 04:00:25 +0000", "snippet": "추가내용문제점2. 메모리상에서 유저데이터의 복제 해결 방법이전 포스팅[실시간 채팅방 구현(2)]에서 문제점2인 메모리상 유저데이터 복제가 일어난다고 기술했다. 이를 해결하기 위해 sticky session과 같이 kafka 파티셔닝을 진행한다고 하였다. 상세내용은 아래의 코드를 통해 보이겠다.KafkaProducerController@Slf4j@RestController@RequestMapping(\"/api\")public class KafkaProducerController { // kafka producer를 위한 KafkaTemplate를 지정한다. private final KafkaTemplate&lt;String, Object&gt; kafkaProducerTemplate; @Value(\"${kafka.topic-login-request}\") public String TOPIC_LOGIN_REQUEST; @Value(\"${kafka.topic-login-response}\") public String TOPIC_LOGIN_RESPONSE; ... // 임시로 PostMapping하였다. // 역할1. userId와 userPw를 담은 RequestLoginDTO가 도착하면 이를 kafka의 특정 파티션으로 전송 @PostMapping(\"login\") public ResponseEntity&lt;?&gt; produceMessageWithRequestLoginDTO(@RequestBody RequestLoginDTO requestLoginDTO) { // 이부분이 중요하다!!!! // userId를 Kafka의 토픽에 key로 던져주면서 특정 파티션에 들어가도록 설정한다 // (던져주면 partition = HASH(useriD) mod (파티션개수) 로 수행할듯) ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_LOGIN, requestLoginDTO.getUserId(), requestLoginDTO); // callback future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() { @Override public void onFailure(Throwable ex) { log.error(\"Unable to send message: {}\", ex.getMessage()); } @Override public void onSuccess(SendResult&lt;String, Object&gt; result) { log.info(\"Sent message with key: {}, offset: {}, partition: {}\", requestLoginDTO.getUserId(), result.getRecordMetadata().offset(), result.getRecordMetadata().partition()); } }); return ResponseEntity.ok(requestLoginDTO); }}MessageListener@Slf4j@Componentpublic class MessageListener { ... // 로그인 @KafkaListener(topics = \"${kafka.topic-login-request}\", containerFactory = \"loginKafkaListenerContainerFactory\") public void listenLogin(RequestLoginDTO loginDTO) { log.info(\"Receive [RequestLoginDTO] Message with userID={},userPw={}\", loginDTO.getUserId(), loginDTO.getUserPw()); Optional&lt;User&gt; user = userService.matchUserIdAndUserPw(loginDTO.getUserId(), loginDTO.getUserPw()); ResponseLoginDTO responseLoginDTO = new ResponseLoginDTO(); responseLoginDTO.setRequestUserID(loginDTO.getUserId()); if (user.isPresent()) { responseLoginDTO.setIsAccept(true); responseLoginDTO.setUser(user.get()); } else { responseLoginDTO.setIsAccept(false); responseLoginDTO.setUser(new User()); } // Kafka 메세지 전송/비동기 처리를 위한 ListenableFuture 사용 ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaProducerTemplate.send(TOPIC_LOGIN_RESPONSE, loginDTO.getUserId(), responseLoginDTO); // 메시지 비동기 callback 처리 future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() { @Override public void onFailure(Throwable ex) { log.error(\"Fail to send message to broker: {}\", ex.getMessage()); } @Override public void onSuccess(SendResult&lt;String, Object&gt; result) { log.info(\"Send message with offset: {}, partition: {}\", result.getRecordMetadata().offset(), result.getRecordMetadata().partition()); } }); }}KafkaTopicConfig// 역할1. Kafka 토픽 할당@Configurationpublic class KafkaTopicConfig { @Autowired private KafkaAdmin kafkaAdmin; @Value(\"${kafka.topic-login-request}\") public String TOPIC_LOGIN_REQUEST; @Value(\"${kafka.topic-login-response}\") public String TOPIC_LOGIN_RESPONSE; // 계산 잘 해야한다. Partition 개수 &gt;= Group내 Conusmer 개수. // 생성하고자 하는 Conumser=2 Partition은 2이기에, 각각 conusmer에게 leader-partition 매칭가능 private NewTopic generateTopic(String topicName) { return TopicBuilder.name(topicName) .partitions(2) // 파티션 할당 개수 .replicas(3) // broker 3대에 할당 .build(); // 즉, 토픽은 총 2개의 leader-partition, 2개의 follow-partition 보유 } @PostConstruct public void init() { kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGIN_REQUEST)); kafkaAdmin.createOrModifyTopics(generateTopic(TOPIC_LOGIN_RESPONSE)); }}KafkaProducerConfig@Configuration@EnableKafkapublic class KafkaProducerConfig { @Value(\"${kafka.bootstrap-servers}\") private String bootstrapServer; private ProducerFactory&lt;String, Object&gt; producerFactory() { Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;(); // Kafka Broker 엔드 포인트 할당(8097, 8098, 8099) configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); // Kafka에 전송될 키는 스트링으로 설정 configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); // Kafka에 전송될 값을 직렬화 하는 방법을 지정 configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class); return new DefaultKafkaProducerFactory&lt;&gt;(configProps); } @Bean public KafkaTemplate&lt;String, Object&gt; kafkaProducerTemplate() { return new KafkaTemplate&lt;&gt;(producerFactory()); }}KafkaConsumerConfig// 역할1. Kafka연결// 역할2. 메세지를 받아서 우리가 원하는 타입인 RequestLoginDTO로 변환@Slf4j@EnableKafka@Configurationpublic class KafkaConsumerConfig { @Value(\"${kafka.bootstrap-servers}\") private String bootstrapServer; ... // login consumer 객체 변환 @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, RequestLoginDTO&gt; loginKafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory&lt;String, RequestLoginDTO&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(loginConsumerFactory(\"chatServerGroup\")); return factory; } // login consumer 객체 변환 public ConsumerFactory&lt;String, RequestLoginDTO&gt; loginConsumerFactory(String groupId) { // 앞서 producer이 json직렬화로 보냈으니, 마찬가지로 jsonDeserialize JsonDeserializer&lt;RequestLoginDTO&gt; deserializer = new JsonDeserializer&lt;&gt;(RequestLoginDTO.class); deserializer.setRemoveTypeHeaders(false); // 모든 패키지 신뢰 deserializer.addTrustedPackages(\"*\"); deserializer.setUseTypeMapperForKey(true); // 객체변환 설정 ImmutableMap&lt;String, Object&gt; config = ImmutableMap.&lt;String, Object&gt;builder() .put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer) .put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class) .put(ConsumerConfig.GROUP_ID_CONFIG, groupId) // 역직렬화 로직 .put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deserializer) .build(); return new DefaultKafkaConsumerFactory&lt;&gt;(config, new StringDeserializer(), deserializer); }}유저의 ID는 모든 서비스에 필요한 중요한 key이며 유니크하다. kafkaProducerTemplate.send(.., key=userId, ..)로 front에서 kafka의 특정 파티션으로 들어가도록 하고, 해당 파티션은 backend 서버 중 하나가 계속(만약 계속 살아있다면) 읽게된다면 메모리의 중복사용이 해결된다.Kafka Test참고로 Kafka브로커는 3대(port:29092, 39092, 49092)를 설정하였고 아래의 docker compose를 통해 실행하였다.version: '3'services: zookeeper: image: confluentinc/cp-zookeeper:7.2.1 container_name: zookeeper environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka1: image: confluentinc/cp-kafka:7.2.1 container_name: kafka1 ports: - \"8097:8097\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8097,INTERNAL://kafka1:9092 KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL kafka2: image: confluentinc/cp-kafka:7.2.1 container_name: kafka2 ports: - \"8098:8098\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8098,INTERNAL://kafka2:9092 KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL kafka3: image: confluentinc/cp-kafka:7.2.1 container_name: kafka3 ports: - \"8099:8099\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 3 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8099,INTERNAL://kafka3:9092 KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL위의 docker compose를 실행하게 되면 아래와 같이 컨테이너 생성된다.gyuminhwangbo@Gyuminui-MacBookPro ghkdqhrbals.github.io % docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9e0fd3e5a419 confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 10 minutes ago Up 10 minutes 0.0.0.0:8098-&gt;8098/tcp, 9092/tcp kafka2b24b4c68887b confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 10 minutes ago Up 10 minutes 0.0.0.0:8099-&gt;8099/tcp, 9092/tcp kafka37dfabb292ff4 confluentinc/cp-kafka:7.2.1 \"/etc/confluent/dock…\" 10 minutes ago Up 10 minutes 0.0.0.0:8097-&gt;8097/tcp, 9092/tcp kafka102adb2b2386d confluentinc/cp-zookeeper:7.2.1 \"/etc/confluent/dock…\" 10 minutes ago Up 10 minutes 2181/tcp, 2888/tcp, 3888/tcp zookeeper이후, 아래와 같이 backend에서 login-response로 전달했던 메세지를 확인해볼 수 있다.POST /login -&gt; producer -&gt; kafka -&gt; consumer 이 부분# kafka3 브로커 접속gyuminhwangbo@Gyuminui-MacBookPro ghkdqhrbals.github.io % docker exec -it kafka3 /bin/bash# login-response에 쌓인 메세지 확인[appuser@b24b4c68887b ~]$ kafka-console-consumer --bootstrap-server localhost:9092 --topic login-response --from-beginning# login fail{\"requestUserID\":\"aa\",\"isAccept\":false,\"user\":{\"userId\":null,\"userPw\":null,\"email\":null,\"userName\":null,\"userStatus\":null,\"joinDate\":null,\"loginDate\":null,\"logoutDate\":null}}# login success{\"requestUserID\":\"a\",\"isAccept\":true,\"user\":{\"userId\":\"a\",\"userPw\":\"1234\",\"email\":\"a@naver.com\",\"userName\":\"user_A\",\"userStatus\":\"안녕하세요!\",\"joinDate\":[2022,12,17],\"loginDate\":[2022,12,17],\"logoutDate\":[2022,12,17]}}추가적인 테스트직접 토픽 생성부터 pub/sub 하는 테스트를 추가적으로 진행해보았다.# kafka2 브로커 접속gyuminhwangbo@Gyuminui-MacBookPro ghkdqhrbals.github.io % docker exec -it kafka2 /bin/bash# 직접 토픽생성[appuser@9e0fd3e5a419 ~]$ kafka-topics --bootstrap-server localhost:9092 --create --topic randomTopic2 --partitions 3 --replication-factor 3Created topic randomTopic2.# 생성된 토픽확인[appuser@9e0fd3e5a419 ~]$ kafka-topics --describe --topic randomTopic2 --bootstrap-server kafka1:9092Topic: randomTopic2\tTopicId: BL7DT0u4SuO_flLPKHbutg\tPartitionCount: 3\tReplicationFactor: 3\tConfigs:\tTopic: randomTopic2\tPartition: 0\tLeader: 3\tReplicas: 3,2,1\tIsr: 3,1,2\tTopic: randomTopic2\tPartition: 1\tLeader: 1\tReplicas: 1,3,2\tIsr: 3,2,1\tTopic: randomTopic2\tPartition: 2\tLeader: 2\tReplicas: 2,1,3\tIsr: 3,1,2# 직접 producing[appuser@9e0fd3e5a419 ~]$ kafka-console-producer --bootstrap-server localhost:9092 --topic randomTopic&gt;Hi&gt;AA# 다른 터미널에서 kafka3 브로커 접속gyuminhwangbo@Gyuminui-MacBookPro ghkdqhrbals.github.io % docker exec -it kafka3 /bin/bash# 직접 consuming[appuser@b24b4c68887b ~]$ kafka-console-consumer --bootstrap-server localhost:9092 --topic randomTopic --from-beginningHi # 이와같이 받아볼 수 있다.AA" }, { "title": "Kafka에 대한 고찰", "url": "/posts/kafka(2)/", "categories": "서버, 메세지 큐", "tags": "", "date": "2022-12-19 01:00:25 +0000", "snippet": " Request-response (HTTP) vs. event streaming (Kafka) 정리 글Use Cases and Architectures for HTTP and REST APIs with Apache Kafka event streaming 2Event Sourcing 페이스북같은 경우, 유저 프로파일을 변경했을 때 여러 다른서버들과 연동이 일어난다. 이러한 서버들은 서버 입장에서 바로 유저에게 반환하지 않아도 된다. Let’s take an example. Consider a Facebook-like social networking app (albeit a completely hypothetical one) that updates the profiles database when a user updates their Facebook profile. There are several applications that need to be notified when a user updates their profile — the search application so the user’s profile can be reindexed to be searchable on the changed attribute; the newsfeed application so the user’s connections can find out about the profile update; the data warehouse ETL application to load the latest profile data into the central data warehouse that powers various analytical queries and so on. Event sourcing involves changing the profile web app to model the profile update as an event — something important that happened — and write it to a central log, like a Kafka topic. In this state of the world, all the applications that need to respond to the profile update event, merely subscribe to the Kafka topic and create the respective materialized views – be it a write to cache, index the event in Elasticsearch or simply compute an in-memory aggregate. The profile web app itself also subscribes to the same Kafka topic and writes the update to the profiles database." }, { "title": "실시간 채팅방 구현(3) - (프로젝트 수행시 고려점2)", "url": "/posts/chatting(3)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-18 04:00:25 +0000", "snippet": "잘못된 이해로 비롯된 에러1. spring-data-jpa save(S entity)본 프로젝트에서는 Service Layer에서 트랜젝션처리를 수행한다. 또한 Repository는 data-jpa를 사용했다. 필자는 서비스 내 예외 발생 시, 원하는 값을 반환하고싶었다. 따라서 service내 try-catch로 UniqueKeyViolation 예외발생 시 값을 반환하는 코드를 추가했다. 하지만, 문제는 data-jpa에서 발생했다. save시, 먼저 select후 insert되는 점이였다. 만약 같은 id값을 가지는 객체가 존재한다면 persist가 아닌 merge가 되었었다. 즉, save는 이미 같은 key를 가지는 데이터가 존재하면 변경감지하여 update하며, 없을때만 insert되기에 예외처리가 수행되지 않았다.아래는 data-jpa에서의 save() 이다.@Transactional@Overridepublic &lt;S extends T&gt; S save(S entity) { if (entityInformation.isNew(entity)) { em.persist(entity); // 원하는 흐름. 이쪽으로 갔을 때, UniqueKeyViolation 예외 발생한다. return entity; } else { return em.merge(entity); // 하지만 이쪽으로 흘러갔다. }}따라서 data-jpa를 사용하기 위해선 먼저 서비스에서 userRepository.findById(user.getUserId())과정을 필수적으로 작성해야한다. // userService @Override public ResponseEntity&lt;?&gt; save(User user) { Optional&lt;User&gt; findUser = userRepository.findById(user.getUserId()); if (findUser.isPresent()) { return ResponseEntity.badRequest().body(\"해당 ID로 등록된 유저가 존재합니다\"); } userRepository.save(user); return ResponseEntity.ok(user); }2. REST and KafkaREST에서는 HTTP통신이기에 GET/POST를 구분할 수 있다. 하지만 Kafka는 이를 구분하지 않는다(물론 이를 지원해주는 Confluent Rest Proxy가 존재한다). Kafka는 data stream platform으로 그 형태자체가 다르기 때문이다. 이를 잠깐 깜빡하고 api를 다 짜놨는데 다시 고치려니 막막하다… PostMapping(\"/user/{userId}\")이런 url로 가져왔던 부분들을 수정해야한다. 즉, producer은 json에 모든 request 내용을 담아 전송하고 consumer에서 이를 읽는 Request/Response 형식으로 작성해야한다. 아래의 포스팅은 필자가 하고자 하는 방향인 Kafka를 Request/Response로 사용하는 것에 대한 고찰을 확인할 수 있다. If you build a modern enterprise architecture and new applications, apply the natural design patterns that work best with the technology. Remember: Data streaming is a different technology than web services and message queues! CQRS with event sourcing is the best pattern for most use cases in the Kafka world: … Nevertheless, it is still only the second-best approach and is often an anti-pattern for streaming data. reference : Request-Response in Kafka 또한 유저가 채팅을 보냈을 때, 제대로 서버에서 저장했는지 서버로부터 다시 받아봐야한다. 필자는 이를 웹소켓으로 받는다. 또 다르게 받아볼 수 있는 Server-Sent Event(SSE)방식이 존재해서 아래와 같이 링크를 남긴다. Server-Sent Events (SSE) is a server push technology where clients receive automatic server updates through the secure http connection. SSE can be used in apps like live stock updates, that use one way data communications and also helps to replace long polling by maintaining a single connection and keeping a continuous event stream going through it. We used a simple Kafka producer to publish messages onto Kafka topics and developed a reactive Kafka consumer by leveraging Spring Webflux to read data from Kafka topic in non-blocking manner and send data to clients that are registered with Kafka consumer without closing any http connections. This implementation allows us to send data in a fully asynchronous &amp; non-blocking manner and allows us to handle a massive number of concurrent connections. We’ll cover: •Push data to external or internal apps in near real time •Push data onto the files and securely copy them to any cloud services •Handle multiple third-party apps integrations reference : Server Sent Events using Reactive Kafka and Spring Web flux즉, SSE는 One-way connection that only server can send data to the client 이다. 본 프로젝트에서 채팅기능은 양방향 통신을 해야하기때문에 SSE는 해당 기능에서 제외한다. 또한 Conenction을 계속 유지해야하기때문에 서버의 리소스 소모가 꽤 클것으로 예상된다(물론 웹소켓도 마찬가지…)." }, { "title": "실시간 채팅방 구현(2) - (프로젝트 수행시 고려점1)", "url": "/posts/chatting(2)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-17 04:00:25 +0000", "snippet": "프로젝트 수행 시 고려할 문제점문제점1. 메모리 누수front-back으로 기존 모노서버를 분산하던 중, JPA로 Chatting 을 저장할 때 메모리 누수가 관찰되었다.먼저 Kafka를 통해 전달받는 메세지 구조는 다음과 같다.public class ChatMessage { private Long roomId; private String writer; private String writerId; private String message; private ZonedDateTime createAt;}필자는 JPA를 통해 persistance를 활용하는데, 아래는 backend 서버에서 저장하는 JPA Chatting 엔티티 구조이다.public class Chatting { @Id @GeneratedValue private Long id; @ManyToOne @JoinColumn(name = \"ROOM_ID\") private Room room; @ManyToOne @JoinColumn(name = \"USER_ID\") private User sendUser; ...}메세지를 저장하는 과정은 다음과 같다. ChatMessage를 수신받는다. roomService.findByRoomId(roomId)와 userService.findByUserId(writerId)를 수행하여 Chatting에 넣을 sendUser와 room객체를 생성한다. Chatting 객체를 생성하고 저장한다.문제는 2번 과정에서 발생한다. 매번 ChatMessage를 수신받을때마다 새로운 sendUser와 room 객체를 생성하기때문에 동시접속자가 많은 경우 메모리 사용량이 증가한다. 해당 문제는 차후 최적화 때 해결할 것이다.문제점2. 메모리상에서 유저데이터의 복제필자는 두 개의 동일 서비스수행 서버를 운영하는데 각각의 메모리가 다르다. 그렇다면, 다른 서버로 요청이 전달되면 해당 서버는 또 쿼리를 날리게 된다. 즉, 1차 캐시에 저장된 유저데이터가 세션에 종속적이지 않으며, 동일한 유저데이터가 두개의 서버의 메모리에 복제되어있는 문제점 발생.해결 방법은 유저가 초기 인증 시, 카프카의 특정 파티션에만 들어가도록 설정하면 된다. 그리고 해당 파티션을 특정 consumer가 구독할 수 있도록 설정함으로써 일종의 sticky session과 비슷하게 흘러가도록 설정한다." }, { "title": "[INFO] Kafka Topic Naming Conventions", "url": "/posts/error2/", "categories": "INFO/WARNING/ERROR, INFO", "tags": "error", "date": "2022-12-12 01:00:25 +0000", "snippet": " Kafka Topic은 다음의 문자열만 사용 가능하다.val legalChars = \"[a-zA-Z0-9\\\\._\\\\-]\" / : X Topic 네이밍의 추천하는 기존 컨벤션들은 다음과 같다. &lt;namespace&gt; .&lt;product&gt;.&lt;event-type&gt; &lt;application&gt;-&lt;data-type&gt;-&lt;event-type&gt; &lt;organization&gt;.&lt;application-name&gt;.&lt;event-type&gt;.&lt;event&gt; 예시 Kakao 어플리케이션에서 ChatMessageDTO를 pub/sub하는 kafka topic이라면,kakao-ChatMessage-Chat 이라고 네이밍하는 것이 일반적이다. " }, { "title": "실시간 채팅방 구현(1) - (STOMP 프로토콜 연동완료)", "url": "/posts/chatting(1)/", "categories": "채팅서버 프로젝트, 프로젝트 진행", "tags": "", "date": "2022-12-11 06:00:25 +0000", "snippet": "현재의 단계현재까지 version1 실시간 채팅방 구현을 위한 STOMP/Spring 연동이 완료되었다. 상세한 내용과 코드는 다음의 github 링크에서 확인할 수 있다. GITHUB : https://github.com/ghkdqhrbals/spring-chatting-server/tree/v1 v1 동작 : https://www.youtube.com/watch?v=nwD3AX6CJcc하지만 여전히 미비한 기능들 부족한 장애대응 Kafka를 사용하기 이전에는 아래와 같이 데이터를 주고받는것을 목표로 하였었다. Kafka 이전에 고려했던 점 프론트에서 WebClient를 사용해서 RestApi호출할 때, Spring의 WebClient를 비동기 + Non-Blocking으로 설계할 것. 이렇게 되면 순서는 다음과 같음. 프론트에서 @Async어노테이션으로 비동기 private 함수 호출(이때, websocket으로 진행중표시) 비동기 함수에서 webClient를 .subscribe(result-&gt; hashset… )로 non-blocking설정하고 restapi들(하나의 api를 호출하는 것은 non-blocking의미가 없다. 호출이 여러개여야지 한방에 다 보내고 wait으로 기다릴 떄, 성능향상을 기대할 수 있기때문)을 호출하고 wait 비동기함수 내 webClient의 response가 도착하면 websocket으로 데이터 표시 하지만 이렇게 설계한다면, 백엔드 서버가 죽었을 때 다른 서버로 누가 대신 요청을 옮겨줄 수가 없다! 부족한 확장성 서비스를 수평적으로 확장하는데 있어 부족함을 느꼈다. Client -- Server -- DB인 단순한 구조였기에 서버가 웹소켓 관리 + 비즈니스 로직수행 + DB 관리 + 로그기록이기에 부하가 많았고, Server를 늘렸을 때 DB sync를 맞추기 힘들다. 로그기록 저장 부하 발생 로그기록은 바로바로 처리하지 않아도 된다. 그러니까 실제로 로그를 저장하는 시점을 보장하지 않아도 된다. 하지만, 기존의 구조에서는 바로바로 처리하는 구조이기에 부하가 발생했다. 그렇다면 필자는 어떤식으로 보완해야할까? = Kafka 사용 장애 대응 용이 Kafka는 장애대응이 용이하다. 특정 서버가 죽었을 때, 다른 서버가 데이터를 받아서 처리하도록 설정가능하다. 이는 Kafka가 죽었을 때도 마찬가지이다. 카프카 클러스터 내 특정 데이터 큐(topic-partition)을 도맡아(leader partition) 처리하는 카프카 서버(broker)가 죽어도 다른 카프카 서버 내 싱크된 큐(follow partition)가 대신해서 처리하도록 대장 카프카 서버(controller broker)가 장애를 대응해준다. 이 대장 카프카 서버는 zookeeper에 의해 선정/관리된다. 확장성 용이 카프카는 ConsumerGroup이 데이터(topic)와 매칭되어 데이터를 받을 수 있도록 도와준다. ConsumerGroup은 쉽게 생각해서 같은 기능의 서버(Consumer)를 수평적으로 확장해놓은 집합이라고 생각하면 된다. 그리고 이러한 ConsumerGroup내 서버(Consumer)들 중 하나가 죽으면, 그 서버에서 처리하던 특정 데이터(topic-partition)들을 다른 서버가 이어받아 처리하도록 도와준다. 메세지 큐로써의 장점보유 카프카는 메세지 큐이다. 메세지 큐는 작업요청의 수행시점을 고려하지 않은 아키텍쳐이다. 이를 제대로 활용가능한 예로 어뷰저 관측을 들 수 있다. 어뷰저 관리는 부가기능으로, 채팅앱 유저가 실시간으로 알 필요가 없다. 따라서 서버는 메세지 큐에 채팅기록들을 넣어놓고 천천히 어뷰저 관측기능을 수행하면서 이후 어뷰저 발견 시, 유저에게 알람하는 식으로 처리할 수 있다. 즉, 데이터의 백본역할을 수행하는 Kafka를 통해 장애대응과 확장성 및 부가기능수행에서 이점을 가져갈 수 있다.어떤식으로 Kafka를 implement해야할까?일단 Kafka의 topic은 stomp처럼 많이 생성할 수 없다. 이는 broker수만큼 topic의 replica를 감당해야하며, 모든 topic들의 partition(ISR-InSync-Replica)들은 follow partition들과 sync과정을 가지기 때문에 각기 broker에 걸리는 부하는 (총 broker개수)(topic개수partition개수) 이상이다. 즉, topic 개수에 limit에 존재한다. 무작정 chat.room.10 이런식으로 채팅방 별 topic을 설정할 수 없다는 것이다. 따라서 클라이언트가 직접 publish/consume할 수 없다.Is There a Limit on the Number of Topics in a Kafka Instance?그렇다면 이상적인 kafka 통신방법은 무엇일까?먼저 서버의 위치부터 정리해보자 client : port:8080 kafka 서버 : port:9092, port:9093 chat Front Server : port:8081 - (기능 : 세션/인증관리/웹소켓 관리, 에러처리) - Confluent Rest Proxy와 같은 역할 Confluent Rest Proxy : 카프카를 사용하지 못하는 클라이언트 환경을 고려해 POST는 Producer로, GET은 Consumer로 HTTP를 Kafka와 매칭시켜주는 기술 or 플랫폼 chat Backend Server : port:8083, port:8084 - (기능 : DB관리, restapiServer, 비즈니스로직수행) consumerGroup:groupId=chat Kafka 통신순서 예시(로컬에서의 채팅방 입장 및 채팅전송) 채팅방 입장 클라이언트는 chat Front Server에 GET localhost:8081/chat/{roomId} 요청. 클라이언트는 비동기로 해당 요청을 실행하며, webclient의 .subcribe()으로 non-blocking 실행한다.(Asnyc-NonBlocking은 아래 참조)reference : Blocking-NonBlocking-Synchronous-Asynchronous localhost:8081은 이를 받고 kakao.chat.chatRoom.REQUEST 토픽으로 localhost:9092에 메세지를 put한다. { “userId”:”HwangboGyumin”, “roomId”:10 } kakao.chat.chatRoom.REQUEST 토픽 내 실제 메세지가 삽입된 leader partition을 구독중인 Consumer localhost:8083 은 메세지를 소비한다. localhost:8083 은 로직 수행하여 DB에서 이전 채팅목록을 kakao.chat.chatRoom.RESPONSE 토픽으로 localhost:9092에 메세지를 put한다. { \"endpoint\":\"ws://localhost:8081/stomp/chat\", \"chatRecord\":{ \"userId\":\"HwangboGyumin\", \"roomId\":10, \"chatMessage\":{ \"userId\":\"A\", \"roomId\":\"10\", \"meesage\":\"안녕?\", \"createdAt\":\"20221213 13:10 +09:00 UST\" },{ \"userId\":\"B\", ... } }} localhost:8081은 해당 메세지를 소비하여 클라이언트에 이전채팅목록 반환 채팅전송 클라이언트는 이전 채팅방 접속 시 반환받은 “endpoint”:ws://localhost:8081/stomp/chat주소에 웹소켓을 연결한다. 연결성공 이후, POST localhost:8081/chat/{roomId}로 chatMessage를 전송한다. localhost:8081은 이를 수신받고 먼저 localhost:9092에 kakao.chat.chatMessage.REQUEST 토픽으로 전달한다. kakao.chat.chatMessage.REQUEST 토픽 내 실제 메세지가 삽입된 leader partition을 구독중인 Consumer localhost:8083 은 메세지를 소비한다. localhost:8083은 로직 수행하여 DB에 chatMessage 저장 및 kakao.chat.chatMessage.RESPONSE에 성공여부 메세지를 put한다. localhost:8081은 kakao.chat.chatMessage.RESPONSE토픽에 저장된 메세지를 소비하여 chatMessage DB저장 성공 여부를 판단한다. 제대로 저장되었다면, localhost:8081에서 stomp.send(topic:/pub/chat/room/{roomId})로 chatMessage를 publish한다. 클라이언트는 아래와 같이 해당 토픽을 웹소켓을 통해 수신받고 있기에 화면에 채팅이 표시된다. stomp.subscribe(\"/sub/chat/room/\" + roomId, function (chatMessage) { ... } 현재의 프로젝트에 Kafka를 연동함으로써 예상되는 장점요약 서버 수평확장에 있어 DB Sync 설정가능(Kafka Connect의 CDC) Kafka Connect : 트랜젝션 로그들로 sync하기에 ACID 보장가능 서버 다운 시 대처가능 kafka의 zooKeeper에서 변화감지 및 Consumer 변경 가능함으로써 서버다운대처가능 부가기능들 자유롭게 수행가능 메세지의 실시간 처리가 필요없는 부가기능들 = 응답 안기다려도 되는 기능 ex) 어뷰저 관측, 로그저장 추가할 작업 Chatting 메세지를 저장하는 Repository 추가 고려할 점 사용자가 마지막에 읽은 채팅의 위치를 저장하는 칼럼 추가 지금 채팅방에 입장할때마다 입장메세지가 표시된다. 이것을 삭제하고 입장메세지의 토픽인 /pub/chat/enter의 핸들러를 거칠때마다 마지막 읽은 메세지의 위치 표시(반환) 카카오의 톡서랍처럼 나한테 중요한 메세지들을 채팅방,채팅메세지,시간 이렇게 저장할 수 있도록 중요메세지 저장 테이블 추가 Room/Participant 제거기능 추가 User가 삭제되었을 때, 연관된 데이터 삭제하도록 데이터 흐름 관찰(JPA실제 쿼리문 관찰) Kafka 설정 UserServer/FriendServer/ChattingServer 전부 따로 떼서 Kafka를 통해 서로 메세지를 주고받도록 설정 고려할 점 Partition + Broker 추가 Docker-compose로 Kafka/DB/Spring 구동 편리성 도모 Spring 서버 다운 시, 대책마련 메시지가 소비될 떄, 멱등성 고려해야함 ElasticSearch LoggingServer 추가 Reference https://softwareengineering.stackexchange.com/questions/422177/is-kafka-needed-in-a-realtime-chat-application https://www.confluent.io/blog/sync-databases-and-remove-silos-with-kafka-cdc/ https://softwareengineering.stackexchange.com/questions/422177/is-kafka-needed-in-a-realtime-chat-application" }, { "title": "Kafka에서 메세지를 처리하는 방식", "url": "/posts/kafka/", "categories": "서버, 메세지 큐", "tags": "", "date": "2022-12-02 06:00:25 +0000", "snippet": "카프카용어정리 Producer : 이벤트를 보내는 주체 Consumer: 토픽의 파티션에 저장되어 있는 메시지를 소비(consume)하는 역할. Consumer Group : 하나의 Topic의 모든 파티션들을 구독하는 consumer 그룹. Partition : Producer로부터 전달받은 메세지를 저장하는 공간 Leader Partition : 실질적으로 Producer/Consumer와 통신하는 파티션 Follower Partition : Leader Partition의 데이터를 복제하여 저장하는 파티션. 그래서 Leader Partition에 장애가 생겼을 때, Follower Partition이 리더로 승격가능. Broker = 카프카 서버 : 토픽과 파티션들을 관리하는 역할. Controller : 하나의 브로커가 이 역할을 맡는다. 리더 파티션이 문제 있을 때, Follow Partition을 Leader로 승격시켜주는 역할을 수행한다. Coordinator : 하나의 브로커가 이 역할을 맡는다. 장애로 인해 특정 파티션의 메세지가 소비되지 않을 때, 다른 Consumer에 매칭시키는 역할을 수행한다. Topic : 메세지의 주제 Offset : 파티션 내 메세지 위치. Kafka Cluster : = Kafka 서버(Broker)의 모임 Event : = Message Zookeeper : 카프카 메타데이터 저장 및 관리 서버. [How Kafka and ZooKeeper Work Together] Broker 관리 Broker 생성/삭제/장애감지 Broker Controller/Coordinator 선정 Topic 관리 Topic 별 권한 설정 요청 쿼터 관리 Replication Factor : Topic별로 처리하는 Broker 개수를 정하는 계수. 3대의 Broker로 처리하도록 하면, Broker이 장애 시 다른 Broker가 처리 가능하도록 도와준다. Topic의 중요도에 따라 Broker 설정해야함. 카프카 진행방식 클라이언트가 메세지를 Producer에게 전송 Producer가 메세지를 batch size 만큼 토픽의 특정 파티션의 리더에 저장한다. 이 때, 파티션의 리더/팔로우 들은 ISR(In Sync Replica)로 묶여 있으며, 서로 데이터 싱크를 맞추게 된다. 각각의 브로커와 연결된 Consumer가 이를 필요할 때 읽으면서 이벤트를 핸들링한다. 이 때, Broker은 전달된 이벤트의 메타 데이터를 기록한다.특성 batching : Producer/Consumer이 여러개의 메세지를 묶어서 Kafka에 전송/수신하는 기능. 이를 이용하면 요청별로 발생하는 오버헤드를 방지할 수 있다. request quota(요청 쿼터) : Broker별로 특정 클라이언트가 전송/수신받을 수 있는 자원양을 설정할 수 있는 기능. 하나의 사용자로 인한 서버 부하 방지 가능. 데이터 영속성 보장 : 메시지를 기본적으로 메모리에 저장하는 기본 메시징 시스템과는 달리 메시지를 파일 시스템에 저장한다. 그래서 데이터 영속성이 보장된다. Consumer pull 방식 : 기존의 메시징 시스템에서는 Broker가 Consumer에게 메시지를 Push해주는 방식인데 반해, Kafka는 Consumer가 Broker로부터 직접 메시지를 가지고 가는 pull방식으로 동작한다. 이를 이용하면 Consumer은 자신의 메세지 처리 성능에 따라 최적으로 메세지를 가져올 수 있게된다. 장점 Leader/Follow 덕에 안정적으로 서버 운영 가능. 예시 시나리오 Zookeeper에서 Controller Broker 장애 감지. 새로운 Controller Broker 선출. 선출 된 Controller Broker은 Zookeeper내 state변화(장애발생 Broker에 할당된 Leader Partition 상태 변화) 감지. 다른 Broker들에게 장애발생된 Leader Partition의 Follow Partition들을 받아와서 이 중 하나를 Leader로 승격. [Controller Broker] Topic 별 데이터를 묶어서 처리하기에 오버헤드를 줄일 수 있음. 예시 : 온라인 상품 구매 프로세스에서 재고 수량은 실시간으로 업데이트 되나(Producer Remain.batch_size = 0), 구매 로그(Producer Remain.batch_size = 50)는 실시간 처리 보다는 배치 처리한다. 주의사항 Consumer은 브로커의 메세지를 여러번 읽을 수 있기 때문에, 멱등성을 고려해야한다. 멱등성 : Consumer이 같은 메세지를 여러번 받아도 한번 받은것처럼 처리하는 것 Reference https://velog.io/@jwpark06/Kafka-시스템-구조-알아보기 Setting client quotas - IBM Event Streams https://jhleed.tistory.com/180 https://engineering.linecorp.com/ko/blog/how-to-use-kafka-in-line-1/ https://galid1.tistory.com/793 https://goyunji.tistory.com/125 https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-brokers.html" }, { "title": "메세지 큐(MQ)의 개념 및 장점", "url": "/posts/message-queue/", "categories": "서버, 메세지 큐", "tags": "", "date": "2022-12-01 06:00:25 +0000", "snippet": "메세지 큐(MQ)MQ 란?클라이언트의 요청을 큐에 저장하고 Consumer이 필요할 때 꺼내쓰는 아키텍처.장점 비동기(Asynchronous) 메시지 큐는 생산된 메시지의 저장, 전송에 대해 동기화 처리를 진행하지 않고, 큐에 넣어 두기 때문에 나중에 처리할 수 있다. 낮은 결합도(Decoupling) 생산자 서비스와 소비자 서비스가 독립적으로 행동하게 됨으로써 서비스 간 결합도가 낮아진다. 확장성(Scalable) 생산자 서비스 혹은 소비자 서비스를 원하는 대로 확장할 수 있기 때문에 확장성이 좋다. 탄력성(Resilience) 소비자 서비스가 다운되더라도 어플리케이션이 중단되는 것은 아니다. 메시지는 메시지 큐에 남아 있다. 소비자 서비스가 다시 시작될 때마다 추가 설정이나 작업을 수행하지 않고도 메시지 처리를 시작할 수 있다. 보장성(Guarantees) 메시지 큐는 큐에 보관되는 모든 메시지가 결국 소비자 서비스에게 전달된다는 일반적인 보장을 제공한다. " }, { "title": "Database-Transaction 격리수준", "url": "/posts/DB-2/", "categories": "CS, 데이터베이스", "tags": "", "date": "2022-11-20 06:00:25 +0000", "snippet": "모든 데이터베이스는 아래의 트랜젝션 격리 수준을 가진다. READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE 이해를 돕기 위해 A,B 트랜젝션이 동시에 특정 데이터에 진입할 때를 가정한다.1. READ UNCOMMITTED트랜젝션간 아무런 격리가 되지 않는 격리레벨이다.A에서 데이터를 1에서 2로 변경했을 때, B가 이를 읽으면 변경된 값인 2로 읽는 것이 본 격리 수준이다.즉 데이터를 변경하는 즉시 다른 트랜잭션이 읽으면, 변경되고 난 이후 값을 읽는다. 발생할 수 있는 문제점 : Dirty Read Dirty Read : A가 값 변경하고 B가 읽었을 때, A가 롤백을 할 경우에 발생한다. 이 때, B는 이미 없는 값으로 로직을 수행하게 되버린다.2. READ COMMITTED백업 레코드로 Dirty Read를 방지하는 격리레벨이다.어떤 트랜잭션의 변경 내용이 COMMIT 되어야만 다른 트랜잭션에서 조회할 수 있다.다른 트랜잭션에서의 변경 사항이 커밋되지 않은 경우, 실제 테이블의 값이 아닌 백업된 레코드에서 값을 가져오게 된다.하지만 NON-REPEATABLE READ 문제점 발생. 발생할 수 있는 문제점 : NON-REPEATABLE READ NON-REPEATABLE READ : 트랜젝션 내, 같은 SELECT 쿼리를 두번 수행할 떄 매번 결과가 달라지는 문제점(중간에 다른 트랜젝션에서 update 커밋했을 떄 발생함) 1번과 4번의 쿼리결과가 일치하지 않는다 번호 Tx1 Tx2 1 SELECT * FROM MEMBER   2   UPDATE MEMBER SET a = ‘c’ WHERE a = ‘a’ 3   COMMIT 4 SELECT * FROM MEMBER   실험(postgresql) 실험 주의점 : 두 개의 터미널에서 psql을 실행하여 각각의 트랜젝션을 실행해야한다. psql은 기본적으로 sql을 실행할 떄, BEGIN/COMMIT을 뒤에 넣어 싱글 트랜젝션으로 실행하기 떄문이다. START TRANSACTION; -- transaction id : 1SELECT * FROM MEMBER; START TRANSACTION; -- transaction id : 2 UPDATE MEMBER SET a = 'c' WHERE a = 'a'; COMMIT;SELECT * FROM MEMBER;COMMIT; 1번 쿼리 결과 : a,b4번 쿼리 결과 : b,c3. REPEATABLE READ백업 레코드+중간커밋 제외로 트랜젝션을 제어하는 격리레벨이다.트랜잭션이 시작되기 전에 커밋된 내용에 대해서만 조회할 수 있는 격리수준. 그래서 같은 쿼리를 날려도 일관된 결과를 보장한다. 문제는 insert로 인해 유령 레코드가 나타나는 PHANTOM READ 문제점이 존재한다(postgres는 이 격리에서 PHANTOM READ 발생 X).postgresql은 이 격리로 트랜젝션을 진행하면 테이블 자체를 백업으로 가져온다. 그래서 PHANTOM READ같은 문제점이 발생하지 않는다.즉, DB마다 성향과 에러 핸들링이 다르기때문에 모두 테스트 해봐야한다. 발생할 수 있는 문제점 : PHANTOM READ PHANTOM READ : 다른 트랜잭션에서 수행한 insert 작업에 의해 레코드가 보였다가 안보였다가 하는 현상 실험(MARIA DB) 1번과 5번의 쿼리결과가 일치하지 않는다 번호 Tx1 Tx2 1 select count(*) from Coupon   2   insert into Coupon values (‘c’) 3   COMMIT 4 update Coupon set name = ‘d’   5 select count(*) from Coupon   하지만 위와 같은 결과는 postgresql에서는 일어나지 않는다.4. SERIALIZABLE어떤 트랜잭션이 접근 하는 테이블 자체의 모든 R/W에 Lock을 건다. 그래서 완벽한 일관성 보장. The transaction waits until rows write-locked by other transactions are unlocked; this prevents it from reading any “dirty” data. The transaction holds a read lock (if it only reads rows) or write lock (if it can update or delete rows) on the range of rows it affects. For example, if the transaction includes the SQL statement SELECT * FROM Orders, the range is the entire Orders table; the transaction read-locks the table and does not allow any new rows to be inserted into it. If the transaction includes the SQL statement DELETE FROM Orders WHERE Status = ‘CLOSED’, the range is all rows with a Status of “CLOSED”; the transaction write-locks all rows in the Orders table with a Status of “CLOSED” and does not allow any rows to be inserted or updated such that the resulting row has a Status of “CLOSED”. Because other transactions cannot update or delete the rows in the range, the current transaction avoids any nonrepeatable reads. Because other transactions cannot insert any rows in the range, the current transaction avoids any phantoms. The transaction releases its lock when it is committed or rolled back. Reference in https://learn.microsoft.com/en-us/sql/odbc/reference/develop-app/transaction-isolation-levels?view=sql-server-ver16JPA 데이터 Lock비관적 잠금(Pessimistic Lock)현재 내가 Read/Write하고 있는 데이터에 Lock을 거는 기능READ COMMIT을 기본격리로 제공하는 데이터베이스의 경우 백업 레코드 접근을 허용한다. 데이터베이스에 쿼리가 날라가기 전 JPA에서 비관적 잠금으로 설정하게 된다면, 백업레코드 접근이 아닌 실제 레코드에 접근한다. h2-database/Postgres 는 기본적으로 READ COMMIT 트랜젝션 격리단계를 가진다.따라서 쿠폰 발급 서비스에 동시에 사용자 요청이 들어왔을 때, 같은 데이터를 들고 수정할 수 있게 되버린다.JPA는 비관적 잠금을 용도에 따라 두 가지로 제공한다. 읽기 잠금(Pessimistic Read Lock) 쓰기 잠금(Pessimistic Write Lock)1. 비관적 읽기 잠금(Pessimistic Read Lock)트랜젝션이 특정 데이터를 UPDATE로 변경할 때만 잠금한다. 해당 잠금은 READ COMMIT 에서는 기본적으로 제공하기에 실제로는 사용될 일이 거의 없다.일기 잠금은 만약 SELECT로 읽는다면 Lock하지 않는다. 따라서 다른 트랜젝션도 이를 읽을 수 있다.2. 비관적 쓰기 잠금(Pessimistic Write Lock)쿼리가 나갈 때, 뒤에 FOR UPDATE를 붙여주는 기능이다. 즉, 읽기에도 Lock을 걸어주는 기능.이 잠금은 데이터 정합성을 잘 지킬 수 있지만, DeadLock과 성능이슈를 조심해야한다. Dead Lock : 트랜젝션들이 서로 Lock되어있는 로우를 무한히 참조하려하는 문제점이다. 성능 이슈 : 간단히 읽을 때도 Lock이 걸리기 때문에 동시사용자가 많은 경우, 사용자들은 오랜시간 기다리게 된다.오라클은 waiting time도 설정할 수 있어 쉽게 핸들링가능. 물론 트랜젝션이 끝나기 전, 다시 한번더 읽어서 데이터 정합성을 유지하는 방법도 존재한다. 하지만 동시 사용자 수가 월등히 많을 때 이 역시 깨질 수 있기에,이후 확장성을 고려한다면 비관적 쓰기 잠금을 고려할만하다.Spring Data JPA에서는 아래와 같이 비관적 쓰기 잠금을 선언할 수 있다.public interface CouponRepository extends JpaRepository&lt;Coupon, Long&gt; { ... @Lock(LockModeType.PESSIMISTIC_WRITE) // Pessimistic Lock 설정(베타적 Lock) @Query(\"select c from Coupon c where c.couponItem.couponCode = :couponCode and c.userId is NULL\") @QueryHints({@QueryHint(name = \"javax.persistence.lock.timeout\", value =\"3000\")}) // 3초 타임아웃(set waiting time) List&lt;Coupon&gt; findByCouponCode(@Param(\"couponCode\") String couponCode, Pageable pageable); // Pageable = limit ...}" }, { "title": "등산코스 정하기", "url": "/posts/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/", "categories": "알고리즘, 그래프", "tags": "", "date": "2022-10-14 06:00:25 +0000", "snippet": "문제 설명산에 n개의 지점이 존재하며, 지점들은 아래와 같이 정의된다. 출입구 쉼터 산봉우리각각의 지점들은 그래프로 주어진다.등산코스를 정할 때, 산봉우리 중 한곳만 한번만 방문하고 다시 출입구로 돌아와야한다.등산코스를 따라 이동하는 중 쉼터 혹은 산봉우리를 방문할 때마다 휴식을 취할 수 있으며,휴식 없이 이동해야 하는 시간 중 가장 긴 시간을 해당 등산코스의 intensity라 한다.처음 출발한 출입구로 다시 되돌아 가야 한다.최소 intensity가 되는 등산코스를 찾아라! 즉, 특정 산봉우리와 해당 산봉우리로 경로를 지정 했을 때, 최대크기 간선이 최소가 되어야 한다.문제 접근 방식가본 관점 봉우리 찍으면 더 이상 bfs를 진행하지 않아도 된다 최대 크기 간선이 최소가 되는 경로를 선택한다 간선크기가 글로벌 최소 간선 크기를 초과하면 백 트래킹 한다 모든 산봉우리를 확인해봐야한다추가 관점 intensity가 최소가 되는 등산코스가 여러 개라면 그중 산봉우리의 번호가 가장 낮은 등산코스를 선택 노드가 gates나 summits의 원소인지 판단할 때 선형 검색을 하면 최악의 경우에 O(n)이 걸린다 gates, summits를 dictionary로 변환 후 사용 visited를 리스트로 작성하면 방문노드 확인할 떄, 최악 O(n)이 걸려버린다 visited를 set으로 변환 후 사용list find는 최악 O(n)인 반면 set find는 O(1) 큐에서 꺼낼 때, 짧은것부터 꺼내야댐설계 bfs로 접근해보자(백 트래킹으로 최소 intensity를 초과할 때 컷할 수 있기때문에 선택) 인접행렬그래프 생성X 2 &lt;= n &lt;= 50000이라서 그래프로 그리면 5만*5만이 되버린다. -&gt; dictionary key 양방향으로 대체 bfs 설계 큐 : [ 현재 지점, 현재까지 최대 크기 간선, 방문한 지점 리스트 ] edge = defaultdict(list)for v1, v2, intensity in paths: edge[v1].append((v2, intensity)) edge[v2].append((v1, intensity)) 느낀점 visited 설계할 떄, list가 아닌 set을 씀으로써 속도가 더 빨라졌다.큐에 set을 추가할 떄, visited.add()해서 visited를 그대로 넣어줬었다.이 때, 주소값이 넘어가는 것이 아닌, 실제 value들만 넘어가기때문에 수월했다. 아니네…. 주소값이 넘어간다. 그래서 copy로 값만 가져와야됨." }, { "title": "이 후 포스팅은 한글로 진행하려합니다", "url": "/posts/%ED%9A%8C%EA%B3%A0/", "categories": "", "tags": "", "date": "2022-10-13 06:00:25 +0000", "snippet": "회고필자는 현재 학사/석사를 마치고 28살 취준을 시작하였다.취준 중, 취업에 도움이 되고자 내 개인 지식들을 영어로 정리하는 블로그를 현재까지 운영해보았다.영어로 운영한 이유는 단 하나였다. “영어에 익숙해지자!”하지만 한국에서 개발자로 취업한다면 영어로 내 생각을 정리한 것은 그리 도움이 되지 않는것 같다.여러 코딩테스트를 통과 후 한글로 면접을 볼 때를 생각해보면 알 수 있다.(한글) 면접질문 -&gt; (영어) 생각 -&gt; (한글) 생각번역위의 예시처럼, 영어를 한글로 생각을 번역하는 과정이 추가되버리는 것이다.즉, 영어로 블로그 포스팅을 하는것에 회의감을 느껴 이 후 포스팅은 한글로 작성하려 한다." }, { "title": "Spring-2", "url": "/posts/spring-2/", "categories": "서버, Spring", "tags": "", "date": "2022-10-12 06:00:25 +0000", "snippet": "In Korea, Spring framework usage takes first placeAbove figure shows the trend of Spring verses GolangThus, from now I will study about the whole Spring framework and also the Java.References https://syntaxsugar.tistory.com/entry/GoGolang-Scheduler https://velog.io/@kineo2k/고루틴은-어떻게-스케줄링되는가 https://go.dev/src/runtime/HACKING" }, { "title": "Spring-1(Web Application Server and Web Server)", "url": "/posts/spring-1/", "categories": "서버, Spring", "tags": "", "date": "2022-10-12 06:00:25 +0000", "snippet": "What is Web Application Server? Web Application Server Server that processing business logic with DB What is Web Server? Web Sever performing Load-balancing caching Nginx, Apache are representative Web Server Originally WAS and Web Server is combined as one big server. But current days, traffic is growing rapidly and also the DB access rate. And here is the problem that big server cannot handle all of traffic. So, engineers seperate big server into two server, i.e. WAS and Web Server. By seperating into difference space, we can get advantages below. First, we can get high scalabilty. We only need additional entry point and server to scale up server. Because WAS can automatically balance(Round Robin, etc.) the traffics by just adding additional server’s entry point. Second, we can reduce load of repetitive Client’s requests. Because the Web Server can caching Client’s requests(HTML, CSS, etc. files), so that prevent from heavy load on business logic in WAS. Web Server can caching static HTML, CSS files. Web Server has two representative frameworks Apache Use a thread for each requests Nginx Asynchronous event-based handling requests Faster than apache when multiple requests come References https://gmlwjd9405.github.io/2018/10/27/webserver-vs-was.html https://losskatsu.github.io/it-infra/webserver/#톰캣tomcat" }, { "title": "Database-Normalization", "url": "/posts/DB-1/", "categories": "CS, 데이터베이스", "tags": "", "date": "2022-10-06 06:00:25 +0000", "snippet": "What is schema? DB schema : a metadata that how data is organized within a relational database this is inclusive of logical constraints such as, table names, fields, data types, and the relationships between these entities Steps of modeling DB(modeling schema) External Schema step for defining (1)entity, (2)attribute, (3)relation result will be “entity relation diagram” which is ERD diagram Conceptual Schema step for Normalization DB normalization : split table for reducing duplication Normalization has 3 stages(i.e. 1,2,3) First Normal Form : split the table so that each column of the table has an atomic value(one value) Second Normal Form : split the table so that column’s data cannot be defined by parts of composite keys Thrid Normal Form : split the table to eliminate transitive dependencies (transitive dependencies : A -&gt; B -&gt; C = A -&gt; C) split table as A -&gt; B, B -&gt; C Here is example for each steps. Internal Schema step for how to express stored data items( ex) “id” = bigint ) CREATE TABLE \"accounts\" ( \"id\" bigserial PRIMARY KEY, \"owner\" varchar NOT NULL, \"balance\" bigint NOT NULL, \"currency\" varchar NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE TABLE \"entries\" ( \"id\" bigserial PRIMARY KEY, \"account_id\" bigint NOT NULL, \"amount\" bigint NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE TABLE \"transfers\" ( \"id\" bigserial PRIMARY KEY, \"from_account_id\" bigint NOT NULL, \"to_account_id\" bigint NOT NULL, \"amount\" bigint NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE INDEX ON \"accounts\" (\"owner\");CREATE INDEX ON \"entries\" (\"account_id\");CREATE INDEX ON \"transfers\" (\"from_account_id\");CREATE INDEX ON \"transfers\" (\"to_account_id\");CREATE INDEX ON \"transfers\" (\"from_account_id\", \"to_account_id\");COMMENT ON COLUMN \"entries\".\"amount\" IS 'it cannot be negative or positive';COMMENT ON COLUMN \"transfers\".\"amount\" IS 'it must be positive';ALTER TABLE \"entries\" ADD FOREIGN KEY (\"account_id\") REFERENCES \"accounts\" (\"id\");ALTER TABLE \"transfers\" ADD FOREIGN KEY (\"from_account_id\") REFERENCES \"accounts\" (\"id\");ALTER TABLE \"transfers\" ADD FOREIGN KEY (\"to_account_id\") REFERENCES \"accounts\" (\"id\");" }, { "title": "Database-JPA", "url": "/posts/JPA-1/", "categories": "서버, 데이터베이스", "tags": "", "date": "2022-10-06 06:00:25 +0000", "snippet": "I generally develop with Golang, not Java.To detaily describe the difference between how Java manage their Relational Database and Golang does, I will sumerize informations about JPA.What is JPA? JPA(Java Persistence API) is standard of Java ORM(Object-Relational Mapping) which iterally map Object to RDBMS. In case Member and Team " }, { "title": "Goroutine structure and behavior", "url": "/posts/thread-goroutine/", "categories": "서버, Golang", "tags": "thread, goroutine", "date": "2022-09-18 06:00:25 +0000", "snippet": "Golang use goroutine which is similar with thread, but little bit different. Goroutine can make you easy to use thread with concurrency. Also goroutine makes your program faster and lighter than when using original OS thread.Basically, goroutine and OS thread are same in how they share their resoruces(Heap, Data, Code section of memory) in process Code section Store whole binary code Data section Store Global variable, static variable, array, structure Heap section With malloc &amp; free or new &amp; delete, structure or variable are allocated &amp; return Stack section Store local variable, argument variable when function called Now, here is a difference of OS thread and goroutine.GoroutineGoroutine is M:N thread G (Goroutine) : Goroutine has Stack Pointer, Program Counter, DX M(Machine) : OS thread has pointer of P run G by P’s LRQ P(Processor) : logical processor has one LRQ allocate G in M in case that if G access to locked resources, G should wait until that resource is unlocked. P re-allocate G into left M, so that wait in other thread to show block is never happened LRQ(Local run queue) : Run Queue P pop G from its LRQ and allocate to M every P has its own LRQ GRQ(Global run queue) : Run Queue if every LRQ full, G is stored in GRQ Memory usage goroutine need 2KB stack as G only need SP, PC, DX thread(Java) need more than 1MB stack (i.e. 16 general purpose registers, PC, SP, segment registers etc.) Context Switch goroutine as goroutine is 2KB, context switch cost is very cheap Also as context switch is very easy, concurrency performance is higher than original OS thread thread(JAVA) context switch cost is expensive Java use OS thread as their abstract thread, concurrency performance is lower than References https://syntaxsugar.tistory.com/entry/GoGolang-Scheduler https://velog.io/@kineo2k/고루틴은-어떻게-스케줄링되는가 https://go.dev/src/runtime/HACKING" }, { "title": "How to design RESTful api server?", "url": "/posts/REST/", "categories": "서버, Golang", "tags": "RESTFUL", "date": "2022-09-11 06:00:25 +0000", "snippet": "Design rules for restful URI URI needs to show information of resource resource should be nouns rather than verbs, lowercase letters rather than uppercase letters ex) GET /Members/show/1 -&gt; GET /members/1 (verbs -&gt; nouns, uppercase -&gt; lowercase) Server’s directory name should be plural nouns Client’s storage name should be plural nouns ex) GET /Member/1 -&gt; GET /members/1 URI should not show act on resources ex) GET /members/delete/1 -&gt; DELETE /members/1 ex) GET /members/insert/2 -&gt; POST /members/2 don’t use _ in URI there is a possibility that you cannot see the proper information File extensions are not included in the URI ex) http://restapi.example.com/members/soccer/345/photo.jpg (X) ex) GET / members/soccer/345/photo HTTP/1.1 Host: restapi.example.com Accept: image/jpg (O) References https://gmlwjd9405.github.io/2018/09/21/rest-and-restful.html" }, { "title": "Advatage of MSA(Micro Service Architecture)", "url": "/posts/micro-service-architecture2/", "categories": "서버, MSA", "tags": "msa", "date": "2022-09-05 07:00:25 +0000", "snippet": "What is MSA? Monolithic’s Disadvantage Changes in some modules affect the entire application Lack of scalability Micro Service Architecture’s Advantage Scalability To update just one service, we don’t need to shut down the whole services Which means, we can focus on specific services which are important Fast Developement We can deploy each service seperately without dependency(which monolithic architecture cannot) You don’t have to wait until other team finish their developement Reusability We can re-use service easily which is directly related to scalability De-coupling As the coupling between services is reduced, team building becomes easier and more free This improves the ability to embrace new technologies Reduced maintenance difficulty. In the case of monolith, if an error occurs. We have to go through the whole project in debug. However in MSA, project is small thus can be easily debugged Micro Service Architecture’s Disadvantage Increased maintenance difficulty As I said above, Unit tests are completed quickly, but integration tests that go back and forth to other parts take a lot more time than monolithic ones Hard to control transactions In the case of an integrated service that use multiple services, it is difficult to handle transactions To overcome with this disadvantage, we can use Saga pattern To summarize MSA, if your service needs a lot of traffic, move a lot of money, need continous intergration, it will be best choise. ex) Portal site, shopping site etc.Beacuse MSA can gives you scalability, fast dev., Reduce server restart time by isolating failed services etc.References https://waspro.tistory.com/735 https://microservices.io/patterns/data/saga.html https://stackoverflow.com/questions/4127241/orchestration-vs-choreography https://waspro.tistory.com/429 https://wakestand.tistory.com/480" }, { "title": "Saga Architecture Pattern", "url": "/posts/micro-service-architecture1/", "categories": "서버, MSA", "tags": "saga", "date": "2022-09-04 07:00:25 +0000", "snippet": "What is Saga Pattern? SAGA pattern is a pattern that guarantees Atomicity in a distributed environment by exchanging events between microservices and sourcing a reward event to microservices that have completed previous work when an operation in a specific microservice fails. Why did this pattern arise? As rise of MSA(that user information is in DB_1, DB_2, DB_3 …), ACID transactions become impossible Need to manage transactions for distributed services Saga has 2 patterns Choreography-based Saga Orchestration-based Saga 1. Choreography-based Saga Pattern Each services manage their own local transaction And within service, it determine which service to send canceled transaction events to The Choreography-based Saga pattern manages local transactions within the service it has, and when the transaction ends, a completion event is issued. If there is a transaction to be performed next, an event is sent to the service that needs to perform the transaction, and the service receives the completion event and proceeds with the next operation. Do this sequentially. Events can then be delivered asynchronously through message queues such as Kafka. 2. Orchestration-based Saga Pattern Each services have their own local transaction However, composite service manages all transcations sequentially. And composite service determine which service to send canceled transaction events to In my opinion, It is necessary to avoid the transaction in a separate space by setting the DB separately. But if you can’t avoid that, also when you doing with your project with MSA, i do recommand choreography-based saga pattern. Since MSA is an architecture adopted for independent services, i think you should keep the key words which is independent. With Choreography, you can mange your services independently. As your services become larger and larger, Choreography makes your management of transaction easier than Orchestration.References https://waspro.tistory.com/735 https://microservices.io/patterns/data/saga.html https://stackoverflow.com/questions/4127241/orchestration-vs-choreography" }, { "title": "Golang vs Spring Native", "url": "/posts/golang-vs-spring(1)/", "categories": "서버, Golang", "tags": "golang", "date": "2022-08-30 07:00:25 +0000", "snippet": "As I’m a golang developer, there is a article that compare between golang and spring. Spring Boot Native vs Go: A performance comparisonThe article said “Golang is 133% faster than Spring”. They run various test to compare performance and it is clear to know the difference in performance! So, I would like to summarize that article in this post.The test ramps up 200 users during the first minute and then keeps constantly 200 users for 2 hours. Spring native server is constructed with RESTFUL architectures. GET /products: Returns the last 20 products. GET /products/{id}: Returns one single product for a given Id. POST /products: Saves a new product. The load test tries to simulate a common use case. Each user will perform the following actions: Get all the latest products. Save a new product. Retrieve the product saved on step 2. The test increments 200 users for the first minute and then maintains 200 users continuously for 2 hours.Test ResultsResponse Time Results Spring Native Golang Resource Usage Results Spring Native Golang To summarize this test results, Golang’s response time is 133% faster than Spring-Native’s. CPU and memory usage is also quite impressing. Golang use their memory less than 2%, while Spring Native use their memory more than 12%. Which means that the performance of Golang is better than Spring Native(although Goalgn doesn’t have Generics).References https://ignaciosuay.com/spring-boot-native-vs-go-a-performance-comparison/" }, { "title": "Golang vs Java(Part.2)", "url": "/posts/golang-vs-java2/", "categories": "서버, Golang", "tags": "golang", "date": "2022-08-25 06:00:25 +0000", "snippet": "In concurrency, Golang and Java has multiple different features.Difference in Memory Usage, Cost of con/de-construction &amp; Context SwitchGolang is known as a first-class support for concurrency. And it has standout ability to deal with multi-processing &amp; multi-threading. So why they said like that? There are many advantages like below. Goroutine is light-weight thread of Golang.Memory Usage goroutine(Golang) need only 2KB stack can add heap storage if you need. thread(java) need 1MB stack(500 times more than goroutine) guard page needed more thread, less heap available. Cost of construct/de-construct thread goroutine(Golang) as memory usage is very small, cost of this is also small. use thread pool thread(java) use thread pool Cost of Context Switch goroutine(Golang) low cost save/restore with only 3 registers PC(Program Counter), Stack Pointer, DX thread(java) high cost save/restore with 16 registers, etc. etc : PC, SP, Segment Register, FP coprocessor state, AVX register, MSR, etc. Feature of goroutineGo’s mechanism for hosting goroutines is an implementation of what’s called an M:N scheduler, which means it maps M green threads to N OS threads.References https://betterprogramming.pub/deep-dive-into-concurrency-of-go-93002344d37b" }, { "title": "Golang vs Java(Part.1)", "url": "/posts/golang-vs-java/", "categories": "서버, Golang", "tags": "golang", "date": "2022-08-23 06:00:25 +0000", "snippet": "Recently, I had a job interview in Ebay. They generally use Java for their development. And my programming language is Golang. So, they asked me that What is difference between Golang and Java?. As a matter of fact, I never thought about that. I would like to take this opportunity to summarize the differences between Golang and Java.Golang vs JavaWhat is Golang and Java?GolangGolang is an open-source language from Google that made in 2011.The syntax of Golang is close to “C” because the language’s compiler was built in C. But it is now written in Golang, allowing the language to remain self-hosted.Golang is a concurrent programming language designed for modern multicore processors, meaning it can do numerous tasks at once. It also features deferred garbage collection, which manages memory to run programs quickly.JavaJava is a statically typed general-purpose programming language. Sun Microsystems developed and released Java in 1995.Java used to be the language of choice for server-side applications, but it no longer holds that position. Despite that, hundreds of various applications around the world employ it. Various platforms, ranging from old legacy software on servers to modern data science and machine learning applications, use Java.There are ample pre-built modules and codes available because Java is famous among developers. These modules and developer availability make coding in Java easy.Java is versatile. It runs anywhere there’s a processor. It’s similar to a compiled language where the virtual machine breaks down the code into bytecode before compiling it.Golang vs Java   Golang Java Type Hierarchy Cannot Can(OOP structure) Performance High(Non-virtual machine) Low(JVM) Community Small Large Concurrency Powerful less-Powerful Garbage Collection Static Dynamic run on OS(so, performance high, light weighted) JVM() Both Java and Golang are powerful, popular, and useful. But still, they have significant differences. Java is 1. object-oriented(OOP), and has a 2. larger library and community. Golang is 1. better supports concurrency, 2. light-weight, fast. While Golang runs faster than Java, Java has more features and better support.Difference in usage of memory and Garbage CollectionOne advantage that we believe Go has over Java is that it gives you more control over memory layout. For example, a simple 2D graphics package might define:type Rect struct { X_top Int X_bottom Int Y_top Int Y_bottome Int}In Go, a Rect is just four integers contiguous in memory.In Java, 4 Integer separately allocated in memory as different objects. This requires more allocated objects, taking up more memory, and giving the garbage collector more to track and more to do. On the other hand, it does avoid ever needing to create a pointer to the middle of an object. Go gives you more control over memory layout, and you can use that control to reduce the load on the garbage collector. So, if you want to remove Rect from memory, just delete start address with four integers size. This avoids memory fragmentation and allocates memory at the end of the heap when new objects are created, allowing for fast memory allocation and deletion. I think that as memory is bigger and bigger, this trend would be fitted. Case of Go, they has GC in their executable file. Java, however, they has GC in JVM. Thus, Go is more light-weight and highly productable.So, in primitive Java, it has memory fragmentation issues. And how do Java overcomes memory fragmentation? To deal with these major disadvantages, they use COMPACTION. Compaction involves moving objects around memory and collect them into contiguous blocks a memory. This is not cheap. Not only does moving the blocks from one memory location to another cost CPU cycles, but updating every reference to these objects to point to the new locations also costs CPU cycles.Doing these updates requires freezing all threads. You cannot update references while they are being used. This typically causes Java programs to have complete freezes of several hundred milliseconds where objects get moved around, references updated and unused memory reclaimed.Adding ComplexityTo reduce these long pauses(Stop-The-World), Java uses what is called GENERATIONAL garbage collectors.The purpose of Generational Garbage Collection is that classifying objects by their age(times they survived in the GC, etc.) to improve GC efficiencyThere is a hypothesis that, in many applications, most newly allocated objects in memory die more frequently. Following this hypothesis, Generational GC can give you improvements in efficiency by eliminating the need to scan long-lived objects multiple times.Thus Java adopted this hypothesis and create their strategy with Generational GC. Frequently performing GC in the new object allocation area(Minor GC) Objects surviving multiple times(oldest) in the area GC are promoted and moved to the less frequent area(Major GC). The collector in the current Go distributions is reasonable but by no means state of the art. To be clear, Go’s garbage collector is certainly not as good as modern Java garbage collectors, but we believe it is easier in Go to write programs that don’t need as much garbage collection to begin with, so the net effect can still be that garbage collection is less of an issue in a Go program than in an equivalent Java program. In summary, if you want a Java class to hold ten different pieces of information of different types, you need to have ten different memory allocations and store ten pointers. In Go, you can use a struct with fields of the appropriate type, and use a single memory allocation. This saves the space required by the pointers and as an extra bonus also saves some time for the garbage collector. So, Java and Go have different concept in how they managing memory. I cannot exactly say which is better than the other. But one thing is sure that as Golang doesnt need virtual machine(only need machine code), Golang consumes less memory than Java. reference from https://www.quora.com/Why-is-Golangs-memory-usage-so-much-better-than-JavasHere is a memory usage when Java and Go compile or run their code. Compile Time Run time Go Source code + machine code Go compiler + machine code Java Source code + bytecode JVM JIT + JVM JIT + JVM Interpreter + JVM Interpreter javac compiler + machine code bytecode Even print “hello world” requires loading the entire JVM in memory.Static GC(Golang) vs Dynamic GC(Java)Golang do not re-arrange objects in heap when they remove some of objects. And that is one of the static GC’s characteristic. Thus, static GC has issue that memory fragmentations occurs. To handle that fragmentation issues, Golang use TCMalloc(Thread-Caching Malloc) for efficient memory management(for multi-thread programming)! What is TCMalloc? TCMalloc is Thread-Caching Malloc. TCMalloc reduces lock contention for multi-threaded programs. TCMalloc is faster than the glibc 2.3 malloc (available as a separate library called ptmalloc2) and other mallocs. ptmalloc2 takes approximately 300 nanoseconds to execute a malloc/free pair on a 2.8 GHz P4 (for small objects). Another benefit of TCMalloc is space-efficient representation of small objects. TCMalloc treates objects with size &lt;= 32K (“small” objects) differently from larger objects. Large objects are allocated directly from the central heap using a page-level allocator (a page is a 4K aligned region of memory). I.e., a large object is always page-aligned and occupies an integral number of pages.Java has Dynamic GC Also Java has adopted aging system(Generational garbage collection) for Garbage Collection. To summarize, Golang use TCMalloc to reduce lock contention for multi-threaded programs. So, as most server leverage multi-threaded programs, Golang can reduce server’s memory(Golang dont need virtual machine) and reduce lock contention which means Golang run faster than Java. – In Korean – Java는 JVM 위에서 돌아가기 때문에 실행하기 위해선 byte코드를 machine코드로 변환하는 과정이 필요하다. 반면 golang은 빌드과정에서 이미 machine코드로 변환했기 때문에 바로 동작할 수 있다. 빌드에 걸리는 시간도 GO 언어 내부적으로 최적화를 많이 해둬서 빠른편이다. GC는 각기 다른 측면이 있기에 무엇이 낫다고 정할 수는 없지만, (1) 경량 스레드를 지원하며, (2) 스레드 별 cashing을 적극적으로 지원하는 Golang이 multi-threading 환경에서는 더 낫다고 보여진다. 즉, 비동기를 위한 multi-threading 환경이 적용된 server는 자신의 퍼포먼스를 증가하기 위해 Golang을 선택하는 것은 타당하다고 생각된다.Difference in Concurrency and Simplicity Java uses OS thread to perform parallel execution of work through green threads(threads managed by language runtime). Golang uses OS thread through goroutines. So in the parallelism there can’t be significant difference between both implementations. But in concurrency there is huge difference. In java JVM map its green threads to OS threads while Golang brings mapping goroutines to OS threads into deep abstraction level through go scheduler(run in go-runtime, not virtual machine). Comparison of Concurrency programming with exampleJavapublic static void main(string[] args){ new FixedThreadPoolExecutor();}package taskExecutor;import java.util.concurrent.TimeUnit;public class Task implements Runnable { private String name; public Task(String name){ this.name = name; } public String getName(){ return name; } @Override public void run() { try{ Long duration = (long)(Math.random()*10); System.out.println(\"Doing a task during : \" + name); TimeUnit.SECONDS.sleep(duration); }catch (InterruptedException e){ e.printStackTrace(); } }package taskExecutor;import java.util.concurrent.Executors;import java.util.concurrent.ThreadPoolExecutor;public class FixedThreadPoolExecutor { public FixedThreadPoolExecutor(){ ThreadPoolExecutor executor = (ThreadPoolExecutor)Executors.newFixedThreadPool(4); for(int i=0; i&lt;10;i++){ Task task = new Task(\"Task\" + i); System.out.println(\"A new task has been added: \"+ task.getName()); executor.execute(task); } System.out.println(\"Maximum threads inside pool \" + executor.getMaximumPoolSize()); executor.shutdown(); }}Golang As you can see, Golang is more simple than Java. With Golang, we can easly access IPC with channel.package mainimport (\t\"runtime\"\t\"fmt\")func main() {\tnCPU:=runtime.NumCPU()\truntime.GOMAXPROCS(nCPU)\tconst maxNumber = 100\tch := make(chan int)\tdefer close(ch)\tgo Generate(ch)\tfor i:=0; i&lt;maxNumber;i++{\t\tprime := &lt;-ch\t\tfmt.Println(prime)\t\tch1:=make(chan int)\t\tgo Filter(ch,ch1,prime)\t\tch=ch1\t}}func Generate(ch chan&lt;- int){\tfor i:=2; ;i++ {\t\tch &lt;-i\t}}func Filter(in &lt;-chan int,out chan &lt;-int, prime int){\tfor{\t\ti:= &lt;-in\t\tif i%prime !=0{\t\t\tout &lt;- i\t\t}\t}} For short, Go has much better concurrency handling compared to Java. In Java the concurrence runs within autonomous threads, which are quite expensive to create and manage and can only communicate with each other via shared (volatile) variables or return values. It has historical reasons, because Java has been designed when very little concurrency in mind, since concurrency was not really possible on personal computers of that time and was added later on, but as an addition, not a core design feature. Go, on the other hand, has been designed with concurrency in mind. It inverts the approach: ‘Do not communicate by sharing memory; instead, share memory by communicating.’ To realize this it introduced channels, which are best understood as synchronized ques in Java word. One go subprocess is called goroutine and can write into a unbuffered channel and wait until the written value has been picked up by a different goroutines or fill its buffer when the channel is buffered. The whole synchronization is handled by go. It’s also worth mentioning that go routines are much cheaper to create and much more lightweight so you’ll find them all over the place/code. reference from https://www.quora.com/How-does-Java-concurrency-compare-to-GolangGo Runtime vs Java Virtual MachineDoes Go have a runtime?Go does have an extensive library, called the runtime, that is part of every Go program. The runtime library implements garbage collection, concurrency, stack management, and other critical features of the Go language. Although it is more central to the language, Go’s runtime is analogous to libc, the C library. Go has small runtime in their binary file when Java has runtime in JVM. It is important to understand, however, that Go’s runtime does not include a virtual machine, such as is provided by the Java runtime. Go programs are compiled ahead of time to native machine code (or JavaScript or WebAssembly, for some variant implementations). Thus, although the term is often used to describe the virtual environment in which a program runs, in Go the word “runtime” is just the name given to the library providing critical language services. reference from https://go.dev/doc/faq#runtimeReferences http://goog-perftools.sourceforge.net/doc/tcmalloc.html https://stackoverflow.com/questions/14322724/what-is-the-go-language-garbage-collection-approach-compared-to-others https://groups.google.com/g/golang-nuts/c/m7IFRYnI-L4 https://go.dev/doc/faq#runtime https://www.turing.com/blog/golang-vs-java-which-language-is-best/ https://velog.io/@kineo2k/Go-언어의-GC https://engineering.linecorp.com/ko/blog/go-gc/" }, { "title": "NPM vs YARN, what is your favorite?", "url": "/posts/npm-vs-yarn/", "categories": "서버, Yarn", "tags": "package install, npm, yarn", "date": "2022-06-08 21:00:25 +0000", "snippet": "To make portfolio in Web service, I complete task as below. Set portfolio frontend(React) and create Dockerfile Set Nginx configuration Set docker-compose. react-web(expose 3000), nginx(80:80) Associate AWS EC2 instance static IP with AWS-route-53 domainI installed react packages with npm. However the speed was so slow.So, while looking for alternatives, there was the following data that comparatively analyzed NPM and YARN. Yarn vs. NPM: How to Choose? It’s essential to consider the advantages and disadvantages of both NPM and Yarn when deciding which one to use. [Yarn] Advantages Supports parallel installation and Zero installs, both of which dramatically increase performance. Newer versions of Yarn offer a more secure form of version locking. Active user community. Disadvantages Yarn doesn’t work with Node.js versions older than version 5. Yarn has shown problems when trying to install native modules. [NPM] Advantages Easy to use, especially for developers used to the workflow of older versions. Local package installation is optimized to save hard drive space.The simple UI helps reduce development time. Disadvantages The online NPM registry can become unreliable in case of performance issues. This also means that NPM requires network access to install packages from the registry. Despite a series of improvements across different versions, there are still security vulnerabilities when installing packages. Command output can be difficult to read. In summary, in terms of speed, npm processes package installs as a sequence, whereas yarn installs in parallel, so yarn has high performance in speed. In particular, the higher the number of packages installed, the higher the performance of yarn.By using yarn, I can deploy my service within 5 minutes without cache. Thanks for parallel installation of yarn that is more faster for importing a large number of packages than npm.(npm took 5 hours to install with no-cache)Below is information about yarn.   Yarn / Yarn 2.0 NPM Zero Installs Uses a “.pnp.cjs” file to reinstall packages offline Unsupported Usage of Workspaces Supported Supported Speed Parallel installation Sequential installation Remote Scripts Supported using the command “yarn dlx” Supported using the command “npx” Plug’n’Play Uses a “.pnp.cjs” file Unsupported License Check Checks each package download while installing Unsupported Generating Lock Files Automatically created as “yarn.lock” Automatically created as “package-lock.json” Dependencies Installs using the “yarn” command Installs dependencies one by one through the “npm install” command So, here is my final version of react Dockerfile.FROM node:18-alpine3.16RUN mkdir /appWORKDIR /appENV PATH /app/node_modules/.bin:$PATHCOPY package.json /app/package.jsonCOPY yarn.lock /app/yarn.lockRUN yarn installCOPY ../.. /appCMD [\"/app/start.sh\"]COPY .env /app/.envCMD [\"npm\", \"start\"]/app/start.sh is a shell script which make .env file to change react default port.#!/bin/shset -eecho \"make .env with PORT=$PORT\"echo \"PORT=$PORT\" &gt; .envAlso, since I manage it with docker, I also have to take care of the image size. Here is the issue of how to reduce image size by 60%. How to reduce docker image size?Additionally, COPY .pnp,cjs in Dockerfile can utilize Zero installation of Yarn." }, { "title": "Relationship between MSA & Docker(proceeding)", "url": "/posts/msa-docker-kubernetes/", "categories": "서버, Docker", "tags": "msa", "date": "2022-05-30 06:00:25 +0000", "snippet": "MSA rises as cloud services and is more and more used. That’s because each service is independent and can be managed its cost efficiently. Also, even without the benefits of cost in cloud service, MSA has advantage of scalability, fast dev., increased team autonomy etc.As each service of MSA has their independent environments, (1) Consistent environmental management(Portability) is required. Docker solves this for you! VM : complicatefor example, just in case you want to upgrade the version of your program. With VM, you need to pull from registry, update all configuration files, reset environments, etc. So it is very hard to change or update your program Docker : easyDocker can simplify this process. Download image, run it! Also Docker can gives you more advantages. Docker can run your application (2) fast in Provisioning &amp; starting, (3) light-weight than VM. This is because Docker based on OS-level process isolation rather than hardware virtualization(VM). containers provide OS-level process isolation whereas virtual machines offer isolation at the hardware abstraction layer (i.e., hardware virtualization). So in IaaS use cases machine virtualization is an ideal fit, while containers are best suited for packaging/shipping portable and modular softwarereference : https://www.upguard.com/blog/docker-vs-vmware-how-do-they-stack-up for example, if you want to input ‘hellow world’ in your Golang program. VM first seperate hardware space in your Host and install GuestOS(which is very big size). Next install Golang compiler, run program, I/O execution. At this time, there is transition between GuestOS and HostOS by its I/O driver. Docker only install in container with bin/lib files that needed for executing program(unlike VM need whole OS which is very big size). That means Docker has light-weight. Also container and HostOS share some parts of kernel which don’t need I/O translation. Thus, speed of Docker is more faster than VM. Details in my posting : ‘Docker vs Virtual Machine’To summary, Docker has many advatages. But if you have many many containers, how can we manage all these containers?Here is two container orchestration technology options, Docker-compose and Kubernetes. They both can manage cpu-usage, memory-usage, etc.Docker-compose Docker Compose is a tool for defining and running multi-container Docker applications used in single host! I usally set my docker configuration with docker-compose. I use this in (1) portfolio project, (2) golang-backend project. My projects have features below. You can check docker-compose setting in my other posting. services: postgres: image: postgres:12-alpine environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=secret - POSTGRES_DB=simple_bank ports: - \"5432:5432\" # purpose to exposeing ports. Except this ports, you can use only inside services api: build: context: . dockerfile: Dockerfile ports: - \"8080:8080\" environment: - DB_SOURCE=postgresql://root:secret@postgres:5432/simple_bank?sslmode=disable depends_on: - postgres entrypoint: [\"/app/wait-for-it.sh\",\"postgres:5432\",\"--\",\"/app/start.sh\"] command: [\"/app/main\"]version: \"3.7\"services: nginx: restart: always container_name: nginx build: context: ./nginx dockerfile: Dockerfile ports: - \"80:80\" networks: - frontend client: container_name: client expose: - \"3000\" restart: \"on-failure\" environment: - PORT=3000 - NODE_ENV=development - CHOKIDAR_USEPOLLING=true build: context: ./client dockerfile: Dockerfile volumes: - \"./client/:/app\" - \"/app/node_modules\" stdin_open: true networks: - frontendnetworks: frontend: driver: bridgeKubernetes Kubernetes is a platform for managing containerized workloads and services, that facilitates both declarative configuration and automation if you run your service in single host, you don’t need to use kubernetes. But if you want to run your service in multiple-host and take leverage in automation, you can use Kubernetes for your convenience. Now, I will explain each components.Components Cluster Cluster is a set of Controll Plane and one or more Worker Node. Controll Plane also called Master Node. And it manages the Worker Nodes and the Pods in the cluster. API server entry point for REST/kubectl Scheduler schedules pods to worker nodes Controll Manager it manages and watches their current state of Worker Node, Pod etcd(key-value store) stores all of Kubernetes cluster data(cluster state and config) Worker Node:maintain running Pod and provide the Kubernetes runtime environment kubelet It makes sure that containers are running in a Pod and they are healthy Path between API server of Controll Plane kube-proxy manages IP translation and routing It facilitating Kubernetes networking services and load-balancing across all pods in a service Container runtime It pulls images from Container Registry and starts and stops containers Container Registry : can be Docker Hub, Amazon Elastic Container Registry(ECR), Google Container Registry(GCR) Types of yaml used in Kubernetes To run kubernetes, we need to set configuration files with yaml format. There are various type. Deployment, Service, Ingress, ClusterIssuer, etc.Deployment: A deployment type is responsible for keeping a set of pods running. Here is an example of deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: golang-backend-api-deployment labels: app: golang-backend-apispec: replicas: 2 selector: matchLabels: app: golang-backend-api template: metadata: labels: app: golang-backend-api spec: containers: - name: golang-backend-api image: ghkdqhrbals/simplebank:latest imagePullPolicy: Always ports: - containerPort: 8080 env: - name: DB_SOURCE value: postgresql://root:secret@postgres:5432/simple_bank?sslmode=disable apiVersion : set api version. Here is a organized API informationhttps://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html kind : this is a type of configuration metadata : store resource label, name spec : details of components replicas : set the number of pod selector : what will deployment want to replicate(find in template) template.spec.container :(1) find ghkdqhrbals/simplebank:latest Docker images from Docker Hub,(2) run container with name golang-backend-api,(3) set container port 8080 Service:A service is responsible for enabling network access to a set of pods. when you use only service, need to create pods passivelyapiVersion: v1kind: Servicemetadata: name: golang-backend-api-servicespec: type: ClusterIP #diff. LoadBalancer, etc. selector: app: golang-backend-api ports: - protocol: TCP # nodePort is external access port outside the cluster. But, as we set type as clusterIP, this setting isn't needed # nodePort: 30131 port: 80 # internal port targetPort: 8080 # forward port reference from https://matthewpalmer.net/kubernetes-app-developer/articles/service-kubernetes-example-tutorial.html spec.type : you may choose within ClusterIP or LoadBalancer or NodePort ClusterIP : The service is only accessible from within the Kubernetes cluster you can’t make requests to your Pod from outside the cluster NodePort : The service can handle requests that originate from outside the cluster LoadBalancer : The service becomes accessible externally through a cloud provider’s load balancer functionality In this case, (1) service get request from port:80 internally, (2) select pods with labeled golang-backend-api, (3) forward request to container port 8080 in golang-backend-api pod.IngressIngress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.apiVersion: networking.k8s.io/v1kind: IngressClassmetadata: name: nginxspec: controller: k8s.io/ingress-nginx---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: golang-backend-api-ingress annotations: cert-manager.io/cluster-issuer: letsencryptspec: ingressClassName: nginx rules: - host: \"api.hwangbogyumin.com\" http: paths: - pathType: Prefix # 443, 80 etc. -&gt; 80 if \"/\" prefix path: \"/\" backend: service: name: golang-backend-api-service port: number: 80 tls: - hosts: - api.hwangbogyumin.com secretName: hwangbogyumin-api.cert~On proceeding…~ I have been already finished this kubernetes works in AWS, not in on-premise. So, here i will set configuration in local environmentReferences https://www.upguard.com/blog/docker-vs-vmware-how-do-they-stack-up https://stackoverflow.com/questions/47536536/whats-the-difference-between-docker-compose-and-kubernetes https://github.com/compose-spec/compose-spec/blob/master/spec.md https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/What-is-Kubernetes-vs-Docker-Compose-How-these-DevOps-tools-compare https://medium.com/devops-mojo/kubernetes-architecture-overview-introduction-to-k8s-architecture-and-understanding-k8s-cluster-components-90e11eb34ccd https://matthewpalmer.net/kubernetes-app-developer/articles/service-kubernetes-example-tutorial.html https://kubernetes.io/docs/concepts/services-networking/ingress/" }, { "title": "RSA ecnryption", "url": "/posts/rsa/", "categories": "서버, 암호학", "tags": "token", "date": "2022-05-10 06:00:25 +0000", "snippet": "Generate A’s RSA public/private key A generate random prime number p,q = 53, 59 n = p*q = 3127 Φ(n) = (p-1)*(q-1) = 3016 A generate random number k = 2 A generate small public exponent e (must be odd number and not share factors with n) = 3 d(priv_key) = (k * Φ(n) + 1) / e pub_key = e, n = 3, 3127 priv_key = d = 2011 Notation m : plain text c(m) : cipher text e,n : public key n = p * q d : private key, = (k * Φ(n) + 1) / e which is e*d ≡ 1 mod n ( k(p-1)(q-1)+1 ) mod n ≡ ( 1 mod n + k(p-1)(q-1) mod n ) mod n e : odd number and not share factors with n, { gcd(e,n) = 1 } k : random number φ(n)≡ ∣ { m : 1 ≤ m ≤ n, gcd(m,n) = 1 } ∣ ( n ∈ N ) if n = prime_number_1 * prime_number_2, then φ(n) = φ(prime_number_1)*φ(prime_number_2) = (prime_number_1 - 1) * (prime_number_2 - 1) How can we get m(plain text) from c(m)(cipher text)?Remind that in RSA, c(m) = m^e (mod n). From now, I will say c(m) is c. We can simply power d to c and add mod n.c^d (mod n)= m^(e*d) (mod n)[1] d = (kΦ(n)+1)/e = (kΦ(pq)+1)/e (mod n) = (kΦ(q)Φ(q)+1)/e (mod n) = (k(p-1)(q-1)+1)/e (mod n)= m^(k(p-1)(q-1)+1) (mod n)[2] = [ (1)(m (mod n)) * (2)m^(k*(p-1)*(q-1))(mod n) ] (mod n)[3] (2-1)m^(k*(p-1)*(q-1))(mod n) = m^(k*(p-1)*(q-1))(mod pq)With chinese remainder theory, we should know pq (mod p), pq (mod q) x ≡ a (mod m) and x ≡ a(mod n) implies x ≡ a (mod mn) if m and n are two relative prime positive integers. in our case, x will be m^ed, a will be m (we want to show that m^ed = m mod n). (2-2)m^(k*(p-1)*(q-1)) (mod p) = 1 (mod p) (2-2)m^(k*(p-1)*(q-1)) (mod q) = 1 (mod q) Fermat’s little theorem Thus, m^(k*(p-1)*(q-1)) = 1 (mod pq), so (2) = 1 (mod n)Finally!!,[3] = (1) * (2) = ( m mod n * 1 mod n ) mod n= m mod n !!!!! Its a long journey to proving RSA encryption/decryption! # Final equation is m = c^d (mod n)So can attacker decrypt message or change cipher text that encrypted with RSA? The answer is YES! Here is an example. A,B are communicating each other and ‘Me’ wants to seize and decrpyt message. A send c = encrypt(m, B’s pub_key:n,e) to B B get m = decrypt(c, B’s priv_key:d) Here ‘Me’ seize c. and send c' = c * r^e(r:arbitrary value) to B. ‘Me’ get c'^d = (c * r^e)^d = (m*r)^(e*d) from B. Because B can not understand the message. Plz remind that, ed = (k * Φ(n) + 1) and message from B = (mr)^(ed), now with mod n, =&gt; m*r. we can get m! as we already know about r value.To prevent these attack, message is padded with arbitrary number. This is called OAEP. OAEP make same sequence of message with different paddings, so attacker cannot see the plain text(because it is salted!).Padding Scheme(RSA- OAEP) Optimal Asymmetric Encryption PaddingMGF : mask generating functionInput words(variable length) -&gt; Output string(desired length)ex) MGF1 Hash the label L using the chosen hash function Hash(L) L is an optional label to be associated with the message (the label is the empty string by default) Generate a padding string PS. length : k-mLen -2*hLen -2, value : 0x00 Concatenate! DB = Hash(L) || PS || 0x01 || m m is the message to be padded. However, OAEP has an vulnerability such as Padding Oracle Attack. In RSA-PKCSv1.5, it is seriously issued. PKCS#1 v1.5 padding should be replaced wherever possible!.Now secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption.Padding Scheme(RSA-PSS) probabilistic signature schemePadding Scheme3(RSAES-PKCS1.5)Encoding Pain Text = ‘HI’ Convert letters to numbers : H = 8 and I = 9 Encrypted Data c(m) = m^e (mod n) = 89^3 (mod 3127) = 1394 Decrypted Data m(c) = c^d (mod n) = c^2011 (mod 3127) = 89Notice! The padded message(in this case, 89) cannot be longer than the modulus(3127), which implies a strict maximum length on the raw message. 2 padding scheme : RSAES-OAEP, [RSAES-PKCS1-v1_5]References RSA-OAEP choosen cyphertext attack" }, { "title": "Cookie and Session, JWT(Json Web Token)", "url": "/posts/cookie-session-storage/", "categories": "CS, 네트워크", "tags": "token", "date": "2022-05-01 11:00:25 +0000", "snippet": "We generally use TOKEN for authentications. But we don’t actually know the details about token. Today I will talk about token, espectially JWT(json web token).First we should know about the cookie.CookieCookie is a database which is stored in client’s web browser by server. Server can store data up to 4KB(this is different among the browsers). To maintain the state(as http is stateless protocol), cookie can be used as a temporary state! Also cookie can reduce resource comsumption of server.This cookie data is key:value pair set. Each cookie data is stored according to its domain.SessionSession is a data that stored in server(while cookie is stored in client). This session is normally used for authentication. To simply show how the session works, I will give you an good example here. User wants to login and get the current balance User request(header:GET /login,body:ID, Password) to Server Server check main_DB whether if users’ID and Password is correct Server create | session_id | username | expiration | ... | data into server’s session_DB Server response with session_id to user User browser’s cookie get session_id and store into its cookie User request(header:GET /user/balance,Authorization:{session_id}) Server check session_DB whether if session_DB has session_id now Server load balance from main_DB Server response with balanceThis is how session works. However, as more users connect to server and request balance at the same time, the load on the session DB of the server increases. You can simply scale up session_DB, but the cost is very expensive.To reduce the load of session_DB when server service high-volume user environment, TOKEN is emerged! User request(header:GET /login,body:ID, Password) to Server Server check main_DB whether if users’ID and Password is correct Server create token data into server’s session_DB It is important that you should never include personal information like password into your token. Because, JWT is basically encrypted with based64, which means that everyone who have this token can look data inside. Signature : HASH(header, payload, {server_secret_key}) Server response with token to user User browser’s cookie get token and store into its cookie User request(header:GET /user/balance,Authorization:{token}) Server validate token —&gt; this is a difference between session management and token Server load balance from main_DB Server response with balanceThis is how token works in login example. Main difference between token and session is that token doesn’t need to maintain session_DBDisadvantage of tokenIt seems that token based authentication is very simple and low cost. However, there is some disadvantages. When if token is stolen? if token is stolen by the others, you cannot restrict authentication process. But with the session DB, you can easily stop authentication process by removing session_DB’s row. Also, session can inform you how many users with same id/pw are currently login. Thus, to prove endpoint, SSL/TLS is essential for http + token because they encrypt http &amp; token and by doing that preventing from man in the middle attack. some signing algorithm are vulnerable RSA PKCSv1.5 : padding oracle attack ECDSA : invaild curve attack Set “alg” header to “HS256” while server verify token with RSA public key(RS256) HASH(header, you must!! check the “alg” in header " }, { "title": "Banking backend server with Golang(Done)", "url": "/posts/golang-backend(3)/", "categories": "서버, Golang", "tags": "golang, backend", "date": "2022-04-23 06:00:25 +0000", "snippet": "ProgressFor my experience in golang-backend, I use below skills for develop my banking service. You can see the source code in my github. Banking backend serverSkills Skills Purposes RDS(Postgresql) storing User,accounts,balance info migration auto-migration sqlc generate Golang interface from sql git-workflow auto-deploy gin HTTP communication bcrypt safe way to store PW Viper auto-server configuration setting Gomock testing RDS Docker auto-setting env and ease for run Kubernetes auto-scaling and managing pods(docker images) JWT or PASETO TOKEN based authentication(reduce session weight) JQ conversion JSON to txt AWS get fixed public IP and for automation, maintenance How to automatically deploy our service?We use AWS with following serviceHow can we safely store user password in RDS?How can we handle multiple api request with asynchronous response?Update[v1.4.4] Set Kubernetes Cluster Set aws-ath.yaml to access AWS-EKS(with granted user) Set deployment.yaml to get image from AWS-ECR and run with 2 replica(pod) Set issuer.yaml to issue TLS certificate get certificate from ‘letsencrypt’ with domain ‘api.hwangbogyumin.com’(free) Set ingress.yaml with Nginx ingress controller request -&gt; api.hwangbogyumin.com api.hwangbogyumin.com -&gt; aws-route-53 my arn aws-route-53 my arn -&gt; nginx-ingress address nginx-ingress address -&gt; ingress-service(TLS) ingress-service -» server pods(1,2) Use AWS-Route-53 to create Domain &amp; Set Kubernetes Ingress-service podsUpdate[v1.4.3] Use Git Action for auto AWS docker image upload Set Configure AWS credentials Add AWS_ACCESS_KEY_ID, KEY in Github Repositry secrets AWS-IAM secrets:AWS_ACCESS_KEY_ID, AWS_ACCESS_KEY Launch deploy action Get secrets from Git and Access with token Login build images and Deploy to AWS-ECR ap-northeast-2 Add services(AWS-ECR, AWS-Secrets Manager, AWS-IAM, AWS-RDS) Secrets Manager for managing symmetric_key that can encrypt/decrypt Paseto Payload and RDS port, RDS root, key Set IAM(Identity and Access Management) for safe AWS resource access Set ECR(Elastic Container Registry) in ap-northeast-2 Set RDS(Relational Database Storage) in us-west-1, postgres12 JQ Get RDS informations and etc. from AWS secrets manager Transform AWS secrets format into JSON format using JQ Based on json data, set app.env with corresponding data Update[v1.4.2] Edit Dockerfile &amp; Docker-compose file Set shell script(wait-for-it.sh) to wait until postgres is readyDetail As we alpine image, ‘apk add bash’ needed Set shell script(start.sh) to migrate db up Edit Dockerfile to add needed files migrate, app.env, main(object file), pre-setting shell script(wait-for-it.sh, start.sh) Make docker-compose.yaml to specify services name and environment variables Update[v1.4.1] Add Token Authentication Middleware Set user.go/loginUser for create/verify TOKEN Set Route(createAccounts, transferMoney, etc.) Group that need authorization. Make authMiddleware for pre-check requests whether they have TOKEN for authorization Edit api/server.go Before get request, check and verify http header’s authorized part. If there is a TOKEN that server created, pass request to actual handler. If no TOKEN exists, abort session and send response. 위의 http통신은 TLS로 encrypt되었음을 가정한다. TLS Details TLS가 적용되지 않았으면 TOKEN가 탈취되었을 때, Server에 권한없이 RPC 통신하여 DB 탐색가능. Testcase정의 1. User ----- Login --&gt; Server [LoginParams] = username, password2. User &lt;---- TOKEN --- Server [TOKEN] = chacha20poly1305(nonce, Server's Key, AEAD, Payload{username, duration})3. User ----- CreateAccount --&gt; Server [Params] = currency, TOKEN4. User &lt;---- Account's Info --- Server [Account] = verifyToken(Server's Key, TOKEN) Update[v1.4.0] JWT(JSON Web Token)의 HMAC-SHA256(HS256) algorithm를 통한 payload+header ‘Encryption’ and ‘MAC’ 생성 Set secretKey as random 256 bits(As we use HS256, Key should be 256 bits) Temporary! Make CreateToken function(interface) ( [HEADER]:’alg:HS256,typ:jwt’, [PAYLOAD]:’id:string, name:string, expiredAt:time’, [SIGNATURE]:’HMAC([HEADER],[PAYLOAD]).TAG’ ) Make VerifyToken function(interface) Check HEADER, SIGNATURE, … Set test enviroments case Invalid Header algorithm, MAC failed, Expiration, etc. PASETO(Platform-Agnostic Security Tokens)의 chacha20Poly1305 algorithm를 통한 payload+header+nonce ‘Encryption’ and ‘MAC’ 생성 Set secretKey as random 256 bits(As we use chacha20Poly1305, Key should be 256 bits) Temporary! Make CreateToken function(interface) Make VerifyToken function(interface) Set test env. Update[v1.3.1] Set Testcase of managing User password Set api/user_test.go TestCreateUserAPI test function cases: “OK”, “InternalError”, “DuplicateUsername”, “InvalidUsername”, “InvalidEmail”, “TooShortPassword” Set Custom reply matcher(gomock) Update[v1.3.0] Use Bcrypt(Blowfish encryption algorithm) for safe storing user password(Detail) Set util/password.go using bcrypt which can randomly generate cost, salt to get hashed password with params Set util/password_test.go for testing Make api/user.go to set createUser handler Set routes(“/user”) for request from clients Update History Use Gin framework to communicate RPC(Details) Set router, routes Set various handler Get http request Use custom validator to check if it is a valid request. Binding JSON to STRUCT(request) Access Local Database -&gt; Execute transactions -&gt; Get results(all process can handle with error) Response Use Viper for auto configuration setting (Details) Set /app.env Set /util/config.go import configurations in /main.go Use Gomock to remove DB dependency from tests in service layer (Details) Use sqlc interface with all query functions to interface Edit /.bash_profile for PATH to go/bin(to using mockgen) Execute mockgen to generate mock functions Set APIs for testing(TestGetAccountAPI) " }, { "title": "REST, JSON RPC, and gRPC", "url": "/posts/REST-JSON-and-gRPC/", "categories": "서버, Golang", "tags": "RESTFUL, JSON RPC, gRPC", "date": "2022-03-19 06:00:25 +0000", "snippet": "From MA(Monolithic Architecture) to MSA(Micro Service Architecture) 옛날에는 모놀리식 아키텍처로 설계를 하였음으로 하나의 거대한 어플리케이션으로 제작되었었다. ex) 뱅킹 서비스 + UI + AD로직 + DB엑세스 로직 + … = Application 하지만 최근 다양한 서비스들을 유동적으로 제공하기 위한 MSA(Micro Service Architecture)를 채택하다보니 다양한 언어로 제작되어있다. ex) 뱅킹 서비스 = API(Application Programming Interface), UI 관리 = API, DB 엑세스 = API, API1+2+3 = Application ex) ID 관리 및 제어 서비스(C++) 서버, 뱅킹 서비스 서버(Golang), UI 관리 서버(Python), … 즉, 각각의 서비스가 독립적으로 동작하며, 팀 단위로 빌드/테스트/배포 가능. 그리고 이러한 서비스들은 서로 HTTP 통신을 수행함으로써 정보교환이 가능하다. HTTP는 API를 제공함으로써 서로 패킷데이터를 라우팅 및 처리하는데, 이러한 API는 여러가지 아키텍처형태로 제공가능하다(REST, JSON-RPC, gRPC).API Architectures(REST, JSON-RPC, gRPC)REST APIREST는 HTTP 프로토콜을 효과적으로 사용하기 위한 아키텍처이다. 그리고 이러한 REST 아키텍처로 제공되는 API를 REST API라 한다. REST API는 다음으로 구성되어있다. 자원 [URI] URI:Uniform Resource Identifier URL:Uniform Resource Location 행위 GET, POST, PUT, DELETE, … GET vs POST GET은 요청하는 데이터를 HTTP Header의 URI에 포함되어 전송된다. 예를 들어 구글에 오늘의 날씨를 검색하면, https://www.google.com/search?q=오늘의날씨 로 URL에 뜨는 것을 확인할 수 있다. 즉, 공격자가 Header의 자원을 스내핑하여 클라이언트가 어떠한 정보를 전송했는지 확인할 수 있어 보안적인 측면을 고려해보았을 때 적합하지 않다. POST는 요청하는 데이터를 HTTP BODY에 포함하여 전송한다. 보안적인 측면에서 딱히 GET방식과 낫다라고 표현할 수는 없다. 이도 마찬가지로 공격자가 스내핑하여 관찰할 수 있기 때문이다. 하지만, BODY는 Header와 별개로 따로 이전에 암호키를 교환하고, 암호화하여 전송할 수 있기에 보안 scalability가 존재한다. 즉, 암호화할 수 있어 보안적으로 선호되는 방식이다. 물론, TLS가 적용된다면 엔드포인트 보안이 설정됨으로 Man in the Middle와 같은 공격에 대해 GET이나 POST는 같은 보안수준을 제공할 수 있다. 표현 JSON, XML, TEXT, … REST 방식의 API 구현은 CRUD(Create/Read/Update/Delete)에 초점이 맞추어져있다. 이는 구체적인 동작을 나타내기에는 쉽지 않다. 다음의 예를 확인하자. 만일 클라이언트는 서버로부터 특정 유저의 ID를 가져오고 싶다라고 가정하자. 이를 위해서 클라이언트는 서버에게 다음의 형태로 HTTP를 전송한다. [자원] : /user, [행위] : GET, [표현] : JSON{“limit”,…} 클라이언트는 전체 유저목록을 반환 받고, 이 중 원하는 특정 유저의 ID 필드를 검색하여 가져간다. 이 때, 클라이언트가 반환받는 Payload가 상당히 커진다. 이는 세부적인 표현이 힘든 REST API가 가지는 일반적인 딜레마이다. 즉, REST형태로 API를 설계할 떄, 일반적인 CRUD 행위가 아닌 추가적인 행위가 필요할 때 클라이언트의 부담이 커지는 경향이 존재한다.JSON-RPC JSON-RPC는 REST보다 더욱 구체적인 API를 제공할 수 있다. 예로 JSON-RPC는 GetUser(String username)와 JSON형태의 데이터를 http body에 담아 전송함으로써 구체적인 표현이 가능하다. ex) BODY: {“jsonrpc”:”2.0”, “method”:”GetUser”, “username”:”Hwangbo Gyumin”} 따라서 JSON-RPC는 High performance이며, Payload가 작다. 반면 서버 마음대로 표준을 생성하기에 다음의 단점이 존재한다. 표준화를 할 수 없다. 실제 function이 노출되는 위험이 존재한다(이는 거꾸로 말하면 API를 노출시키기 좋다라는 것임). Examplepackage mainimport (\t\"log\"\t\"net/rpc\")// rpc clienttype Args struct{}func main() {\thostname := \"localhost\"\tport := \":1122\"\tvar reply string\targs := Args{}\tclient, err := rpc.DialHTTP(\"tcp\", hostname+port)\tif err != nil {\t\tlog.Fatal(\"dialing: \", err)\t}\t// Call normally takes service name.function name, args and\t// the address of the variable that hold the reply. Here we\t// have no args in the demo therefore we can pass the empty\t// args struct.\terr = client.Call(\"Attack.Stop\", args, &amp;reply)\tif err != nil {\t\tlog.Fatal(\"error\", err)\t}\t// log the result\tlog.Printf(\"%s\\n\", reply)}package mainimport (\t\"fmt\"\t\"log\"\t\"net\"\t\"net/http\"\t\"net/rpc\"\t\"sync\"\t\"time\")// an RPC server in Gotype Args struct{}type Attack struct {\tstop chan bool\tstart chan bool\tquit chan bool}var nodeID stringvar total_packet intvar N_value intvar time_duration time.Timevar attack_success boolfunc (a *Attack) Method_info() error {\tfor {\t\tselect {\t\tcase &lt;-a.start:\t\t\tfmt.Printf(\"Eclipse Attack start to %s\\n\", nodeID)\t\tcase &lt;-a.stop:\t\t\tfmt.Println(\"Stop attack\")\t\t\tfmt.Println(\"--------- Attack Results -------\")\t\t\tfmt.Printf(\"| Total Packet \\t\\t: %d\\t|\\n\", total_packet)\t\t\tfmt.Printf(\"| N value \\t\\t: %d\\t\\t|\\n\", N_value)\t\t\tfmt.Printf(\"| Time \\t\\t\\t: %s\\t|\\n\", time_duration)\t\t\tfmt.Printf(\"| Attack Success \\t: %t\\t\\t|\\n\", attack_success)\t\t\tfmt.Println(\"--------------------------------\")\t\tcase &lt;-a.quit:\t\t\tfmt.Println(\"Exit Attack\")\t\t\treturn nil\t\t}\t}}func (a *Attack) Start(args *Args, reply *string) error {\ta.start &lt;- true\t*reply = \"Start Attack Server\"\treturn nil}func (a *Attack) Stop(args *Args, reply *string) error {\ta.stop &lt;- true\t*reply = \"Stop Attack Server\"\treturn nil}func (a *Attack) Quit(args *Args, reply *string) error {\ta.quit &lt;- true\t*reply = \"Quit Attack Server\"\treturn nil}func main() {\tvar wg sync.WaitGroup\tattack := new(Attack)\trpc.Register(attack)\trpc.HandleHTTP()\tattack.start = make(chan bool)\tattack.stop = make(chan bool)\tattack.quit = make(chan bool)\tgo func() {\t\tdefer wg.Done()\t\tattack.Method_info()\t}()\t// set a port for the server\tport := \":1122\"\t// listen for requests on 1122\tlistener, err := net.Listen(\"tcp\", port)\tif err != nil {\t\tlog.Fatal(\"listen error: \", err)\t}\thttp.Serve(listener, nil)\twg.Wait()}gRPCJSON-RPC는 JSON 형태로 주고받지만, gRPC는 Protocol-Buffer 형태로 주고받는다. 차이점은 아래와 같다. 기능 gRPC JSON-RPC 계약 필수(.proto) 선택 사항(OpenAPI) 프로토콜 HTTP/2 HTTP Payload Protobuf(소형, 이진), JSON도 사용 가능 JSON(대형, 사람이 읽을 수 있음) 규범 엄격한 사양 느슨함. 모든 HTTP가 유효합니다. 스트리밍 클라이언트, 서버, 양방향 클라이언트, 서버 브라우저 지원 아니요(gRPC-웹 필요) 예 보안 전송(TLS) 전송(TLS) 클라이언트 코드 생성 예 OpenAPI + 타사 도구 gRPC는 MSA 및 Docker-based application들의 RPC 설계를 단순화 시킴으로써 충분히 사용가치가 뛰어나다." }, { "title": "Docker vs Virtual Machine", "url": "/posts/docker2/", "categories": "서버, Docker", "tags": "golang, backend", "date": "2022-03-13 05:00:25 +0000", "snippet": "Main difference between Docker and VM Size VM : large size To execute program, VM needs to have GuestOS which is very large size Docker : small size Docker needs only bin/lib files which is compact size Speed VM : slow For example, when I/O occurs, VM needs to transfer the I/O from the GuestOS into HostOS, and transform to HostOS I/O driver Docker : fast Docker container share its kernel with host OS, so doesn’t need to transform it lifecycle(Portability) VM : complicate for example, just in case you want to upgrade the version of your program. With VM, you need to pull from registry, update all configuration files, reset environments, etc. So it is very hard to change or update your program Docker : easy Docker can simplify this process. Download image, run it! Thus, with Docker, you can easily scale out your app. And this is a basic skill that lead you to make your service with Micro Architecture Service!" }, { "title": "Docker Setting", "url": "/posts/docker1/", "categories": "서버, Docker", "tags": "golang, backend", "date": "2022-03-10 05:00:25 +0000", "snippet": "DockerfileBelow is my Banking Server backend Dockerfile. Here, I’m going to explain each steps.# Build stageFROM golang:1.18.3-alpine3.16 AS builderWORKDIR /appCOPY ../.. .RUN go build -o main main.goRUN apk add curlRUN curl -L https://github.com/golang-migrate/migrate/releases/download/v4.15.2/migrate.linux-amd64.tar.gz | tar xvz# Run stageFROM alpine:3.16WORKDIR /appCOPY --from=builder /app/main .COPY --from=builder /app/migrate ./migrateRUN apk add --no-cache bashCOPY app.env .COPY start.sh .COPY wait-for-it.sh .COPY db/migration ./migrationEXPOSE 8080CMD [\"/app/main\"]ENTRYPOINT [\"/app/start.sh\"]Lets start from the beginning!Build Stage FROM golang:1.18.3-alpine3.16 AS builderFirst, alpine is small version of linux. They dont have bash, curl or other libraries. So for me, I need only golang environment and basic linux. Thus this compact version of linux fits to me. WORKDIR /app,COPY . .Set my local environment in /app. RUN go build -o main main.gorun go build -o main main.go in docker container terminal. This command help me to get a compiled version of excutable file:main RUN apk add curlAs I said before, in alpine version, there is no curl inside. So we need to install curl for download from github. RUN curl -L https://github.com/golang-migrate/migrate/releases/download/v4.15.2/migrate.linux-amd64.tar.gz | tar xvzI’m going to install golang-migrate to use migrate command tools for migration. So install and decompress files. the -L option means that redirect to location. The purpose of this build stage is that literally building basic environments. Now its Run Stage.Build Stage FROM alpine:3.16WORKDIR /appBasic Linux and Setting up. COPY --from=builder /app/main .In build stage, we will copy executable file main into /app COPY --from=builder /app/migrate ./migrateAlso we will copy migrates(.exe) which is generated from curl -L github.~~~. RUN apk add --no-cache bashalpine image doesn’t have bash, so I will install that. I will use bash for run shell scripts. COPY app.env .app.env contains server configuration like below. From this environment files, I use Viper library to automatically set configuration on server. But, only use Viper as local testing. DB_DRIVER=postgresDB_SOURCE=postgresql://root:secret@localhost:5432/simple_bank?sslmode=disableSERVER_ADDRESS=0.0.0.0:8080ACCESS_TOKEN_DURATION=15mTOKEN_SYMMETRIC_KEY=12345678901234567890123456789012 COPY start.sh .I use start.sh to run db migration to my database and start the app.#!/bin/shset -eecho \"run db migration\"/app/migrate -path /app/migration -database \"$DB_SOURCE\" -verbose upecho \"start the app\"exec \"$@\" COPY wait-for-it.sh .As I use two containers, Postgres and Backend-Service, my service needs to wait until Postgres container is ready. This shell script listen port 5432, so can decide whethere this port is ready. This file is long so you can check my github. COPY db/migration ./migrationstart.sh generate SQL queries for migration in db/migration, and I will copy these schemes into /migration EXPOSE 8080I will expose this service locally with port 8080. CMD [\"/app/main\"]Set /app/main as a default command. ENTRYPOINT [“/app/start.sh”]When this Dockerfile build &amp; run in container, /app/start.sh will be executed first. docker-compose Docker compose simplifies management by automatically building and executing services from multiple containers. Below is my setting for composing docker containers. like kubernetes services: postgres: image: postgres:12-alpine environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=secret - POSTGRES_DB=simple_bank ports: - \"5432:5432\" # purpose to exposeing ports. Except this ports, you can use only inside services api: build: context: . dockerfile: Dockerfile ports: - \"8080:8080\" environment: - DB_SOURCE=postgresql://root:secret@postgres:5432/simple_bank?sslmode=disable depends_on: - postgres entrypoint: [\"/app/wait-for-it.sh\",\"postgres:5432\",\"--\",\"/app/start.sh\"] command: [\"/app/main\"] services:Declare that we are going to setting containers. postgres:This service name &amp; container name. image: postgres:12-alpineWe now get image of postgres from dockerhub. environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=secret - POSTGRES_DB=simple_bankSetting user, password, db name. ports:Just for here, we will expose port outside. api, build, context: ., dockerfile: DockerfileWe will build image with Dockerfile in this folder. ports: - \"8080:8080\"Normally, backend server’s port doesn’t need to show their ports outside. Actually, It is critical when you expose Postgres ports outside. Because malicious users can access to server’s DB and take out all informations. Also it is very vulnerable to DoS/DDoS attack(especially low-rate DoS Attack). environment: - DB_SOURCE=~~~Setting Env. Instead of secret@localhost, set as secret@postgres so postgres service’s network will be fitted.Generate services using docker compose will seperate postgres service and api service as a different IP address like postgres=’172.16.0.2’,api=’172.16.0.3’. So secret@localhost means secret@172.16.0.3. But it should be secret@172.16.0.2. So rather set secret@172.16.0.2, you can just set a service name there. depends_on: - postgresapi service will need postgres service. So After postgres service is running, api service will run. entrypoint: [\"/app/wait-for-it.sh\",\"postgres:5432\",\"--\",\"/app/start.sh\"]First, run /app/wait-for-it.sh, so set listening port for postgres server.Second, set postgres:5432 as a argument of wait-for-it.sh.Lastly, all things ready, start start.sh Beforegyuminhwangbo@Gyuminui-MacBookPro simplebank % docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEAftergyuminhwangbo@Gyuminui-MacBookPro simplebank % docker compose up...gyuminhwangbo@Gyuminui-MacBookPro simplebank % docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEsimplebank_api latest 7da6148aa0f0 3 weeks ago 52.1MBgyuminhwangbo@Gyuminui-MacBookPro simplebank % docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESde0dfecaf757 simplebank_api \"/app/wait-for-it.sh…\" 47 seconds ago Up 45 seconds 0.0.0.0:8080-&gt;8080/tcp simplebank-api-15a9c72502355 postgres:12-alpine \"docker-entrypoint.s…\" 47 seconds ago Up 45 seconds 0.0.0.0:5432-&gt;5432/tcp simplebank-postgres-1gyuminhwangbo@Gyuminui-MacBookPro simplebank % docker network inspect simplebank_default[ { \"Name\": \"simplebank_default\", \"Id\": \"8e834d593f880a08051e480628d94b0c61c91964a0e7d76c1e63ae249140193f\", \"Created\": \"2022-07-23T08:03:57.59455131Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": null, \"Config\": [ { \"Subnet\": \"192.168.96.0/20\", \"Gateway\": \"192.168.96.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": { \"5a9c72502355f346543b02b0ba1a89563333f2c3d5450da25e68f6c8c32a14f9\": { \"Name\": \"simplebank-postgres-1\", \"EndpointID\": \"08fdd10da2e5d6ba2b5b9b7b69aebab0797c1e5fd2c24ce5a9a0f2be96aaa4e1\", \"MacAddress\": \"02:42:c0:a8:60:02\", \"IPv4Address\": \"192.168.96.2/20\", \"IPv6Address\": \"\" }, \"de0dfecaf75713dbc30595e21cf978472919d6bce9101acba36ab49708fa53cd\": { \"Name\": \"simplebank-api-1\", \"EndpointID\": \"b969560e4d29bcf7cbfbbaf67d524215704919ae5720ce40befbbe7ebc19fb1a\", \"MacAddress\": \"02:42:c0:a8:60:03\", \"IPv4Address\": \"192.168.96.3/20\", \"IPv6Address\": \"\" } }, \"Options\": {}, \"Labels\": { \"com.docker.compose.network\": \"default\", \"com.docker.compose.project\": \"simplebank\", \"com.docker.compose.version\": \"2.4.1\" } }]By sending HTTP request using Postman, results are below.Login Register ghkdqhrbals Register again with ghkdqhrbals and here is violations! Register Kim Login Kim Both ghkdqhrbals, Kim’s register informations are successfully stored in postgres database. " }, { "title": "Network-3:TLS 1.2 vs TLS 1.3", "url": "/posts/network(3)/", "categories": "CS, 네트워크", "tags": "CS information", "date": "2022-02-22 10:00:25 +0000", "snippet": "TLS 1.2 TLS encrpyt all your application data in packet. Here is a whole process of how server/client exhange their keys and how they encrypt packets.Before TLS, Server create CSR CSR contains Country Name, State or Province Name, Locality Name, Organization, Organization Unit, Common Name And next, Server send CSR to CA(Certificate Authority) A -&gt; Certificate Authority(Google, Amazon, etc.) CSR(Certificate Signing Request) : A’s pub_key + identity + sign(A’s priv_key,(A’s pub_key, identity)) Certificate Authority -&gt; A check A’s sign and sign with Certificate Authority’s priv_key A’s Certificate : CSR + sign(Certificate Authority’s priv_key, content1) A -&gt; B with A’s Certificate B verify A’s Certificate with Certificate Authority’s pub_key Here, Man in the Middle can not replace A’s pub_key with their pub_key. Now Server has certificate! And from now, they can open port HTTPS(443). To exchange server/client encryption key, TLS do handshakes. It is little bit different by its version. First TLS 1.2 version do their handshake as below.Initial Handshake(2-RTT) Client Hello Client send Client Hello to Server Client Hello : [Version, Nonce, Session ID, Cipher Suites, Compression Methods, etc.] Server Hello Server send Server Hello to Client Server Hello : [Version, Nonce, Session ID, Cipher Suites, Compression Methods, etc.] Certificate Server also send certificate to Client If Server need Client’s certificate, they reqeust. Client verify Server’s certificate with CA’s pubkey and etc. Server Key Exchange Server make ECDHE key pairs and send public key to Client with message type Server Key Exchange This key will be used to make ECDHE shared secret(symetric key). Certificate Request(optional) Server send Certificate Request to Client(if Server wants it) Server Hello Done Server inform Client that my handshake process is done! Certificate(optional) Client send certificate to Server Server verify Client’s certificate Client Key Exchange Cient make ECDHE key pairs and send public key to Server with message type Client Key Exchange Encryption Keys Calculation Server caculate PreMasterSecret(ECDHE shared secret) using Client’s public key. Client caculate PreMasterSecret(ECDHE shared secret) using Server’s public key. Server/Client caculate MasterSecret with PreMasterSecret, Nonce using HMAC. MasterSecret has Server/Client’s MAC key Server/Client’s symetric key Server/Client IV(Initial Vector) for CBC. Certificate Verify Client hash all handshake messages and sign, send to Server. Server verify sign and compare hash that is same as mine. Change Cipher Spec Client inform Server that from now, i will send all messages with encrpyted data. Finished(Encrypted Handshake Message) Client hash all handshake messages and encrypt with shared-key, send to Server. Change Cipher Spec Server inform Client that from now, i will send all messages with encrpyted data. Finished(Encrypted Handshake Message) Server hash all handshake messages and encrypt with shared-key, send to Client. TLS 1.3 Add 0-RTT, 1-RTT for handshake. Remove key exchange methods that dont support forward secrecy.Initial Handshake(1-RTT) Client Hello Same with TLS 1.2, but Client make ECDHE key pairs and send public key to Server with message type Client Hello Server Hello ServerHello + Server Key Exchange + Certificate Request + Certificate + Finished = encrypt(TLS1.3 Server Hello)” Finished(Certificate + CertificateVerify + Finished=encrypt(TLS1.3 ClientHello)+ “Application Data” ) Certificate + CertificateVerify + Finished = encrypt(TLS1.3 ClientHello) Here, Client can send add Application data(which is supported from TLS 1.3 1-RTT) TCP+HTTPS+DNS RTT Comparison(TLS 1.2 vs TLS 1.3)To summary, Here is table for comparing RTT with TLS 1.2 and TLS 1.3. is initial or resumption? TLS 1.2 TLS 1.3 TLS 1.3 + 0-RTT New connection 3 RTT(TCP:1,TLS:2) 2 RTT(TCP:1,TLS:1) + DNS 3 RTT + DNS Resume connection 3 RTT(TCP:1,TLS:2)+DNS 2 RTT(TCP:1,TLS:1) + DNS 2 RTT + DNS " }, { "title": "[ERROR] Yarn install error", "url": "/posts/error1/", "categories": "INFO/WARNING/ERROR, ERROR", "tags": "error", "date": "2022-02-11 06:00:25 +0000", "snippet": " Case 1 This means that you dont have leftover disk space. So run docker docker system prune all stopped containers all networks not used by at least one container all dangling images all dangling build cache Case 2 Also, it can be a timeout problem that yarn struggling with installing so many packages. And inside yarn certain timeout is predefined. To change that defined timeout, you can write as below yarn install --network-timeout 1000000 that 1000000 is millisecond " }, { "title": "What can actually Nginx handle?", "url": "/posts/nginx1/", "categories": "서버, Nginx", "tags": "nginx", "date": "2022-02-10 06:00:25 +0000", "snippet": "My portfolio site use nginx for reverse proxy. It doesn’t need to have nginx in frontend, because the main purpose of reverse proxy is to hide the backend port(or load-balance). But I made it for the purpose of getting used to nginx....http {\t... upstream docker-client { server client:3000; } server { listen 80; # server_name portfolio.hwangbogyumin.com; server_name localhost; # Frontend React Page location / { proxy_pass http://docker-client; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }\t\t\t\t... } ...}Here is a several main functions of Nginx.1. Request routing With location {path_from}, you can route to other url. http://localhost –&gt; http://client:3000 This client should be docker container name!gyuminhwangbo@Gyuminui-MacBookPro client % docker ps -aCONTAINER ID IMAGE ... PORTS NAMES363ad79372d0 react-n ... 0.0.0.0:80-&gt;80/tcp nginx4264e85ea71c react-c ... 3000/tcp \"client\"gyuminhwangbo@Gyuminui-MacBookPro client % docker network inspect react-portfolio-website_frontend[\t{\t\t\"Name\": \"react-portfolio-website_frontend\",\t\t...\t\t\"Containers\": {\t\t\t\"a387532db6d518ea882c62d985a6b15c1f49f490bc40b36ebb8a525cedffdc1d\": {\t\t\t\t\"Name\": \"client\",\t\t\t\t...\t\t\t\t\"IPv4Address\": \"172.19.0.3/16\",\t\t\t\t...\t\t\t},\t\t\t...\t\t},\t\t...\t}]As IPv4Address is definded by Docker, it has dynamic IP address. Thus nginx get this IP address by just saying client to follow dynamic IP address.So when User request through http://localhost, nginx get this request and pass to http://client:3000(front-end).2. Load-balancingupstream backend_servers { server server_1:4000; server server_2:4001; } server { listen 80; location / { proxy_pass http://backend_servers; }} Line 1~4 : define server pool called backend_servers with server_1:4000 and server_2:4001gyuminhwangbo@Gyuminui-MacBookPro client % docker ps -aCONTAINER ID IMAGE ... PORTS NAMES363ad79372d0 react-n ... 4000/tcp server_14264e85ea71c react-c ... 4001/tcp server_2 Line 5~10 : pass http request to backend_servers poolHere you can also set load-balancing with weight.upstream backend_servers { server server_1:4000 weight=5 max_fails=10 fail_timeout=60s; server server_2:4001 weight=2 max_fails=10 fail_timeout=60s; } server { listen 80; location / { proxy_pass http://backend_servers; }} Line 2: server_1:server_2 = 5:2 is percentage of every reqeust pass. And max_fails,fail_timeout is if 10 times request fails occured, shutdown according server until 60 second passed.After setting load-balancer and others, you can also set sticky session.upstream backend_servers { server server_1:4000 ...; server server_2:4001 ...;\tsticky route $route_cookie $route_uri; } server { listen 80; location / { proxy_pass http://backend_servers; }}What is Sticky Session?First request/response server will be handling next request. Advantage Minimized data exchange : When using sticky sessions, servers within your network don’t need to exchange session data, a costly process when done on scale. ex) If you login with A server. And by server’s load-balancing, your next reqeust is pass to B server. If so, B server dont have information that you are already logged in. Here Sticky Session in load-balancer makes you to go not B server, but go A server for the next request. B server has same function as A server. This can be replaced with TOKEN, cookie. RAM cache utilization : Sticky sessions allow for more effective utilization of your application’s RAM cache, resulting in better responsiveness. Disadvantage Reduce performance of Load-balancer : Initial purpose of load-balancing is that make your server to have more scalability and balance the traffic. However, their can be unbalance between A server and B server’s traffic. Because not all the traffic is distributed, but only the initial traffic does. Lack of countermeasure when Session Failed : When if A server failed and A server’s session is maintain by only A server, every connection is loss. As I said before, sticky session comes up to handle how the servers can sharing their information. And by setting client:server one by one, server can maintain user’s information with stateful. However, pros and cons exist. To handle that issue, token-based methods are emerged.More information about JWT here, https://ghkdqhrbals.github.io/posts/json-web-token/upstream backend_servers { server server_1:4000 ...; server server_2:4001 ...;} server { listen 80; location / { \t\t\tauth_jwt on; \tauth_jwt_key_file /etc/nginx/api_secret.jwt; # Secret Key of Server(symmetric key)\t\t\t... \tproxy_pass http://backend_servers; }} To summary, using API Gateway(nginx), you can do balancing the traffics, setting sticky sessions, verify token(jwt), etc.References https://nginx.org/en/docs/http/ngx_http_auth_jwt_module.html https://browndwarf.tistory.com/83" }, { "title": "Database-Transaction", "url": "/posts/DB-transcation/", "categories": "CS, 데이터베이스", "tags": "database, golang, postgres", "date": "2022-02-08 06:00:25 +0000", "snippet": "What is Database Transaction? It refers to the unit of work performed by changing the state of the database. Simply put, it means accessing the database using the following query(SELECT, UPDATE, DELETE, ALTER, etc.). Transaction can be figured as multiple queries. Why do we need transaction? To provide reliable and consistant unit of work, even if system fails. To provide isolation between programs that access to database at same time(concurrently).ACID(Atomicity, Consistency, Isolation, Durability)Transaction has 4 characters. Atomicity : Whether all transactions are reflected in the database or not at all(all or none). Consistency : The result of transaction processing should always be consistent. When we access to value in database, even if that value is updated, the operation must be performed with the value initially referenced. Isolation : If one transaction perform calculation in specific value, other transaction can not access to that value. Durability : When transaction is done successfully, results must be reflected to database. Transaction use callback and rollback to maintain 4 characters.Here is a example how we can use transaction in golang(Querier is made by sqlc).type Store interface {\tQuerier\tTransferTx(ctx context.Context, arg TransferTxParams) (TransferTxResult, error)}// 쿼리들을 저장type SQLStore struct {\t*Queries //앞의 Queries 생략하면 *Queries에서 가져와 만들어짐.\tdb *sql.DB}// 만들어진 Store 주소반환func NewStore(db *sql.DB) Store {\treturn &amp;SQLStore{\t\tdb: db,\t\tQueries: New(db),\t}}// 쿼리문을 실행한다. Beigin, Rollback조건, commit로 최종실행 확인func (store *SQLStore) execTx(ctx context.Context, fn func(*Queries) error) error {\ttx, err := store.db.BeginTx(ctx, nil) // BEGIN\tif err != nil {\t\treturn err\t}\tq := New(tx) //\terr = fn(q) // EXECUTE\tif err != nil { // ROLLBACK\t\tif rbErr := tx.Rollback(); rbErr != nil {\t\t\treturn fmt.Errorf(\"tx err: %v, rb err:%v\", err, rbErr)\t\t}\t\treturn err\t}\treturn tx.Commit() // COMMIT}" }, { "title": "How can we generate golang interface from SQL queries?", "url": "/posts/golang-backend(1)/", "categories": "서버, Golang", "tags": "golang, backend", "date": "2022-02-01 21:00:25 +0000", "snippet": "Database designFirst I’m going to write DB scheme using dbdiagram. We can make general sql code as below.Table users as U{ username varchar [pk] hased_password varchar [not null] full_name varchar [not null] email varchar [unique, not null] password_change_at timestamptz [not null,default:'0001-01-01 00:00:00Z'] created_at timestamptz [not null, default: `now()`]}Table accounts as A { id bigserial [pk] owner varchar [ref: &gt; U.username ,not null] balance bigint [not null] currency varchar [not null] created_at timestamptz [not null, default: `now()`] Indexes { owner (owner, currency) [unique] }}Table entries { id bigserial [pk] account_id bigint [ref: &gt; A.id, not null] amount bigint [not null, note: 'can be negative or positive'] created_at timestamptz [not null, default: `now()`] Indexes { account_id }}Table transfers { id bigserial [pk] from_account_id bigint [ref: &gt; A.id, not null] to_account_id bigint [ref: &gt; A.id, not null] amount bigint [not null, note: 'must be positive'] created_at timestamptz [not null, default: `now()`] Indexes { from_account_id to_account_id (from_account_id, to_account_id) }}Now using dbdiagram, we can change sql code above into postgresql code.CREATE TABLE \"users\" ( \"username\" varchar PRIMARY KEY, \"hased_password\" varchar NOT NULL, \"full_name\" varchar NOT NULL, \"email\" varchar UNIQUE NOT NULL, \"password_change_at\" timestamptz NOT NULL DEFAULT '0001-01-01 00:00:00Z', \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE TABLE \"accounts\" ( \"id\" bigserial PRIMARY KEY, \"owner\" varchar NOT NULL, \"balance\" bigint NOT NULL, \"currency\" varchar NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE TABLE \"entries\" ( \"id\" bigserial PRIMARY KEY, \"account_id\" bigint NOT NULL, \"amount\" bigint NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE TABLE \"transfers\" ( \"id\" bigserial PRIMARY KEY, \"from_account_id\" bigint NOT NULL, \"to_account_id\" bigint NOT NULL, \"amount\" bigint NOT NULL, \"created_at\" timestamptz NOT NULL DEFAULT (now()));CREATE INDEX ON \"accounts\" (\"owner\");CREATE UNIQUE INDEX ON \"accounts\" (\"owner\", \"currency\");CREATE INDEX ON \"entries\" (\"account_id\");CREATE INDEX ON \"transfers\" (\"from_account_id\");CREATE INDEX ON \"transfers\" (\"to_account_id\");CREATE INDEX ON \"transfers\" (\"from_account_id\", \"to_account_id\");COMMENT ON COLUMN \"entries\".\"amount\" IS 'can be negative or positive';COMMENT ON COLUMN \"transfers\".\"amount\" IS 'must be positive';ALTER TABLE \"accounts\" ADD FOREIGN KEY (\"owner\") REFERENCES \"users\" (\"username\");ALTER TABLE \"entries\" ADD FOREIGN KEY (\"account_id\") REFERENCES \"accounts\" (\"id\");ALTER TABLE \"transfers\" ADD FOREIGN KEY (\"from_account_id\") REFERENCES \"accounts\" (\"id\");ALTER TABLE \"transfers\" ADD FOREIGN KEY (\"to_account_id\") REFERENCES \"accounts\" (\"id\");With erdCloud, we can simply show ERD(Entity Relationship Diagram) in this way.So we got SQL code for migrate to our postgres databse. Now, we will transform sql queries into golang interface using sqlc.I will give one example of accounts.sql-- name: CreateAccount :oneINSERT INTO accounts ( owner, balance, currency) VALUES ( $1, $2, $3) RETURNING *;-- name: GetAccount :oneSELECT * FROM accountsWHERE id = $1 LIMIT 1;-- name: GetAccountForUpdate :oneSELECT * FROM accountsWHERE id = $1 LIMIT 1FOR NO KEY UPDATE;-- name: ListAccount :manySELECT * FROM accountsWHERE owner = $1ORDER BY idLimit $2OFFSET $3;-- name: UpdateAccount :oneUPDATE accountsSET balance = $2WHERE id = $1RETURNING *;-- name: DeleteAccount :execDELETE FROM accounts WHERE id = $1;-- name: AddAccountBalance :oneUPDATE accountsSET balance = balance + sqlc.arg(amount)WHERE id = sqlc.arg(id)RETURNING *;To generate interface, we should declare method name and return type like this -- name: [Method Name] :[return columns]. And SQL queries abvoce turn into Interface as below.// Code generated by sqlc. DO NOT EDIT.// versions:// sqlc v1.13.0// source: account.sqlpackage dbimport (\t\"context\")const addAccountBalance = `-- name: AddAccountBalance :oneUPDATE accountsSET balance = balance + $1WHERE id = $2RETURNING id, owner, balance, currency, created_at`type AddAccountBalanceParams struct {\tAmount int64 `json:\"amount\"`\tID int64 `json:\"id\"`}func (q *Queries) AddAccountBalance(ctx context.Context, arg AddAccountBalanceParams) (Accounts, error) {\trow := q.db.QueryRowContext(ctx, addAccountBalance, arg.Amount, arg.ID)\tvar i Accounts\terr := row.Scan(\t\t&amp;i.ID,\t\t&amp;i.Owner,\t\t&amp;i.Balance,\t\t&amp;i.Currency,\t\t&amp;i.CreatedAt,\t)\treturn i, err}const createAccount = `-- name: CreateAccount :oneINSERT INTO accounts ( owner, balance, currency) VALUES ( $1, $2, $3) RETURNING id, owner, balance, currency, created_at`type CreateAccountParams struct {\tOwner string `json:\"owner\"`\tBalance int64 `json:\"balance\"`\tCurrency string `json:\"currency\"`}func (q *Queries) CreateAccount(ctx context.Context, arg CreateAccountParams) (Accounts, error) {\trow := q.db.QueryRowContext(ctx, createAccount, arg.Owner, arg.Balance, arg.Currency)\tvar i Accounts\terr := row.Scan(\t\t&amp;i.ID,\t\t&amp;i.Owner,\t\t&amp;i.Balance,\t\t&amp;i.Currency,\t\t&amp;i.CreatedAt,\t)\treturn i, err}...}Thus, when we want to use those interface, just call it!Next will talk about how to connect with RDS(Postgresql)." }, { "title": "OS-7(정리 in Korean)", "url": "/posts/OS-7/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-29 06:00:25 +0000", "snippet": " Process vs Thread In Concept 프로세스 : 디스크에 있는 프로그램이 실행되기 위하여 메모리에 올라왔을 때의 형태 스레드 : 프로세스의 실제 CPU 실행단위 In Structure 프로세스 : PCB(PID, State, PC, Register, memory limits, file location, etc) + Code + Data + Heap + User Stack + Kernel Stack 프로세스의 PCB, Kernel Stack은 메모리의 커널 영역에 저장되며, Code, Data, Heap, User Stack은 유저 영역에 저장된다. Code Data Heap User Stack 프로그램 바이너리 코드 전역변수 + 정적변수 동적 메모리 할당된 변수 로컬 변수 스레드 : TCB(PC, Register) + Kernel Stack + User Stack, 프로세스의 Code, Data, Heap을 공유한다 Kernel Stack : 스레드가 커널안에서 명령어를 수행할 때 커널에서 사용되는 별도의 스택 Multi-thread vs Multi-process 하나의 어플리케이션 내에서 동시성/병렬성을 가져야 할 때, 어느 방법이 좋을까? 멀티 스레드 장점 : Context Switch cost가 낮다 프로세스를 생성하거나 Context switching 하는 작업은 너무 무겁고 잦으면 성능 저하가 발생하는데, 스레드를 생성하거나 switching 하는 것은 그에 비해 가볍다. 멀티 프로세스 장점 : Critical Section설정이 쉽다 멀티 스레드는 프로세스의 Code, Data, Heap section을 공유한다. 이 때 Critical section이 공유 자원에 존재할 시, 구현관점(mutex, semaphore, monitor)에서 이를 방지해야하기에 까다롭다. 하지만 멀티 프로세스는 서로 공유하는 자원이 없기에 Critical section또한 없음으로 구현이 쉽다 CPU Scheduler(Short-term scheduler)하나의 프로세스만 실행할 수 없으니 이를 선점/비선점 방법으로 번갈아 가면서 실행시키기 위한 scheduler 선점 scheduler : 프로세스 중지 후 변경가능 RR(Round Robin) : CPU 선점중인 프로세스를 time_slice가 지나면 Ready큐의 끝으로 다시 넣음. Ready큐에서 가져오는것은 FCFS을 따름. 장점 : 대기시간이 짧아짐. n 개의 프로세스가 Ready큐에 있고 할당시간이 q인 경우 각 프로세스는 q 단위로 CPU 시간의 1/n 을 얻는다. 즉, 어떤 프로세스도 (n-1)q이상의 대기시간을 가지지 않음 SRTF(Shortest Remaining Time First) : CPU가 선점중일 때, Reay큐에 더 짧은 시간이 남은 프로세스가 추가되면 현재 프로세스 중단 및 해당 프로세스 실행 문제점 : starvation(+CPU 수행시간 측정 불가능) 비선점 scheduler : 프로세스 중지 불가능, 하나의 프로세스가 CPU를 선점하면 끝까지 수행하여야함 FCFS(First Come Frist Served) : Ready큐에 도착한 순서대로 CPU 할당 문제점 : convoy effect(수행시간이 긴 프로세스가 먼저 도착하면 나머지 프로세스들의 대기시간, 반환시간이 급증함) SJF(Shortest Job First) : Ready큐에 가장 짧은 수행시간을 가진 프로세스 먼저 CPU 할당 문제점 : starvation(조금 짧은 프로세스들이 상당수라면 대기시간, 반환시간이 기약없음) Scheduler Long-term scheduler : Job큐에서 Ready큐로 적절하게 프로세스 배치 사분할 시스템(Window, Unix)에서는 Job큐가 따로 존재하지 않고 바로 Ready큐에 적재하기에 장기 스케쥴러는 존재하지 않는다. 즉, 모든 프로세스를 Ready큐에 배치함. Short-term scheduler(CPU scheduler) : CPU가 Ready큐의 어떤 프로세스를 실행하게 할지 스케쥴링 Medium-term scheduler : 메모리 부족일 때, Disk로 “PCB, Code, Data, Heap ,etc.” Swapping Synchronous vs Asynchronous일반적으로 동기와 비동기의 차이는 메소드를 실행시킴과 동시에 반환 값이 기대되는 경우를 동기 라고 표현하고 그렇지 않은 경우에 대해서 비동기 라고 표현 동기 비동기 Critical Section : 둘 이상의 프로세스가 동시에 접근해서는 안되는 공유 자원 or 구역 3 things must be satisfied Mutual Exclusion : CS에는 하나의 스레드만 점유할 수 있음 Bounded Waiting : 프로세스를 무한정 기다리게 할 수 없음 Progress : CS를 사용하고 있는 프로세스가 없다면, 누구나 사용할 수 있음 Solution Mutex Lock : 하나의 프로세스/스레드만 CS에 접근가능하도록 들어가기 전에 Lock하고 들어감. 이후 나올때 release 단점 : 다중처리기 환경에서는 시간적인 효율성 측면에서 적용할 수 없다 Semaphores : 여러개의 프로세스/스레드가 CS에 접근가능하도록 자원개수를 카운팅함. 즉, 다중 Mutex lock 단점 : Busy Waiting Busy Waiting : 프로세스/스레드는 진입코드를 반복실행하기에 불필요한 CPU자원이 낭비되는 문제점 Monitoring : 고급 언어의 설계 구조물로서, 개발자의 코드를 상호배제 하게끔 만든 추상화된 데이터 형태 Memory management strategy Background of memory management Paging Segmentation Virtual memory Background Virtual memory usage Demand Paging : TLB, PT Page replacement algorithm Locality of Cache Locality Caching line 고아/좀비 프로세스 고아 프로세스 : 자식 프로세스보다 부모 프로세스가 먼저 죽는 경우, 자식 프로세스를 고아 프로세스라고 칭한다. 이 때, 자식 프로세스는 init 프로세스(PID==1)가 새로운 부모 프로세스로 된다. 좀비 프로세스 : 자식 프로세스가 exit syscall를 통해 실행을 마쳤지만, 부모 프로세스가 wait syscall를 통해 자식 프로세스의 상태를 반환받지 못했을 때 자식 프로세스를 좀비 프로세스라고 칭한다. cgroup(Control group) cgroups은 프로세스들의 자원의 사용(CPU, 메모리, 디스크 입출력, 네트워크 등)을 제한하고 격리시키는 리눅스 커널 기능이다. SLB/GSLB SLB(Server Load-Balancing)는 말 그대로 서버들에게 로드밸런싱으로 부하를 줄여주는 기법이다(L4이상) Load Balancing 기법은 다음과 같은 기법들이 존재 Round Robin : Round Robin 알고리즘을 바탕으로 Server를 선택 Least Connection : 현재 Connection의 개수가 가장 적은 Server를 선택 RTT : RTT(Round Trip Time)이 가장 작은 Server를 선택 Priority : 우선순위가 높은 Server 선택한다. 만약 우선순위가 높은 서버의 상태가 비정상이라면, 그 다음 우선순위가 높은 서버를 선택 Session 유지가 중요 포인트임, sticky session이나 session clustering으로 일관된 session 관리가 필요함 Sticky session : 클라이언트가 초기에 매칭된 서버와만 이후 모든 패킷들을 라우팅 시킴으로써 session 유지하는 기법 sticky session의 단점은 해당 매칭된 서버가 다운될 떄, session 복구가 불가능함 Session clustering : session 서버를 하나 만들어서 이를 통해 일관된 session을 유지하도록 하는 기법 session clustering의 단점은 부하 분산이 제대로 이루어지지 않는다는 점임 GSLB(Global Server Load-Balancing)은 기존 DNS서비스에 (1) 서버상태 모니터링, (2) DNS 서버별 레이턴시 모니터링 등을 더한 향상된 DNS 서비스임 Service를 제공하는 Server들이 여러 지역에 분리되어 완전히 다른 네트워크에서 운용 될 때 이용하는 기법 위치기반, TTL, 등으로 도메인의 여러 ip중 가까운 ip를 제공 Healthcheck으로 주기적인 서버상태 모니터링 진행 " }, { "title": "OS-6(CPU scheduling algorithm)", "url": "/posts/OS-6/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-28 06:00:25 +0000", "snippet": "CPU scheduling algorithm CPU schedule CPU resouces to each threads with following algorithm from Ready Queue non-preemptive : doesn’t stop Job FCFS : execute First in SJF : execute shortest running time job preemptive : does stop Job when assigned time is running out Round Robin : when time(you can set) exceed, insert job to queue again in the back SRF : when shortest job insert in Ready Queue, stop current job and run it To summarizeOS’s has 4 jobs CPU scheduling and process management Memory management Disk file management I/O device management" }, { "title": "OS-5(Context Switch/Critical Section)", "url": "/posts/OS-5/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-27 06:00:25 +0000", "snippet": "What is Context Switch Context Switch : replace PCB inside the memory with other PCB. store Process A’s PCB load Process B’s PCB to memory from disk When system call is occured or interrupt, store Process B’s PCB load Process C’s PCB to memoery from disk What is Shared resource and Critical sectionCritical Section’s three features should be satisfied mutual exclusion : if process A run CS(Critical Section), other can not go into bounded waiting : Process should not wait indefinitely progress flexibility, progress : If there is no process in the CS, any process can enter and utilize the resource.How to satisfy CS features Shared Resource : resource(Printer, specific memory location, files, data) that can be shared with other processes, threads. Critical Section : Areas that depend on the order of access. To solve with this problems, mutex, semaphore exist. mutex : before using critical section, first lock, after use, second release. semaphore : can access critical section with multiple users. What is Deadlock Deadlock : A state in which two or more processes are stopped waiting for each other’s resources" }, { "title": "OS-4(Process/Thread/Memory)", "url": "/posts/OS-4/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-25 06:00:25 +0000", "snippet": "Difference in concept Process program that currently running when program in disk load on memory, it become process Thread basic unit of CPU to execute task process contains one or more threads Difference in structure Process has Code, Data, Stack, Heap, PCB(ID, state, PC, memory limits, Register, priority, files, etc.) Thread has Stack, TCB(ID, state, PC, SP, Register, etc.)Multithread in GolangThis case is when you run Golang executable files with three go-routines which is user-level thread.And these OS kernel thread is matched with goroutine(user-level thread) with this featuresTo get more information about goroutine, please check my last posting here Goroutine structure and behaviorBack to the subject, Users can manage their process with Goruntime or Thread-libraries. And how about OS level?They manage their process with above lifecylceImplementing the Stack in GolangImagin that your stack in Golang program is exceeded. How can you manage your stack? Here is 2 options. Add stack segment with linked list Create larger stack and copying stackThe first option seems very nice, but it runs into certain serious performance problems. Creating more stack doesn’t occur problems, but it will be when you shrink your segment stack just after you finish your function(stack frame). This will take a lot of cost when it is repeated inside the loop. This is called hot split problem.Thus, Golang choose second option.Golang had to choose 2 option in many reasons.First reason : Golang leverage contignous memory allocations for their Garbage Collection. Which means that Golang have to store their local variable in contiguous way(first option doesn’t store their variable contiguously).Second reason : As I said, it has performance problems(hot split).To make contiguous stacks, Golang have 4 stage. create a new, somewhat larger stack copy the contents of the old stack to the new stack re-adjust every copied pointer to point to the new addresses destroy the old stackTo get more information about how Golang managing their stacks, see https://go.dev/doc/go1.4#runtime, https://without.boats/blog/futures-and-segmented-stacks/References https://www.scaler.com/topics/operating-system/process-control-block-in-os/ https://blog.cloudflare.com/how-stacks-are-handled-in-go/" }, { "title": "OS-3(Memory)", "url": "/posts/OS-3/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-22 06:00:25 +0000", "snippet": " This post is based on Unix v6 which is a root for current version of LinuxHow UNIXv6 manage their memoryDefinition of process is “Instance of a program in execution”. And in UNIXv6, they manage Code : store program code Data : store global &amp; static variable, heap local static variable : its data is located in memory until the program exit, but can be only accessed inside the function global static variable : its also located in memory, and also can be accessed outside the function. But not outside the program. User Stack : store arguments, local variable, etc. User : store process information(file location, Registers, etc.) Kernel Stack : store system variable proc : store essential process information which is always located in memory(PID, priority, status, location of program in disk) PCB(Process Control Block) : User + proc, which has metadata for processAnd Text &amp; Data segments will be replaced when Context Switch is occured(by I/O interrupt, error, child process, cpu time scheduling, etc.) We will now note Text + Data segment as swappable image or process image. It is little different with PCB. Beacuse normally PCB contains PID and Status, etc. but here these essential information is always on memory. Thus, Swappable image = PCB - (PID, Satus, Priority, etc.) to reduce the cost of context switchingBut in case of that other user access directly to your memory address space that you currently using, how can you handle?With MMU(Memory management uniut), OS can prevent from misdirection.MMU’s can gives you 3 advantage3 advantages of MMU(Memory Management Unit) Protection prevent R/W/Jump to other process image in memory Scattered Allocation process image can be scattered, so user can manage their memory space more efficiently With Page Table, Scattered Allocation can be managed efficiently. But in UNIXv6, APR(Active Page Register) is used. Relocation when using memory, you can swap image Scattered Allocation could be trade-off comparing to Contignous Allocation In Contignous Allocation, it is more easy to check memory protection fault But nowadays, we use paging tech. in memory. Thus rather Contignous Allocation, Scattered Allocation is used in current daysUNIXv6’s MMU don’t use TLB and Page Table, but APR for their translation between VA(Virtual Address) and PA(Physical Address)" }, { "title": "OS-2(Types of cache mapping)", "url": "/posts/OS-2/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-20 06:00:25 +0000", "snippet": " Direct Mapping Advantage Fast as only tag field matching is required while searching for a word It is comparatively less expensive than associative mapping. Disadvantage high conflict miss rate(lots of replacement) this could be occured when same cache line is used by two or more frequent memory frames. Associative Mapping Advantage easy to implement and low hit latency Disadvantage high cost for searching(hit latency) it needs to search every cache if its hit.it could be fine with placing parallel comparator to each cache line, but still high hit-latency than direct mapping k-way Set Associative Mapping Advantage can store same modular index up to k, lower conflict miss When cache miss occured and needs to write new data, you can write both main memory and cache(Write-through) or write in cache and later write main memory(Write-back). And you can upgrade your performance of cahce with locality(spartial, temporal)References https://www.geeksforgeeks.org/difference-between-direct-mapping-associative-mapping-set-associative-mapping/" }, { "title": "OS-1", "url": "/posts/OS-1/", "categories": "CS, 운영체제", "tags": "CS information", "date": "2022-01-18 06:00:25 +0000", "snippet": "Computer Architecture CPU-CORE Register : Data storage that needed for CU and ALU PC (Program Counter) : store address which is next instruction set AC (Accumulator) : temporary store intermediate caculation result IR (Instruction Register) : store instruction address that run now MAR (Memory Address Register) : temporary store PC MBR (Memory Buffer Register) : store instruction Example of how CPU run their instruction ADD addr 1st step, Fetch cycle load next instruction into IR 1) MAR &lt;- PC 2) MBR &lt;- M[MAR] 3) PC &lt;- PC + 1 4) IR &lt;- MBR 2nd step, Execute cycle get insturction from IR and adding with AC 1) MAR &lt;- IR 2) MBR &lt;- M[MAR] 3) AC &lt;- AC + MBR in LOAD addr, 2nd step will be… LOAD 1) MAR &lt;- IR 2) MBR &lt;- M[MAR] 3) AC &lt;- MBR in STORE addr, , 2nd step will be… STORE 3-1) MAR &lt;- IR 3-2) MBR &lt;- AC 3-3) M[MAR] &lt;- MBR CU(Control Unit) : lead ALU with decoded instructions ALU(Arithmetic Logical Unit) : get decoded instructions and calculate bit by bit TLB(Translation Lookaside Buffer) : Index Table for transform virtual address into physical address TLB has {page:frame} pair Page Table : same function as TLB, but has no page index column ex) address in PC : page number | offset L1 &amp; L2 &amp; L3 cache miss, find TLB if TLB has page number, change page number into frame number if not, find Page Table if Page Table[page number] is not empty, change page number into frame number And fetch Register informations from memory that located in frame number | offsetif not, search in disk and fill or replace TLB and Page Table Why Page Table is needed? 1) CPU can run the process continuously, so it will be fast 2) OS can execute context switching fastTo understand this concept, we need to understand how memory is managed. One Program can be allocated seperately(Page) into memory or just big one chunk(Segment).   Paing Segmentation section size fixed size pages variable size sections context switch fast slow fragmentation internal external Comparison Paging can manage memory with fixed size, so can be switched more faster than Segmentation. Segmentation should know about leftover memory usage, left program memory, limits etc. Paging could be result in internal fragmentation that if your program is smaller than page(usally 4KB) Segmentation has no internal but external fragmentation. Because, as time goes by, the number of small hole in memory is increased. At some point, there is a enough memory left but is not fitted for bigger program. You can seperate your program to fit with small hole. But when you do this, you lose your advantage of Segmentation. So for these reasons, Paging is more popular than segmentation for managing memory!But here is a problem when if your program is seperately paged. If your program is seperately allocated, CPU needs to know where is next Page.Here, Page Table can make your program adjacently(which is contignous allocation in virtual memory)!Why TLB(Translation Lookaside Buffer) is needed?Actually, this Page Table is located inside the main memory. Thus, to reference prgoram inside memory, we need to access memory 2 times. This is very inefficient. Because to access memory, we need to access with memory bus. And this bus is made up with the data bus and the address bus. The speed and delays of an action made in a computer system depends greatly on the address bus since it is the entity locating the information. Which means that if we use address bus a lot, speed will be low!TLB can reduce the number of access time in memory!. Becuase TLB acts as a cache for Page Table.I will finish with this figure.References https://www.geeksforgeeks.org/difference-between-paging-and-segmentation/ https://www.techopedia.com/definition/2236/memory-bus" }, { "title": "Network-2:HTTP comparison by its version(1.0/1.1/2.0/3.0)+basic TLS", "url": "/posts/network(2)/", "categories": "CS, 네트워크", "tags": "CS information", "date": "2022-01-08 06:00:25 +0000", "snippet": "Application Layer Various protocols exist in the application layer. Among them, HTTP is a widely used protocol, and today I will describe it in detail.HTTP HTTP(Hyper Text Transfer Protocol) is a protocol to send and receive data between the browser and the server, so it has become an essential knowledge for developers. Looks like this HTTP is a stateless protocol. What is the stateless meaning of? Here is an example that shows the difference between Stateful and Stateless. Stateful Alice : Hey Bob, can i buy a coffee? Bob : which one? Alice : latte plz. Bob : cost will be 2$ Alice : here is 2$. Bob : here is latte! Stateful Alice : Hey Bob, can i buy a coffee? Bob : which one? Alice : latte plz. Bob : cost will be 2$ Alice : 2$ for what? Bob : for latte. Alice : how much for latte? Bob : cost will be 2$. Alice : 2$ for what? Bob : … In this example, Alice is a server and Bob is a client. Alice don’t remember the state that Bob said. So Bob should remember the cost of latte(this job will be charged at client’s Cookie) and say “I want latte and here is 2 dollars”. Stateless will help server to reduce the load and to improve scalability. But not everything can be designed stateless. Even if you simply look at the LOGIN, the server must maintain the logged-in state of the user. In this case, the state is maintained using browser cookies or server sessions.Many Web service use HTTP to produce services. This HTTP has been changed a lot since 1990, from HTTP 1.0 to HTTP 3.0. I will describe HTTP in detail.(HTTP 1.0~2.0 based on TCP, HTTP 3.0 based on UDP)HTTP 1.0 vs 1.1HTTP 1.0 can only handle with one Request/Response. This occurs problems. Increaed RTT(Round Trip Time) : client should establish/terminate TCP connection for very reqeust. So, there is a unnecessary additional process here. Increaed header size : because of metadata, cookie things.To handle these problems, HTTP 1.1 use base64 encoding, image spliting, keep-alive. Base64 encoding : compress header. Image spliting : load merged image and split it, so reduce RTT for loading multiple images. Keep alive : Reduce unnecessary additional TCP Establish/Terminate process. Pipeline : HTTP 1.0 cannot send multiple reqeust, because they have to wait until the first response come. Pipelining can make client to send multiple request.However HTTP 1.1 still have problem of HOLB. HOLB(Head Of Line Blocking): Every Request &amp; Response should be sequential. Although client get second request faster than first request, client have to wait until first response arrive. There are a lot of chunks that splited. And In HTTP 1.1, client have to wait until every chunk arrive to get next response. HTTP 1.x vs 2.0To handling HOLB problems, HTTP 2.0 use Serverpush, Multiplexing.(Additionally Header Compression, Stream Prioritization) ServerPush : Server can client with additional data without client’s request. Multiplexing : Multistream + Resource-Priority. So client dont need to wait until full responsed(chunks1 + chunks2 + chunks3 + …). Of course if there is a priority between data, client have to wait!HTTP 2.0 vs 3.0To know about HTTP 3.0, we should know about TLS(Transport Layer Security)! TLS is a security layer between Application and Transport Layer(TCP/IP 4 Layers). When we communicate with HTTP, we can capture HTTP packet and look inside. Which means that we can actually snatch HTTP packet and see the private informations inside the packet. This is critical when you send password. TLS can encrpyt all Application data and prevent from capturing private information by others. I already describe in detail in my github wiki What is SSL/TLS?I will shortly describe TLS 1.3TLS 1.3v Handshake RFC#8446 Client Server Key ^ ClientHello Exch | + key_share* send A,g,p: A = g^a (mod p) | + signature_algorithms* | + psk_key_exchange_modes* v + pre_shared_key* --------&gt; ServerHello ^ Key + key_share* | Exch send B: B = g^b (mod p), driven Pre-Shard-Key(PSK) = A^b (mod p) + pre_shared_key* v {EncryptedExtensions} ^ Server send Encrypted message m'=GCM(Key=K,IV=Client's random + Server's random, M=Extensions from Client) {CertificateRequest*} v Params {Certificate*} ^ Server's pub_key, sign, CAs' pub_key, signs {CertificateVerify*} | Auth sign(RSA priv_key, handshake context+certificate) {Finished} v send HMAC(all handshake) &lt;-------- [Application Data*] driven Pre-Shard-Key(PSK) = B^a (mod p) ^ {Certificate*} Auth | {CertificateVerify*} v {Finished} send HMAC(all handshake) --------&gt; [Application Data] &lt;-------&gt; [Application Data] + Indicates noteworthy extensions sent in the previously noted message. * Indicates optional or situation-dependent messages/extensions that are not always sent. {} Indicates messages protected using key PSK : [] Indicates messages protected using key Kn : HKDF derived from [[sender](https://datatracker.ietf.org/doc/html/rfc8446#ref-sender)]_application_traffic_secret_N.Since HTTP 3.0 use TLS 1.3, here is comparation between TLS 1.2 and TLS 1.3. Skills TLS 1.3 TLS 1.2 Key exchange (EC)DHE that can serve forward secrecy RSA, DH, DHE Handshake 1-RTT, 0-RTT(forward secrecy X) 2-RTT Encrpytion AES-256-GCM-SHA384 DHE-RSA-WITH-AES-256-CBC-SHA256 Sign-part Every handshake is signed(Finished) small part of handshake Final comparison, HTTP 1.x vs 2.0 vs 3.0 Lastly here is a camparation of HTTP 1.x, 2.0, 3.0. HTTP 1.x HTTP 2.0 HTTP 3.0 [HTTP 1.x] [TLS(optional)] [TCP] [HTTP 2.0] [TLS 1.2+] [TCP] [HTTP 3.0] [Quic [TLS1.3] ] [UDP] * Keep-alive * image spliting * Multiplexing * ServerPush * Reduce RTT for establishing TCP connection by using UDP Problems : Head Of Line Blocking Problems : Still high RTT   What is RTT of sending 2 HTTP requests in different HTTP versions?If there are any mistakes, please email me." }, { "title": "Network-1:OSI 7 Layers", "url": "/posts/network(1)/", "categories": "CS, 네트워크", "tags": "CS information", "date": "2021-12-29 06:00:25 +0000", "snippet": "What is difference between OSI 7 Layers and TCP/IP 4 Layers?There are 7 layers of OSI and 4 layers of TCP/IP which suppport every protocol of computer. The difference is that how they divide the layers. OSI 7 layers and TCP/IP 4 layers have some overlap and some don’t. See below.Physical Layer PDU = Bit unit PDU=Protocol Data Unit. Every layer has its PDU. For Physical Layer, “Bit” Mesh, Start (Hub) and Ring are representative of this layer. This layer does not concern whether the bits wrong or have error. Only transmit or recept! DataLink Layer PDU = Frame unit L2-Switch is representative of this layer DataLink Layer(sublayers)|____ LCC(Logical Link Control)| |___ Flow control| |___ Multiplexing||____ MAC(Medium Access Control) |___ CSMA/CD |___ CSMA/CA |___ Retransmission |___ Framing |___ FCS frame check sequence LCC : Provide Flow control &amp; Multiplexing for logical link Flow-control : Prevents excessive reception by others Multiplexing : Mapping into a original packet from separated packets MAC : Dealing with collisions, framing, checking sequences. CSMA/CD,CA(Carrier Sense Multiple Access with Collision Detetion, Avoidance): If two users are both listen or transmit their packet in a same time, there should be collapsed!. With Media Access Control, frame recovery &amp; avoid &amp; retransmit are occured here. Framing : = capsulation. I bring the representative example of Framing(Ethernet) here. MAC address consist of 12 hexadecimal characters. And MAC address is also called as NIC(Network Interface Card) number. Network Layer PDU = Packet unit Router is representative of this layerNetwork Layer(protocol)|____ ARP|____ IP|____ ICMPARP(Address Resolution Protocol) ARP : To communicate inside local network, MAC address is essential. ARP help to find right MAC address. Here is a cached arp address in local network ARP steps for find right MAC address. First Apple_e0:7f:fe is Desination address and that’s me. As fe:8f:20:52:f2:03 already know my IP and MAC address, he wants to know my IP and MAC address is correct. ARP Request Response just switching Dst and Src. ARP Response However, if he dont know about MAC address or IP address? Here is examples. ARP Request broadcasting(know Target IP) Here, you can see Target IP is declared but still, MAC address is empty. This packet is broadcasted and if 192.168.219.108 get this packet, 192.168.219.108 send back to 192.168.219.1 filled with its MAC address in Src MAC address. Thus, when you want to know target’s IP or MAC address, you should know at least one either IP or MAC address.IP IP : In local network, we can communicate each others with MAC address. However, if you want to communicate outside of local network(Router), IP address is essential. Here is example of how IP looks like. TTL(Time To Live) is a hop limit for alive, and it is a good way for self destructing. However, TTL has limitations as below The IPv4 RFC states that TTL is measured in seconds but acknowledges this is an imperfect measure. There is no way of knowing how long any particular host will take to process a packet and most will do so in far less than a second. In any case, based on this logic, in theory the maximum time a packet can exist in the network is 4.25 minutes (255 seconds). Hence, the TTL is described as a “self destruct time limit”. (reference from https://packetpushers.net/ip-time-to-live-and-hop-limit-basics/) TTL is different by its OS. As my OS is MACOS, my hop limit will be 64. OS hop limits protocol Linux kernel 2.4 (circa 2001) 255 TCP, UDP and ICMP Linux kernel 4.10 (2015) 64 TCP, UDP and ICMP Windows XP (2001) 128 TCP, UDP and ICMP Windows 10 (2015) 128 TCP, UDP and ICMP Windows Server 2008 128 TCP, UDP and ICMP Windows Server 2019 (2018) 128 TCP, UDP and ICMP MacOS (2001) 64 TCP, UDP and ICMP ICMP ICMP(Internet Control Message Protocol) : This protocol manage whether specific IP address is reachable using various flags(e.g. Destination Unreachable, Ping, etc.)Transport Layer PDU = Segment unit L4-switch is representative of this layer This layer allows users communicate with reliable data. So here also exist error detection and recovery. Transport Layer(protocol)|____ TCP|____ UDP The difference between TCP and UDP Protocol Type TCP UDP Connection Type connection-oriented protocol connection-less protocol Optimal use HTTPS(&lt;=HTTP 2.x), HTTP, SMTP, POP, FTP HTTPS(HTTP 3.0), Video conferencing, streaming, DNS Error checking checksum, ack(duplication, sequence) checksum Data sequencing able unable Retransmission able unable Guaranteed delivery able Unable Speed low high TCP use 3-way handshake to verify the endpoint of users. 4-way handshake for cutting off connection. That means TCP stands for connection-oriented. TCP structureRTT is round trip time. SEQ/ACK analysis shows that this segment belongs to frame 88, so can concatenate all segment into one packet for upper layer. UDP doesn’t use any handshake. Sender dont know that receiver get the right packet or not. UDP structure Nowadays packet loss rate is lower and lower, so we dont have to use TCP protocol. So recently HTTP 3.0 is made on UDP(not based on TCP) to speed up RTT.Session Layer Session layer supports establishment, management, and termination of connections between the local and remote application. It provides for full-duplex, half-duplex, or simplex operation, and establishes check-pointing, adjournment, termination, and restart procedures. if we set connection using TCP 3-way handshake, ACK and SEQ numbers, timestamps, etc. are stored in here. Presentation Layer This layer supports encoding and decoding packets by its format(e.g. base64-encoding, image-format, html, etc.).Application LayerThis layer has so many things to talk. So I will talk in next posts." }, { "title": "Eclipse Attack in Geth v1.8.x", "url": "/posts/ethereum-eclipse-attack2/", "categories": "블록체인, 이더리움", "tags": "Network, Ethereum", "date": "2020-12-04 06:00:25 +0000", "snippet": "Countermeasures of Eclipse Attack in Geth v1.8.x In Geth v1.6.6, If Attacker send Ping to the victim, attacker can monopolize connections of the victim To prevent mopolized connection, Geth 1.8.x limit their inbound connection up to 17 Also in Geth v1.8.x, they limit IP in their DHT. In DHT’s bucket, node with same IP can be exist up to 2 In DHT, node with same IP can be exist up to 10 But still, Eclipse Attack exists …As show in previously, now the victim use outbound connections for safety connection to normal nodes. However, outbound connection is also monopolized by the attacker. Outbound connection is consist of 2 types of functions(lookupBuffer, ReadRandomNodes). lookupBuffer is a storage of closest nodes. This is filled when the victim send FindNode packets to other nodes. ReadRandomNodes is a function that find nodes for outbound connection from the DHT. Monopolize lookupBuffer Attacker generate multiple nodes for attack preparation. When the attacker get FindNode packets from the victim with random target, using prepared ndoes, attacker send closest nodes to the victim. lookupBuffer is filled with attacker’s nodes.Monopolize ReadRandomNodesReadRandomNodes load the nodes from DHT. But, only load top of the buckets!(This is a critical vulnerability that allows the attacker to fill the small portion of buckets) Attacker send Ping to the victim victim send Pong and fill a attakcer node to top of the bucket. Since the victim has only 17 buckets, attacker send 17 Pings to the victim(to fill each buckets). Thus, this vulnerability make the attacker to reduce their resource for the attack.Analysis of Geth v1.9.24 Network" }, { "title": "Eclipse Attack in Geth v1.6.6", "url": "/posts/ethereum-eclipse-attack/", "categories": "블록체인, 이더리움", "tags": "Network, Ethereum", "date": "2020-11-27 06:00:25 +0000", "snippet": "Analysis of Geth 1.6.6v’s P2P network create UDP/TCP listener schedule 2 types of task(discoverTask, dialTask) seeding Ethereum nodes into Kademlia DHT create Kademlia DHT get Ethereum node’s information from local database(level db) for seeding get seedCount(default = 30) Ethereum nodes from DB and also hard-coded bootstrap_nodes. insert bootstrap_nodes + DB_nodes into DHT keep DHT fresh and do bonding process red-box is a goroutine use lookup(random target) process to populate DHT validating nodes in DHT with Ping/Pong pakcets(1h) keep the old nodes in DHT in DB(5m) connection read loop create TCP Listener create 50 channel for connection and scheduling pass the results of handshake to srv.run goroutine’s channel add peer to eth.peer RLPx handshake set both in/outbound connection goroutine 2 types Task scheduling(discover, dial) By its channel, add peer and execute discoverTask : kademlia-like lookup add nodes into lookupBuf when discoverTask is done dialTask : dialing for setupConnection kademlia-like lookup Get closest(random Target) 16 nodes from DHT Among these nodes, if there are first-seen-nodes, doing bonding process and insert into db. run when running task is below 16 MaxDynamicDial(MaxOutboundConn) = ( 1+maxpeer(25) )/2 = 13 can add staticNode leftover dialing count = needDynDials extract 6 nodes( needDynDials/2 ) from DHT, and create dialTask if it still require more nodes, extract 7 nodes from lookupBuf, and create dialTask Where Inbound connection are actually confirmed Inbound connection is no limited Attacker create a lot of inbound connection requests to the victim in 20 seconds right after the victim boot.Thus, to prevent all connection from dominated by adversary connections, in Geth 1.9.24v, they set limit of inbound connections. So if attacker send multiple inbound connection requests, there is a limit! And the victim can still connect with normal nodes by outbound connection." } ]
